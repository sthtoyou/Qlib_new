# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os
import re
import sys
import qlib
import shutil
import zipfile
import requests
import datetime
import pandas as pd
import numpy as np
import struct
import time
try:
    import yfinance as yf
except ImportError:
    logger.warning("yfinance not installed, US data download will not be available")
    yf = None
from tqdm import tqdm
from pathlib import Path
from loguru import logger
from qlib.utils import exists_qlib_data


class GetDataEnhanced:
    """
    å¢å¼ºç‰ˆæ•°æ®ä¸‹è½½ç±»
    æ”¯æŒä¸­å›½ã€ç¾è‚¡å’Œæ¸¯è‚¡æ•°æ®ä¸‹è½½ï¼Œè‡ªåŠ¨ç”Ÿæˆæ ‡å‡†qlibæ ¼å¼
    """
    REMOTE_URL = "https://github.com/SunsetWolf/qlib_dataset/releases/download"

    def __init__(self, delete_zip_file=False):
        """
        Parameters
        ----------
        delete_zip_file : bool, optional
            Whether to delete the zip file, value from True or False, by default False
        """
        self.delete_zip_file = delete_zip_file

    def merge_remote_url(self, file_name: str):
        """Generate download links."""
        return f"{self.REMOTE_URL}/{file_name}" if "/" in file_name else f"{self.REMOTE_URL}/v0/{file_name}"

    def download(self, url: str, target_path: [Path, str]):
        """Download a file from the specified url."""
        file_name = str(target_path).rsplit("/", maxsplit=1)[-1]
        resp = requests.get(url, stream=True, timeout=60)
        resp.raise_for_status()
        if resp.status_code != 200:
            raise requests.exceptions.HTTPError()

        chunk_size = 1024
        logger.warning(
            f"The data for the example is collected from Yahoo Finance. Please be aware that the quality of the data might not be perfect. (You can refer to the original data source: https://finance.yahoo.com/lookup.)"
        )
        logger.info(f"{os.path.basename(file_name)} downloading......")
        with tqdm(total=int(resp.headers.get("Content-Length", 0))) as p_bar:
            with target_path.open("wb") as fp:
                for chunk in resp.iter_content(chunk_size=chunk_size):
                    fp.write(chunk)
                    p_bar.update(chunk_size)

    def download_data(self, file_name: str, target_dir: [Path, str], delete_old: bool = True):
        """Download the specified file to the target folder."""
        target_dir = Path(target_dir).expanduser()
        target_dir.mkdir(exist_ok=True, parents=True)
        # saved file name
        _target_file_name = datetime.datetime.now().strftime("%Y%m%d%H%M%S") + "_" + os.path.basename(file_name)
        target_path = target_dir.joinpath(_target_file_name)

        url = self.merge_remote_url(file_name)
        self.download(url=url, target_path=target_path)

        self._unzip(target_path, target_dir, delete_old)
        if self.delete_zip_file:
            target_path.unlink()

    def check_dataset(self, file_name: str):
        url = self.merge_remote_url(file_name)
        resp = requests.get(url, stream=True, timeout=60)
        status = True
        if resp.status_code == 404:
            status = False
        return status

    @staticmethod
    def _unzip(file_path: [Path, str], target_dir: [Path, str], delete_old: bool = True):
        file_path = Path(file_path)
        target_dir = Path(target_dir)
        if delete_old:
            logger.warning(
                f"will delete the old qlib data directory(features, instruments, calendars, features_cache, dataset_cache): {target_dir}"
            )
            GetDataEnhanced._delete_qlib_data(target_dir)
        logger.info(f"{file_path} unzipping......")
        with zipfile.ZipFile(str(file_path.resolve()), "r") as zp:
            for _file in tqdm(zp.namelist()):
                zp.extract(_file, str(target_dir.resolve()))

    @staticmethod
    def _delete_qlib_data(file_dir: Path):
        rm_dirs = []
        for _name in ["features", "calendars", "instruments", "features_cache", "dataset_cache"]:
            _p = file_dir.joinpath(_name)
            if _p.exists():
                rm_dirs.append(str(_p.resolve()))
        if rm_dirs:
            flag = input(
                f"Will be deleted: "
                f"\n\t{rm_dirs}"
                f"\nIf you do not need to delete {file_dir}, please change the <--target_dir>"
                f"\nAre you sure you want to delete, yes(Y/y), no (N/n):"
            )
            if str(flag) not in ["Y", "y"]:
                sys.exit()
            for _p in rm_dirs:
                logger.warning(f"delete: {_p}")
                shutil.rmtree(_p)

    def _get_us_stocks_list(self):
        """è·å–ç¾è‚¡è‚¡ç¥¨åˆ—è¡¨"""
        logger.info("æ­£åœ¨è·å–ç¾è‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        # æ–¹æ³•1ï¼šä»WikipediaåŠ¨æ€è·å–æ ‡æ™®500æˆåˆ†è‚¡
        try:
            import pandas as pd
            import requests
            
            logger.info("å°è¯•ä»Wikipediaè·å–æ ‡æ™®500æˆåˆ†è‚¡...")
            
            url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
            response = requests.get(url, timeout=15)
            
            if response.status_code == 200:
                logger.info("âœ“ æˆåŠŸè¿æ¥Wikipedia")
                
                # ä½¿ç”¨pandasç›´æ¥è¯»å–è¡¨æ ¼
                tables = pd.read_html(url)
                sp500_table = tables[0]  # ç¬¬ä¸€ä¸ªè¡¨æ ¼æ˜¯æ ‡æ™®500åˆ—è¡¨
                
                # è·å–è‚¡ç¥¨ä»£ç åˆ—
                if 'Symbol' in sp500_table.columns:
                    symbols = sp500_table['Symbol'].tolist()
                elif 'Ticker symbol' in sp500_table.columns:
                    symbols = sp500_table['Ticker symbol'].tolist()
                else:
                    # å°è¯•ç¬¬ä¸€åˆ—
                    symbols = sp500_table.iloc[:, 0].tolist()
                
                # æ¸…ç†æ•°æ®ï¼šç§»é™¤æ— æ•ˆç¬¦å·
                clean_symbols = []
                for symbol in symbols:
                    if isinstance(symbol, str) and len(symbol) <= 6:
                        # å…è®¸å­—æ¯ã€æ•°å­—å’Œè¿å­—ç¬¦
                        clean_symbol = str(symbol).strip().upper()
                        if clean_symbol and all(c.isalnum() or c in ['-', '.'] for c in clean_symbol):
                            clean_symbols.append(clean_symbol)
                
                if len(clean_symbols) >= 450:  # ç¡®ä¿è·å–åˆ°è¶³å¤Ÿå¤šçš„è‚¡ç¥¨
                    logger.info(f"âœ… æˆåŠŸè·å– {len(clean_symbols)} åªæ ‡æ™®500æˆåˆ†è‚¡")
                    
                    # éªŒè¯ä¸€äº›çŸ¥åè‚¡ç¥¨
                    famous_stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'BRK.B']
                    # BRK-Båœ¨ä¸åŒæ¥æºå¯èƒ½æ˜¾ç¤ºä¸ºBRK.B
                    found_count = 0
                    for stock in famous_stocks:
                        if stock in clean_symbols or stock.replace('.', '-') in clean_symbols:
                            found_count += 1
                    logger.info(f"çŸ¥åè‚¡ç¥¨éªŒè¯: {found_count}/{len(famous_stocks)} åª")
                    
                    return clean_symbols
                else:
                    logger.warning(f"è·å–çš„è‚¡ç¥¨æ•°é‡ä¸è¶³: {len(clean_symbols)}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            else:
                logger.warning(f"Wikipediaè®¿é—®å¤±è´¥: HTTP {response.status_code}")
                
        except ImportError as e:
            logger.warning(f"ç¼ºå°‘å¿…è¦åº“: {e}ï¼Œä½¿ç”¨å¤‡ç”¨è‚¡ç¥¨åˆ—è¡¨")
        except Exception as e:
            logger.warning(f"ä»Wikipediaè·å–æ ‡æ™®500å¤±è´¥: {e}")
        
        # æ–¹æ³•2ï¼šå°è¯•å…¶ä»–æ•°æ®æºï¼ˆé¢„ç•™ï¼‰
        try:
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–æ•°æ®æºï¼Œå¦‚Yahoo Financeã€Alpha Vantageç­‰
            logger.info("å…¶ä»–æ•°æ®æºæ–¹æ³•é¢„ç•™ï¼Œä½¿ç”¨å¤‡ç”¨åˆ—è¡¨")
            
        except Exception as e:
            logger.warning(f"å…¶ä»–æ•°æ®æºè·å–å¤±è´¥: {e}")
        
        # æ–¹æ³•3ï¼šå¤‡ç”¨æ ‡æ™®500ä¸»è¦æˆåˆ†è‚¡åˆ—è¡¨
        logger.info("ä½¿ç”¨å¤‡ç”¨æ ‡æ™®500ä¸»è¦æˆåˆ†è‚¡åˆ—è¡¨...")
        
        # ç²¾é€‰çš„æ ‡æ™®500é‡è¦æˆåˆ†è‚¡ï¼ˆæŒ‰å¸‚å€¼å’ŒæµåŠ¨æ€§ç­›é€‰ï¼‰
        sp500_symbols = [
            # ç§‘æŠ€å·¨å¤´ (Technology)
            'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'META', 'TSLA', 'NVDA', 'NFLX', 'ADBE',
            'CRM', 'ORCL', 'AMD', 'INTC', 'IBM', 'QCOM', 'TXN', 'AVGO', 'INTU', 'CSCO',
            'PYPL', 'EBAY', 'UBER', 'LYFT', 'SNAP', 'TWTR', 'ZOOM', 'DOCU', 'OKTA', 'CRWD',
            
            # é‡‘èæœåŠ¡ (Financial Services)
            'BRK-B', 'JPM', 'BAC', 'WFC', 'GS', 'MS', 'C', 'AXP', 'V', 'MA',
            'COF', 'DIS', 'USB', 'PNC', 'TFC', 'BLK', 'SCHW', 'CB', 'ICE', 'CME',
            'AON', 'MMC', 'AJG', 'TRV', 'ALL', 'PGR', 'AFL', 'MET', 'PRU', 'AIG',
            
            # åŒ»ç–—ä¿å¥ (Healthcare) 
            'UNH', 'JNJ', 'PFE', 'ABBV', 'LLY', 'TMO', 'ABT', 'MDT', 'BMY', 'AMGN',
            'GILD', 'CVS', 'CI', 'HUM', 'ANTM', 'ZTS', 'SYK', 'BSX', 'EW', 'DHR',
            'BDX', 'BAX', 'HCA', 'CNC', 'MOH', 'DVA', 'MCK', 'ABC', 'CAH', 'WBA',
            
            # æ¶ˆè´¹å“ (Consumer)
            'PG', 'KO', 'PEP', 'WMT', 'HD', 'MCD', 'NKE', 'SBUX', 'TGT', 'LOW',
            'COST', 'TJX', 'BKNG', 'MAR', 'HLT', 'MGM', 'LVS', 'WYNN', 'CCL', 'RCL',
            'YUM', 'CMG', 'QSR', 'DPZ', 'MCD', 'SBUX', 'KHC', 'GIS', 'K', 'CAG',
            
            # å·¥ä¸š (Industrial)
            'BA', 'CAT', 'GE', 'MMM', 'HON', 'UPS', 'FDX', 'LMT', 'RTX', 'NOC',
            'GD', 'LHX', 'ITW', 'EMR', 'ETN', 'PH', 'CMI', 'DE', 'DOV', 'ROK',
            'JCI', 'CARR', 'OTIS', 'FTV', 'XYL', 'PNR', 'WAT', 'IEX', 'ROP', 'FAST',
            
            # èƒ½æº (Energy)
            'XOM', 'CVX', 'COP', 'EOG', 'SLB', 'PSX', 'VLO', 'MPC', 'KMI', 'OKE',
            'WMB', 'EPD', 'MRO', 'DVN', 'FANG', 'APA', 'EQT', 'CNX', 'RRC', 'AR',
            
            # å…¬ç”¨äº‹ä¸š (Utilities)
            'NEE', 'DUK', 'SO', 'D', 'AEP', 'EXC', 'XEL', 'PEG', 'SRE', 'ES',
            'ED', 'FE', 'EIX', 'ETR', 'WEC', 'DTE', 'PPL', 'CMS', 'CNP', 'ATO',
            
            # æˆ¿åœ°äº§ (Real Estate)
            'AMT', 'PLD', 'CCI', 'EQIX', 'SPG', 'PSA', 'O', 'WELL', 'DLR', 'EXR',
            'AVB', 'EQR', 'VTR', 'ESS', 'MAA', 'UDR', 'CPT', 'HST', 'REG', 'BXP',
            
            # ææ–™ (Materials)
            'LIN', 'APD', 'ECL', 'SHW', 'FCX', 'NUE', 'DD', 'DOW', 'PPG', 'IP',
            'PKG', 'CF', 'FMC', 'ALB', 'CE', 'VMC', 'MLM', 'NEM', 'AA', 'X',
            
            # é€šä¿¡æœåŠ¡ (Communication Services)
            'T', 'VZ', 'CMCSA', 'CHTR', 'DISH', 'SIRI', 'LUMN', 'TMUS', 'TTWO', 'EA',
            'ATVI', 'NWSA', 'NYT', 'IPG', 'OMC', 'PARA', 'FOXA', 'FOX', 'WBD', 'LYV',
            
            # æ¶ˆè´¹å¿…éœ€å“ (Consumer Staples)
            'WBA', 'CVS', 'KR', 'WMT', 'COST', 'TGT', 'DG', 'DLTR', 'SYY', 'KDP',
            'MNST', 'PEP', 'KO', 'CL', 'PG', 'UL', 'NSRGY', 'KHC', 'GIS', 'K'
        ]
        
        # å»é‡å¹¶æ’åº
        sp500_symbols = sorted(list(set(sp500_symbols)))
        
        logger.info(f"âœ“ ä½¿ç”¨å¤‡ç”¨è‚¡ç¥¨åˆ—è¡¨: {len(sp500_symbols)} åªç¾è‚¡ï¼ˆåŒ…å«æ ‡æ™®500ä¸»è¦æˆåˆ†è‚¡ï¼‰")
        return sp500_symbols

    def _get_hk_stocks_list(self):
        """è·å–æ¸¯è‚¡è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰
        åŒ…å«æ’ç”ŸæŒ‡æ•°+æ’ç”Ÿä¸­å›½ä¼ä¸šæŒ‡æ•°+æ’ç”Ÿç§‘æŠ€æŒ‡æ•°+å¯Œæ—¶ä¸­å›½50æŒ‡æ•°æˆåˆ†è‚¡
        """
        logger.info("æ­£åœ¨è·å–æ‰©å±•ç‰ˆæ¸¯è‚¡è‚¡ç¥¨åˆ—è¡¨...")
        logger.info("è¦†ç›–èŒƒå›´: æ’ç”ŸæŒ‡æ•° + æ’ç”Ÿä¸­å›½ä¼ä¸šæŒ‡æ•° + æ’ç”Ÿç§‘æŠ€æŒ‡æ•° + å¯Œæ—¶ä¸­å›½50æŒ‡æ•°")
        
        # æ–¹æ³•1ï¼šå°è¯•åŠ¨æ€è·å–æˆåˆ†è‚¡ä¿¡æ¯
        dynamic_stocks = self._get_hk_stocks_dynamic()
        if dynamic_stocks and len(dynamic_stocks) >= 150:
            logger.info(f"âœ… åŠ¨æ€è·å–æˆåŠŸ: {len(dynamic_stocks)} åªæ¸¯è‚¡")
            return dynamic_stocks
        
        # æ–¹æ³•2ï¼šä½¿ç”¨æ‰©å±•çš„é™æ€è‚¡ç¥¨åˆ—è¡¨
        logger.info("ä½¿ç”¨æ‰©å±•ç‰ˆæ¸¯è‚¡æˆåˆ†è‚¡åˆ—è¡¨...")
        
        hk_symbols = []
        
        # === æ’ç”ŸæŒ‡æ•° (HSI) æˆåˆ†è‚¡ ===
        hsi_stocks = [
            # é‡‘èæ¿å—
            '0005.HK',  # æ±‡ä¸°æ§è‚¡
            '0011.HK',  # æ’ç”Ÿé“¶è¡Œ
            '0388.HK',  # é¦™æ¸¯äº¤æ˜“æ‰€
            '0939.HK',  # å»ºè®¾é“¶è¡Œ
            '1398.HK',  # å·¥å•†é“¶è¡Œ
            '3988.HK',  # ä¸­å›½é“¶è¡Œ
            '2318.HK',  # ä¸­å›½å¹³å®‰
            '2388.HK',  # ä¸­é“¶é¦™æ¸¯
            '1288.HK',  # å†œä¸šé“¶è¡Œ
            '6030.HK',  # ä¸­ä¿¡è¯åˆ¸
            
            # ç§‘æŠ€æ¿å—
            '0700.HK',  # è…¾è®¯æ§è‚¡
            '9988.HK',  # é˜¿é‡Œå·´å·´
            '3690.HK',  # ç¾å›¢
            '1024.HK',  # å¿«æ‰‹ç§‘æŠ€
            '9618.HK',  # äº¬ä¸œé›†å›¢
            '1810.HK',  # å°ç±³é›†å›¢
            '0981.HK',  # ä¸­èŠ¯å›½é™…
            '2382.HK',  # èˆœå®‡å…‰å­¦
            
            # åœ°äº§æ¿å—
            '0016.HK',  # æ–°é¸¿åŸºåœ°äº§
            '0017.HK',  # æ–°ä¸–ç•Œå‘å±•
            '0083.HK',  # ä¿¡å’Œç½®ä¸š
            '0101.HK',  # æ’éš†åœ°äº§
            '1109.HK',  # åæ¶¦ç½®åœ°
            '1997.HK',  # ä¹é¾™ä»“ç½®ä¸š
            
            # èƒ½æºçŸ³åŒ–
            '0857.HK',  # ä¸­å›½çŸ³æ²¹
            '0883.HK',  # ä¸­æµ·æ²¹
            '0386.HK',  # ä¸­å›½çŸ³åŒ–
            '2628.HK',  # ä¸­å›½äººå¯¿
            
            # åŸºå»ºå…¬ç”¨
            '0002.HK',  # ä¸­ç”µæ§è‚¡
            '0003.HK',  # é¦™æ¸¯ä¸­åç…¤æ°”
            '0066.HK',  # æ¸¯é“å…¬å¸
            '0267.HK',  # ä¸­ä¿¡è‚¡ä»½
            '0688.HK',  # ä¸­å›½æµ·å¤–
            '1038.HK',  # é•¿æ±ŸåŸºå»º
            '1044.HK',  # æ’å®‰å›½é™…
            '6862.HK',  # æµ·åº•æ
            
            # æ¶ˆè´¹åŒ»è¯
            '0288.HK',  # ä¸‡æ´²å›½é™…
            '0291.HK',  # åæ¶¦å•¤é…’
            '0762.HK',  # ä¸­å›½è”é€š
            '0823.HK',  # é¢†å±•æˆ¿äº§åŸºé‡‘
            '0960.HK',  # é¾™æ¹–é›†å›¢
            '0968.HK',  # ä¿¡ä¹‰å…‰èƒ½
            '1093.HK',  # çŸ³è¯é›†å›¢
            '1113.HK',  # é•¿å®é›†å›¢
            '1177.HK',  # ä¸­å›½ç”Ÿç‰©åˆ¶è¯
            '1211.HK',  # æ¯”äºšè¿ª
            '1299.HK',  # å‹é‚¦ä¿é™©
            '1876.HK',  # ç™¾å¨äºšå¤ª
            '1918.HK',  # èåˆ›ä¸­å›½
            '1972.HK',  # å¤ªå¤åœ°äº§
            '2007.HK',  # ç¢§æ¡‚å›­
            '2018.HK',  # ç‘å£°ç§‘æŠ€
            '2020.HK',  # å®‰è¸ä½“è‚²
            '2269.HK',  # è¯æ˜ç”Ÿç‰©
            '2313.HK',  # ç”³æ´²å›½é™…
            '2319.HK',  # è’™ç‰›ä¹³ä¸š
            '2331.HK',  # æå®
            '2518.HK',  # æ±½è½¦ä¹‹å®¶
            '3968.HK',  # æ‹›å•†é“¶è¡Œ
            '6098.HK',  # ç¢§æ¡‚å›­æœåŠ¡
            '9888.HK',  # ç™¾åº¦é›†å›¢
            '9999.HK',  # ç½‘æ˜“
        ]
        
        # === æ’ç”Ÿä¸­å›½ä¼ä¸šæŒ‡æ•° (HSCEI) Hè‚¡æˆåˆ†è‚¡ ===
        hscei_stocks = [
            # å¤§å‹å›½ä¼Hè‚¡
            '0914.HK',  # æµ·èºæ°´æ³¥
            '0728.HK',  # ä¸­å›½ç”µä¿¡
            '0753.HK',  # ä¸­å›½å›½èˆª
            '0670.HK',  # ä¸­å›½ä¸œæ–¹èˆªç©º
            '1088.HK',  # ä¸­å›½ç¥å
            '1336.HK',  # æ–°åä¿é™©
            '1359.HK',  # ä¸­å›½ä¿¡è¾¾
            '1368.HK',  # ç‰¹æ­¥å›½é™…
            '1448.HK',  # ç¦å¯¿å›­
            '1478.HK',  # å››ç¯åŒ»è¯
            '1658.HK',  # é‚®å‚¨é“¶è¡Œ
            '1833.HK',  # å¹³å®‰å¥½åŒ»ç”Ÿ
            '1928.HK',  # é‡‘æ²™ä¸­å›½
            '2238.HK',  # å¹¿æ±½é›†å›¢
            '2380.HK',  # ä¸­å›½ç”µåŠ›
            '2588.HK',  # ä¸­é“¶èˆªç©ºç§Ÿèµ
            '3328.HK',  # äº¤é€šé“¶è¡Œ
            '3800.HK',  # åé‘«æ–°èƒ½æº
            '6178.HK',  # å…‰å¤§è¯åˆ¸
            '6690.HK',  # æµ·å°”æ™ºå®¶
            '9626.HK',  # å“”å“©å“”å“©
            '9961.HK',  # æºç¨‹é›†å›¢
            
            # çº¢ç­¹è‚¡
            '0992.HK',  # è”æƒ³é›†å›¢
            '1186.HK',  # ä¸­å›½é“å»º
            '1347.HK',  # åè™¹åŠå¯¼ä½“
            '1800.HK',  # ä¸­å›½äº¤å»º
            '1919.HK',  # ä¸­è¿œæµ·æ§
            '1988.HK',  # æ°‘ç”Ÿé“¶è¡Œ
            '2899.HK',  # ç´«é‡‘çŸ¿ä¸š
            '3993.HK',  # æ´›é˜³é’¼ä¸š
            '6886.HK',  # HTSC
        ]
        
        # === æ’ç”Ÿç§‘æŠ€æŒ‡æ•° (HSTECH) ç§‘æŠ€è‚¡ ===
        hstech_stocks = [
            # äº’è”ç½‘å¹³å°
            '0700.HK',  # è…¾è®¯æ§è‚¡
            '9988.HK',  # é˜¿é‡Œå·´å·´
            '3690.HK',  # ç¾å›¢
            '9618.HK',  # äº¬ä¸œé›†å›¢
            '1024.HK',  # å¿«æ‰‹ç§‘æŠ€
            '9888.HK',  # ç™¾åº¦é›†å›¢
            '9999.HK',  # ç½‘æ˜“
            '9626.HK',  # å“”å“©å“”å“©
            '9961.HK',  # æºç¨‹é›†å›¢
            '2518.HK',  # æ±½è½¦ä¹‹å®¶
            
            # ç¡¬ä»¶åˆ¶é€ 
            '1810.HK',  # å°ç±³é›†å›¢
            '2018.HK',  # ç‘å£°ç§‘æŠ€
            '2382.HK',  # èˆœå®‡å…‰å­¦
            '0981.HK',  # ä¸­èŠ¯å›½é™…
            '1347.HK',  # åè™¹åŠå¯¼ä½“
            '0992.HK',  # è”æƒ³é›†å›¢
            
            # æ–°èƒ½æºæ±½è½¦
            '1211.HK',  # æ¯”äºšè¿ª
            '2015.HK',  # ç†æƒ³æ±½è½¦
            '9868.HK',  # å°é¹æ±½è½¦
            '9866.HK',  # è”šæ¥
            
            # ç”Ÿç‰©åŒ»è¯ç§‘æŠ€
            '2269.HK',  # è¯æ˜ç”Ÿç‰©
            '1177.HK',  # ä¸­å›½ç”Ÿç‰©åˆ¶è¯
            '6160.HK',  # ç™¾æµç¥å·
            '9926.HK',  # åº·æ–¹ç”Ÿç‰©
            '1833.HK',  # å¹³å®‰å¥½åŒ»ç”Ÿ
            
            # é‡‘èç§‘æŠ€
            '6993.HK',  # è“æœˆäº®
            '1772.HK',  # èµ£é”‹é”‚ä¸š
            '2020.HK',  # å®‰è¸ä½“è‚²ï¼ˆæ™ºèƒ½åˆ¶é€ ï¼‰
            
            # æ¸…æ´èƒ½æº
            '0968.HK',  # ä¿¡ä¹‰å…‰èƒ½
            '1772.HK',  # èµ£é”‹é”‚ä¸š
            '3800.HK',  # åé‘«æ–°èƒ½æº
        ]
        
        # === å¯Œæ—¶ä¸­å›½50æŒ‡æ•° (FTSE China 50) æˆåˆ†è‚¡ ===
        ftse_china50_stocks = [
            # å¤§ç›˜è“ç­¹ï¼ˆä¸HSIé‡å è¾ƒå¤šï¼Œè¡¥å……ä¸€äº›ç‹¬æœ‰æ ‡çš„ï¼‰
            '1038.HK',  # é•¿æ±ŸåŸºå»º
            '1044.HK',  # æ’å®‰å›½é™…
            '0175.HK',  # å‰åˆ©æ±½è½¦
            '1066.HK',  # å¨é«˜è‚¡ä»½
            '0151.HK',  # ä¸­å›½æ—ºæ—º
            '1119.HK',  # å°æŸ“è‚¡ä»½
            '1122.HK',  # åº†é¾„æ±½è½¦
            '1128.HK',  # ä¸‡ç¦ç”Ÿç§‘
            '1137.HK',  # ä¸­å›½åŸå»º
            '1157.HK',  # ä¸­è”é‡ç§‘
            '1199.HK',  # ä¸­è¿œæµ·å‘
            '1230.HK',  # é›…å±…ä¹
            '1339.HK',  # ä¸­å›½äººæ°‘ä¿é™©
            '1378.HK',  # ä¸­å›½å®æ¡¥
            '1668.HK',  # åå—åŸ
            '1818.HK',  # æ‹›é‡‘çŸ¿ä¸š
            '1988.HK',  # æ°‘ç”Ÿé“¶è¡Œ
            '2600.HK',  # ä¸­å›½é“ä¸š
            '2823.HK',  # åº·å¸ˆå‚…
            '2866.HK',  # ä¸­æµ·é›†è¿
            '3883.HK',  # ä¸­å›½å¥¥å›­
            '6808.HK',  # é«˜é‘«é›¶å”®
            '9923.HK',  # ç§»å¡
        ]
        
        # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨å¹¶å»é‡
        all_stocks = set()
        all_stocks.update(hsi_stocks)
        all_stocks.update(hscei_stocks) 
        all_stocks.update(hstech_stocks)
        all_stocks.update(ftse_china50_stocks)
        
        # è½¬æ¢ä¸ºæœ‰åºåˆ—è¡¨
        hk_symbols = sorted(list(all_stocks))
        
        logger.info(f"âœ… æ‰©å±•ç‰ˆæ¸¯è‚¡è‚¡ç¥¨æ± æ„å»ºå®Œæˆ:")
        logger.info(f"  - æ’ç”ŸæŒ‡æ•°æˆåˆ†è‚¡: {len(hsi_stocks)} åª")
        logger.info(f"  - æ’ç”Ÿä¸­å›½ä¼ä¸šæŒ‡æ•°æˆåˆ†è‚¡: {len(hscei_stocks)} åª") 
        logger.info(f"  - æ’ç”Ÿç§‘æŠ€æŒ‡æ•°æˆåˆ†è‚¡: {len(hstech_stocks)} åª")
        logger.info(f"  - å¯Œæ—¶ä¸­å›½50æŒ‡æ•°æˆåˆ†è‚¡: {len(ftse_china50_stocks)} åª")
        logger.info(f"  - å»é‡åæ€»è®¡: {len(hk_symbols)} åªæ¸¯è‚¡")
        
        return hk_symbols
    
    def _get_hk_stocks_dynamic(self):
        """åŠ¨æ€è·å–æ¸¯è‚¡æˆåˆ†è‚¡ä¿¡æ¯"""
        try:
            logger.info("å°è¯•åŠ¨æ€è·å–æ¸¯è‚¡æŒ‡æ•°æˆåˆ†è‚¡...")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ ä»æ’ç”ŸæŒ‡æ•°å…¬å¸ã€å¯Œæ—¶æŒ‡æ•°ç­‰å®˜æ–¹ç½‘ç«™çˆ¬å–æ•°æ®çš„é€»è¾‘
            # ç”±äºæ¶‰åŠç½‘é¡µç»“æ„å˜åŒ–å’Œåçˆ¬è™«ï¼Œç›®å‰è¿”å›Noneï¼Œä½¿ç”¨é™æ€åˆ—è¡¨
            logger.info("åŠ¨æ€è·å–æš‚æœªå®ç°ï¼Œä½¿ç”¨é™æ€åˆ—è¡¨")
            return None
            
        except Exception as e:
            logger.warning(f"åŠ¨æ€è·å–æ¸¯è‚¡æˆåˆ†è‚¡å¤±è´¥: {e}")
            return None

    def _get_cn_stocks_list(self):
        """è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ²ªæ·±300 + ä¸­è¯500ï¼‰"""
        logger.info("æ­£åœ¨è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ²ªæ·±300 + ä¸­è¯500ï¼‰...")
        
        # æ–¹æ³•1ï¼šä½¿ç”¨baostockè·å–å®Œæ•´æ²ªæ·±300å’Œä¸­è¯500æˆåˆ†è‚¡
        try:
            import baostock as bs
            import pandas as pd
            
            logger.info("å°è¯•ä½¿ç”¨baostockè·å–æ²ªæ·±300 + ä¸­è¯500æˆåˆ†è‚¡...")
            
            # ç™»å½•baostock
            lg = bs.login()
            if lg.error_code == '0':
                logger.info("âœ“ ç™»å½•baostockæˆåŠŸ")
                
                all_stocks = []
                
                # è·å–æ²ªæ·±300æˆåˆ†è‚¡
                logger.info("æ­£åœ¨è·å–æ²ªæ·±300æˆåˆ†è‚¡...")
                rs_hs300 = bs.query_hs300_stocks()
                if rs_hs300.error_code == '0':
                    hs300_count = 0
                    while (rs_hs300.error_code == '0') & rs_hs300.next():
                        stock_data = rs_hs300.get_row_data()
                        original_code = stock_data[1]  # baostockæ ¼å¼ï¼šsz.000001 æˆ– sh.600000
                        
                        # è½¬æ¢ä¸ºyfinanceæ ¼å¼
                        if original_code.startswith('sz.'):
                            yf_code = original_code.replace('sz.', '') + '.SZ'
                        elif original_code.startswith('sh.'):
                            yf_code = original_code.replace('sh.', '') + '.SS'
                        else:
                            continue  # è·³è¿‡æ— æ•ˆæ ¼å¼
                            
                        all_stocks.append(yf_code)
                        hs300_count += 1
                    
                    logger.info(f"âœ“ è·å–åˆ° {hs300_count} åªæ²ªæ·±300æˆåˆ†è‚¡")
                else:
                    logger.warning(f"æŸ¥è¯¢æ²ªæ·±300å¤±è´¥: {rs_hs300.error_msg}")
                
                # è·å–ä¸­è¯500æˆåˆ†è‚¡
                logger.info("æ­£åœ¨è·å–ä¸­è¯500æˆåˆ†è‚¡...")
                rs_zz500 = bs.query_zz500_stocks()
                if rs_zz500.error_code == '0':
                    zz500_count = 0
                    while (rs_zz500.error_code == '0') & rs_zz500.next():
                        stock_data = rs_zz500.get_row_data()
                        original_code = stock_data[1]  # baostockæ ¼å¼ï¼šsz.000001 æˆ– sh.600000
                        
                        # è½¬æ¢ä¸ºyfinanceæ ¼å¼
                        if original_code.startswith('sz.'):
                            yf_code = original_code.replace('sz.', '') + '.SZ'
                        elif original_code.startswith('sh.'):
                            yf_code = original_code.replace('sh.', '') + '.SS'
                        else:
                            continue  # è·³è¿‡æ— æ•ˆæ ¼å¼
                            
                        all_stocks.append(yf_code)
                        zz500_count += 1
                    
                    logger.info(f"âœ“ è·å–åˆ° {zz500_count} åªä¸­è¯500æˆåˆ†è‚¡")
                else:
                    logger.warning(f"æŸ¥è¯¢ä¸­è¯500å¤±è´¥: {rs_zz500.error_msg}")
                
                bs.logout()
                
                # å»é‡åˆå¹¶
                unique_stocks = sorted(list(set(all_stocks)))
                
                if len(unique_stocks) >= 700:  # ç¡®ä¿è·å–åˆ°è¶³å¤Ÿå¤šçš„è‚¡ç¥¨ï¼ˆé¢„æœŸçº¦800åªï¼‰
                    logger.info(f"âœ… æˆåŠŸè·å– {len(unique_stocks)} åªAè‚¡æˆåˆ†è‚¡ï¼ˆæ²ªæ·±300 + ä¸­è¯500ï¼Œå·²å»é‡ï¼‰")
                    return unique_stocks
                else:
                    logger.warning(f"è·å–çš„è‚¡ç¥¨æ•°é‡ä¸è¶³: {len(unique_stocks)}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                    
            else:
                logger.warning(f"ç™»å½•baostockå¤±è´¥: {lg.error_msg}")
                
        except ImportError:
            logger.warning("baostockæœªå®‰è£…ï¼Œä½¿ç”¨å¤‡ç”¨è‚¡ç¥¨åˆ—è¡¨")
        except Exception as e:
            logger.warning(f"ä½¿ç”¨baostockè·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        
        # æ–¹æ³•2ï¼šä½¿ç”¨tushareè·å–ï¼ˆéœ€è¦tokenï¼‰
        try:
            import tushare as ts
            logger.info("å°è¯•ä½¿ç”¨tushareè·å–æ²ªæ·±300æˆåˆ†è‚¡...")
            
            # æ³¨æ„ï¼štushareéœ€è¦tokenï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªç¤ºä¾‹
            # ç”¨æˆ·éœ€è¦åˆ° https://tushare.pro/ æ³¨å†Œè·å–token
            # ts.set_token('your_token_here')
            # pro = ts.pro_api()
            # df = pro.index_weight(index_code='000300.SH', trade_date='20241201')
            # è¿™é‡Œæš‚æ—¶è·³è¿‡tushareæ–¹æ¡ˆï¼Œå› ä¸ºéœ€è¦ç”¨æˆ·é…ç½®token
            
            logger.info("tushareéœ€è¦ç”¨æˆ·é…ç½®tokenï¼Œè·³è¿‡")
            
        except ImportError:
            logger.info("tushareæœªå®‰è£…")
        except Exception as e:
            logger.warning(f"ä½¿ç”¨tushareè·å–å¤±è´¥: {e}")
        
        # æ–¹æ³•3ï¼šå¤‡ç”¨è‚¡ç¥¨åˆ—è¡¨ï¼ˆç²¾é€‰çš„æ²ªæ·±300 + ä¸­è¯500é‡è¦æˆåˆ†è‚¡ï¼‰
        logger.info("ä½¿ç”¨å¤‡ç”¨æ²ªæ·±300 + ä¸­è¯500ä¸»è¦æˆåˆ†è‚¡åˆ—è¡¨...")
        
        # ç²¾é€‰çš„æ²ªæ·±300 + ä¸­è¯500é‡è¦æˆåˆ†è‚¡ï¼ˆæŒ‰æƒé‡å’ŒæµåŠ¨æ€§ç­›é€‰ï¼‰
        cn_symbols = [
            # æ²ªæ·±300 - æ²ªå¸‚æƒé‡è‚¡ï¼ˆé‡‘èã€èƒ½æºã€åŸºå»ºï¼‰
            '600036.SS', '600000.SS', '600016.SS', '601328.SS', '601398.SS', '601166.SS', '601318.SS',
            '601988.SS', '601288.SS', '600519.SS', '600887.SS', '600809.SS', '600028.SS', '601857.SS', '601088.SS',
            '601601.SS', '601766.SS', '601628.SS', '601669.SS', '601800.SS', '601985.SS', '601939.SS',
            '600009.SS', '600104.SS', '600115.SS', '600196.SS', '600276.SS', '600309.SS', '600346.SS', '600383.SS',
            '600406.SS', '600438.SS', '600570.SS', '600585.SS', '600588.SS', '600690.SS', '600703.SS', '600745.SS',
            '600795.SS', '600837.SS', '600867.SS', '600893.SS', '600958.SS', '600999.SS', '601006.SS', '601012.SS',
            '601066.SS', '601117.SS', '601138.SS', '601169.SS', '601186.SS', '601211.SS', '601229.SS', '601336.SS',
            '601390.SS', '601633.SS', '601658.SS', '601688.SS', '601788.SS', '601816.SS', '601818.SS', '601865.SS',
            '601888.SS', '601899.SS', '601919.SS', '601933.SS', '601989.SS', '601995.SS', '601998.SS', '600018.SS',
            '600019.SS', '600025.SS', '600027.SS', '600029.SS', '600030.SS', '600031.SS', '600048.SS', '600050.SS',
            '600061.SS', '600066.SS', '600085.SS', '600089.SS', '600111.SS', '600118.SS', '600150.SS', '600160.SS',
            '600170.SS', '600177.SS', '600188.SS', '600362.SS', '600398.SS', '600436.SS', '600547.SS', '600600.SS',
            '600606.SS', '600760.SS', '600848.SS', '600886.SS', '600010.SS', '600011.SS', '600015.SS', '600023.SS',
            
            # æ²ªæ·±300 - æ·±å¸‚æƒé‡è‚¡ï¼ˆç§‘æŠ€ã€æ¶ˆè´¹ã€åŒ»è¯ï¼‰
            '000001.SZ', '000002.SZ', '000063.SZ', '000069.SZ', '000100.SZ', '000157.SZ', '000166.SZ', '000333.SZ',
            '000338.SZ', '000401.SZ', '000402.SZ', '000538.SZ', '000568.SZ', '000596.SZ', '000625.SZ', '000627.SZ',
            '000651.SZ', '000661.SZ', '000671.SZ', '000703.SZ', '000708.SZ', '000725.SZ', '000728.SZ', '000738.SZ',
            '000776.SZ', '000783.SZ', '000792.SZ', '000858.SZ', '000876.SZ', '000895.SZ', '000938.SZ', '000961.SZ',
            '000977.SZ', '002001.SZ', '002007.SZ', '002008.SZ', '002027.SZ', '002032.SZ', '002044.SZ', '002050.SZ',
            '002142.SZ', '002146.SZ', '002152.SZ', '002230.SZ', '002236.SZ', '002241.SZ', '002271.SZ', '002304.SZ',
            '002311.SZ', '002352.SZ', '002415.SZ', '002466.SZ', '002475.SZ', '002493.SZ', '002508.SZ', '002594.SZ',
            '002601.SZ', '002602.SZ', '002714.SZ', '002736.SZ', '002837.SZ', '002916.SZ', '002938.SZ', '003816.SZ',
            '000012.SZ', '000021.SZ', '000039.SZ', '000046.SZ', '000060.SZ', '000089.SZ', '000301.SZ', '000413.SZ',
            '000423.SZ', '000425.SZ', '000503.SZ', '000513.SZ', '000629.SZ', '000636.SZ', '000656.SZ', '000709.SZ',
            '000723.SZ', '000729.SZ', '000755.SZ', '000759.SZ', '000768.SZ', '000786.SZ', '000800.SZ', '000826.SZ',
            '000839.SZ', '000860.SZ', '000877.SZ', '000898.SZ', '000917.SZ', '000932.SZ', '000959.SZ', '000963.SZ',
            
            # ä¸­è¯500 - æ²ªå¸‚æˆåˆ†è‚¡ï¼ˆè¡¥å……æ²ªæ·±300ä¹‹å¤–çš„é‡è¦è‚¡ç¥¨ï¼‰
            '600064.SS', '600068.SS', '600079.SS', '600096.SS', '600123.SS', '600125.SS', '600141.SS', '600161.SS',
            '600162.SS', '600176.SS', '600183.SS', '600202.SS', '600208.SS', '600219.SS', '600233.SS', '600251.SS',
            '600258.SS', '600266.SS', '600270.SS', '600285.SS', '600297.SS', '600307.SS', '600312.SS', '600315.SS',
            '600316.SS', '600332.SS', '600335.SS', '600340.SS', '600348.SS', '600369.SS', '600372.SS', '600377.SS',
            '600380.SS', '600387.SS', '600390.SS', '600399.SS', '600409.SS', '600410.SS', '600415.SS', '600426.SS',
            '600428.SS', '600449.SS', '600460.SS', '600466.SS', '600481.SS', '600482.SS', '600489.SS', '600498.SS',
            '600507.SS', '600511.SS', '600516.SS', '600518.SS', '600521.SS', '600522.SS', '600525.SS', '600528.SS',
            '600536.SS', '600549.SS', '600563.SS', '600566.SS', '600567.SS', '600572.SS', '600580.SS', '600583.SS',
            '600584.SS', '600590.SS', '600592.SS', '600595.SS', '600597.SS', '600598.SS', '600602.SS', '600604.SS',
            '600608.SS', '600612.SS', '600614.SS', '600615.SS', '600617.SS', '600618.SS', '600619.SS', '600622.SS',
            '600623.SS', '600628.SS', '600629.SS', '600633.SS', '600635.SS', '600637.SS', '600639.SS', '600641.SS',
            '600642.SS', '600643.SS', '600645.SS', '600649.SS', '600652.SS', '600653.SS', '600655.SS', '600658.SS',
            '600660.SS', '600662.SS', '600663.SS', '600664.SS', '600665.SS', '600673.SS', '600674.SS', '600675.SS',
            '600677.SS', '600678.SS', '600679.SS', '600682.SS', '600683.SS', '600684.SS', '600685.SS', '600686.SS',
            '600688.SS', '600689.SS', '600692.SS', '600693.SS', '600694.SS', '600695.SS', '600696.SS', '600697.SS',
            '600698.SS', '600699.SS', '600701.SS', '600702.SS', '600704.SS', '600705.SS', '600706.SS', '600707.SS',
            '600708.SS', '600710.SS', '600711.SS', '600712.SS', '600713.SS', '600714.SS', '600715.SS', '600716.SS',
            '600717.SS', '600718.SS', '600719.SS', '600720.SS', '600721.SS', '600722.SS', '600723.SS', '600724.SS',
            '600725.SS', '600726.SS', '600727.SS', '600728.SS', '600729.SS', '600730.SS', '600731.SS', '600732.SS',
            '600733.SS', '600734.SS', '600735.SS', '600736.SS', '600737.SS', '600738.SS', '600739.SS', '600741.SS',
            
            # ä¸­è¯500 - æ·±å¸‚æˆåˆ†è‚¡ï¼ˆè¡¥å……æ²ªæ·±300ä¹‹å¤–çš„é‡è¦è‚¡ç¥¨ï¼‰
            '000004.SZ', '000005.SZ', '000006.SZ', '000007.SZ', '000008.SZ', '000009.SZ', '000010.SZ', '000011.SZ',
            '000014.SZ', '000016.SZ', '000017.SZ', '000018.SZ', '000019.SZ', '000020.SZ', '000022.SZ', '000023.SZ',
            '000025.SZ', '000026.SZ', '000027.SZ', '000028.SZ', '000029.SZ', '000030.SZ', '000031.SZ', '000032.SZ',
            '000034.SZ', '000035.SZ', '000036.SZ', '000037.SZ', '000038.SZ', '000040.SZ', '000042.SZ', '000043.SZ',
            '000045.SZ', '000048.SZ', '000049.SZ', '000050.SZ', '000055.SZ', '000056.SZ', '000058.SZ', '000059.SZ',
            '000061.SZ', '000062.SZ', '000065.SZ', '000066.SZ', '000068.SZ', '000070.SZ', '000078.SZ', '000088.SZ',
            '000090.SZ', '000096.SZ', '000099.SZ', '000150.SZ', '000155.SZ', '000158.SZ', '000159.SZ', '000333.SZ',
            '000338.SZ', '000401.SZ', '000402.SZ', '000415.SZ', '000416.SZ', '000417.SZ', '000418.SZ', '000419.SZ',
            '000420.SZ', '000421.SZ', '000422.SZ', '000426.SZ', '000428.SZ', '000429.SZ', '000430.SZ', '000488.SZ',
            '000501.SZ', '000502.SZ', '000504.SZ', '000505.SZ', '000506.SZ', '000507.SZ', '000509.SZ', '000510.SZ',
            '000511.SZ', '000514.SZ', '000516.SZ', '000517.SZ', '000518.SZ', '000519.SZ', '000520.SZ', '000521.SZ',
            '000523.SZ', '000524.SZ', '000525.SZ', '000526.SZ', '000528.SZ', '000529.SZ', '000530.SZ', '000531.SZ',
            '000532.SZ', '000533.SZ', '000534.SZ', '000536.SZ', '000537.SZ', '000539.SZ', '000540.SZ', '000541.SZ',
            '000543.SZ', '000544.SZ', '000545.SZ', '000546.SZ', '000547.SZ', '000548.SZ', '000549.SZ', '000550.SZ',
            '000551.SZ', '000552.SZ', '000553.SZ', '000554.SZ', '000555.SZ', '000556.SZ', '000557.SZ', '000558.SZ',
            '000559.SZ', '000560.SZ', '000561.SZ', '000562.SZ', '000563.SZ', '000564.SZ', '000565.SZ', '000566.SZ',
            '000567.SZ', '000570.SZ', '000571.SZ', '000572.SZ', '000573.SZ', '000576.SZ', '000581.SZ', '000582.SZ',
            '000584.SZ', '000585.SZ', '000586.SZ', '000587.SZ', '000589.SZ', '000590.SZ', '000591.SZ', '000592.SZ',
            '000593.SZ', '000594.SZ', '000595.SZ', '000597.SZ', '000598.SZ', '000599.SZ', '000600.SZ', '000601.SZ',
            '000603.SZ', '000605.SZ', '000606.SZ', '000607.SZ', '000608.SZ', '000609.SZ', '000610.SZ', '000611.SZ',
            
            # åˆ›ä¸šæ¿é‡è¦æˆåˆ†è‚¡ï¼ˆä¸­è¯500éƒ¨åˆ†ï¼‰
            '300003.SZ', '300014.SZ', '300015.SZ', '300033.SZ', '300059.SZ', '300122.SZ', '300142.SZ', '300144.SZ',
            '300347.SZ', '300408.SZ', '300413.SZ', '300433.SZ', '300498.SZ', '300595.SZ', '300628.SZ', '300750.SZ',
            '300760.SZ', '300896.SZ', '300919.SZ', '300999.SZ', '301236.SZ', '301095.SZ', '301110.SZ', '301187.SZ',
            '300124.SZ', '300383.SZ', '300454.SZ', '300601.SZ', '300782.SZ', '300888.SZ', '300979.SZ',
            '300017.SZ', '300024.SZ', '300026.SZ', '300027.SZ', '300028.SZ', '300030.SZ', '300034.SZ', '300037.SZ',
            '300039.SZ', '300040.SZ', '300041.SZ', '300045.SZ', '300046.SZ', '300048.SZ', '300049.SZ', '300050.SZ',
            '300051.SZ', '300052.SZ', '300054.SZ', '300055.SZ', '300056.SZ', '300058.SZ', '300061.SZ', '300062.SZ',
            
            # ç§‘åˆ›æ¿ä¼˜è´¨è‚¡ç¥¨
            '688009.SS', '688036.SS', '688047.SS', '688065.SS', '688111.SS', '688169.SS', '688187.SS', '688223.SS',
            '688303.SS', '688598.SS', '688981.SS', '688050.SS', '688012.SS', '688126.SS', '688180.SS', '688202.SS',
            '688256.SS', '688363.SS', '688518.SS', '688700.SS', '688819.SS', '688072.SS', '688008.SS', '688099.SS',
            '688151.SS', '688199.SS', '688276.SS', '688396.SS', '688561.SS', '688733.SS', '688898.SS',
            '688016.SS', '688018.SS', '688019.SS', '688020.SS', '688021.SS', '688022.SS', '688023.SS', '688025.SS',
            '688026.SS', '688027.SS', '688028.SS', '688029.SS', '688030.SS', '688031.SS', '688032.SS', '688033.SS',
        ]
        
        # å»é‡å¹¶æ’åº
        cn_symbols = sorted(list(set(cn_symbols)))
        
        logger.info(f"âœ“ ä½¿ç”¨å¤‡ç”¨è‚¡ç¥¨åˆ—è¡¨: {len(cn_symbols)} åªAè‚¡ï¼ˆåŒ…å«æ²ªæ·±300 + ä¸­è¯500ä¸»è¦æˆåˆ†è‚¡ï¼‰")
        return cn_symbols

    def _create_us_calendar(self, start_date: str, end_date: str, target_dir: Path):
        """åˆ›å»ºç¾è‚¡äº¤æ˜“æ—¥å†"""
        logger.info("åˆ›å»ºç¾è‚¡äº¤æ˜“æ—¥å†...")
        
        if yf is None:
            logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºç¾è‚¡äº¤æ˜“æ—¥å†")
            return []
        
        # ä½¿ç”¨yfinanceè·å–AAPLçš„äº¤æ˜“æ—¥å†ï¼ˆæ›´ç¨³å®šï¼‰
        ticker = yf.Ticker("AAPL")
        hist = ticker.history(start=start_date, end=end_date, interval="1d")
        
        # è·å–äº¤æ˜“æ—¥æœŸ
        trading_dates = hist.index.to_series().dt.strftime('%Y-%m-%d').tolist()
        
        # åˆ›å»ºcalendarsç›®å½•
        calendar_dir = target_dir / "calendars"
        calendar_dir.mkdir(exist_ok=True)
        
        # å†™å…¥day.txtæ–‡ä»¶
        with open(calendar_dir / "day.txt", 'w') as f:
            for date in trading_dates:
                f.write(f"{date}\n")
        
        logger.info(f"åˆ›å»ºç¾è‚¡äº¤æ˜“æ—¥å†: {len(trading_dates)} ä¸ªäº¤æ˜“æ—¥")
        logger.info(f"æ—¥å†èŒƒå›´: {trading_dates[0]} åˆ° {trading_dates[-1]}")
        
        return trading_dates

    def _create_us_instruments(self, stocks: list, target_dir: Path):
        """åˆ›å»ºç¾è‚¡è‚¡ç¥¨åˆ—è¡¨"""
        logger.info("åˆ›å»ºç¾è‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        instruments_dir = target_dir / "instruments"
        instruments_dir.mkdir(exist_ok=True)
        
        # å†™å…¥all.txtæ–‡ä»¶
        with open(instruments_dir / "all.txt", 'w') as f:
            for stock in stocks:
                # æ ¼å¼: symbol	start_date	end_date
                f.write(f"{stock.lower()}\t1990-01-01\t2030-12-31\n")
        
        logger.info(f"åˆ›å»ºç¾è‚¡è‚¡ç¥¨åˆ—è¡¨: {len(stocks)} åªç¾è‚¡")

    def _create_hk_calendar(self, start_date: str, end_date: str, target_dir: Path):
        """åˆ›å»ºæ¸¯è‚¡äº¤æ˜“æ—¥å†"""
        logger.info("åˆ›å»ºæ¸¯è‚¡äº¤æ˜“æ—¥å†...")
        
        if yf is None:
            logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºæ¸¯è‚¡äº¤æ˜“æ—¥å†")
            return []
        
        # ä½¿ç”¨yfinanceè·å–è…¾è®¯(0700.HK)çš„äº¤æ˜“æ—¥å†ï¼ˆæ¸¯è‚¡ä»£è¡¨ï¼‰
        ticker = yf.Ticker("0700.HK")
        hist = ticker.history(start=start_date, end=end_date, interval="1d")
        
        # è·å–äº¤æ˜“æ—¥æœŸ
        trading_dates = hist.index.to_series().dt.strftime('%Y-%m-%d').tolist()
        
        # åˆ›å»ºcalendarsç›®å½•
        calendar_dir = target_dir / "calendars"
        calendar_dir.mkdir(exist_ok=True)
        
        # å†™å…¥day.txtæ–‡ä»¶
        with open(calendar_dir / "day.txt", 'w') as f:
            for date in trading_dates:
                f.write(f"{date}\n")
        
        logger.info(f"åˆ›å»ºæ¸¯è‚¡äº¤æ˜“æ—¥å†: {len(trading_dates)} ä¸ªäº¤æ˜“æ—¥")
        logger.info(f"æ—¥å†èŒƒå›´: {trading_dates[0]} åˆ° {trading_dates[-1]}")
        
        return trading_dates

    def _create_hk_instruments(self, stocks: list, target_dir: Path):
        """åˆ›å»ºæ¸¯è‚¡è‚¡ç¥¨åˆ—è¡¨"""
        logger.info("åˆ›å»ºæ¸¯è‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        instruments_dir = target_dir / "instruments"
        instruments_dir.mkdir(exist_ok=True)
        
        # å†™å…¥all.txtæ–‡ä»¶
        with open(instruments_dir / "all.txt", 'w') as f:
            for stock in stocks:
                # æ¸¯è‚¡ä»£ç è½¬æ¢ï¼š0700.HK -> 0700_hk
                symbol_qlib = stock.replace('.HK', '_hk').lower()
                # æ ¼å¼: symbol	start_date	end_date
                f.write(f"{symbol_qlib}\t1990-01-01\t2030-12-31\n")
        
        logger.info(f"åˆ›å»ºæ¸¯è‚¡è‚¡ç¥¨åˆ—è¡¨: {len(stocks)} åªæ¸¯è‚¡")

    def _create_cn_calendar(self, start_date: str, end_date: str, target_dir: Path):
        """åˆ›å»ºAè‚¡äº¤æ˜“æ—¥å†"""
        logger.info("åˆ›å»ºAè‚¡äº¤æ˜“æ—¥å†...")
        
        if yf is None:
            logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºAè‚¡äº¤æ˜“æ—¥å†")
            return []
        
        # ä½¿ç”¨yfinanceè·å–ä¸­å›½å¹³å®‰(000001.SZ)çš„äº¤æ˜“æ—¥å†ï¼ˆAè‚¡ä»£è¡¨ï¼‰
        ticker = yf.Ticker("000001.SZ")
        hist = ticker.history(start=start_date, end=end_date, interval="1d")
        
        # è·å–äº¤æ˜“æ—¥æœŸ
        trading_dates = hist.index.to_series().dt.strftime('%Y-%m-%d').tolist()
        
        # åˆ›å»ºcalendarsç›®å½•
        calendar_dir = target_dir / "calendars"
        calendar_dir.mkdir(exist_ok=True)
        
        # å†™å…¥day.txtæ–‡ä»¶
        with open(calendar_dir / "day.txt", 'w') as f:
            for date in trading_dates:
                f.write(f"{date}\n")
        
        logger.info(f"åˆ›å»ºAè‚¡äº¤æ˜“æ—¥å†: {len(trading_dates)} ä¸ªäº¤æ˜“æ—¥")
        logger.info(f"æ—¥å†èŒƒå›´: {trading_dates[0]} åˆ° {trading_dates[-1]}")
        
        return trading_dates

    def _create_cn_instruments(self, stocks: list, target_dir: Path):
        """åˆ›å»ºAè‚¡è‚¡ç¥¨åˆ—è¡¨"""
        logger.info("åˆ›å»ºAè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        instruments_dir = target_dir / "instruments"
        instruments_dir.mkdir(exist_ok=True)
        
        # å†™å…¥all.txtæ–‡ä»¶
        with open(instruments_dir / "all.txt", 'w') as f:
            for stock in stocks:
                # Aè‚¡ä»£ç è½¬æ¢ï¼š600000.SS -> 600000_ss, 000001.SZ -> 000001_sz
                symbol_qlib = stock.replace('.SS', '_ss').replace('.SZ', '_sz').lower()
                # æ ¼å¼: symbol	start_date	end_date
                f.write(f"{symbol_qlib}\t1990-01-01\t2030-12-31\n")
        
        logger.info(f"åˆ›å»ºAè‚¡è‚¡ç¥¨åˆ—è¡¨: {len(stocks)} åªAè‚¡")

    def _download_us_stock_data(self, symbol: str, start_date: str, end_date: str, 
                              target_dir: Path, incremental_update: bool = False):
        """ä¸‹è½½å•åªç¾è‚¡æ•°æ®å¹¶è½¬æ¢ä¸ºæ ‡å‡†qlibæ ¼å¼"""
        if yf is None:
            logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½ç¾è‚¡æ•°æ®")
            return False
            
        symbol_lower = symbol.lower()
        stock_dir = target_dir / "features" / symbol_lower
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
            existing_data = None
            csv_file = target_dir / "features" / f"{symbol}.csv"
            if incremental_update and csv_file.exists():
                existing_data = pd.read_csv(csv_file)
                if 'Date' in existing_data.columns:
                    existing_data['Date'] = pd.to_datetime(existing_data['Date'], utc=True).dt.tz_convert(None)
                    latest_date = existing_data['Date'].max()
                    logger.info(f"    ğŸ“ å‘ç°ç°æœ‰æ•°æ®: {symbol} ({len(existing_data)} æ¡è®°å½•)")
                    logger.info(f"        ç°æœ‰æ•°æ®èŒƒå›´: {existing_data['Date'].min()} åˆ° {latest_date}")
                    
                    # è°ƒæ•´å¼€å§‹æ—¥æœŸä¸ºæœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©
                    new_start = (latest_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
                    if new_start >= end_date:
                        logger.info(f"    âœ… {symbol} æ•°æ®å·²æ˜¯æœ€æ–°")
                        return self._convert_to_qlib_format(existing_data, stock_dir, symbol)
                    else:
                        logger.info(f"    ğŸ”„ {symbol} éœ€è¦è¡¥å……æ•°æ®: {new_start} åˆ° {end_date}")
                        start_date = new_start
            
            # ä¸‹è½½æ•°æ®
            ticker = yf.Ticker(symbol)
            data = ticker.history(start=start_date, end=end_date, interval="1d")
            
            if data.empty:
                logger.warning(f"    âŒ {symbol} æ²¡æœ‰è·å–åˆ°æ•°æ®")
                return False
            
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼
            df = data.reset_index()
            
            # å¤„ç†ç¾è‚¡çš„æ—¶åŒºä¿¡æ¯ï¼ˆç§»é™¤æ—¶åŒºï¼Œä¿ç•™æ—¥æœŸï¼‰
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date']).dt.tz_convert(None)
            
            # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œåˆå¹¶æ•°æ®
            if existing_data is not None and not existing_data.empty:
                df = pd.concat([existing_data, df], ignore_index=True)
                df = df.drop_duplicates(subset=['Date'], keep='last')
                df = df.sort_values('Date').reset_index(drop=True)
            
            logger.info(f"    ğŸ“Š {symbol} æ–°æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # ä¿å­˜CSVæ–‡ä»¶ï¼ˆç”¨äºåç»­å¢é‡æ›´æ–°ï¼‰
            csv_file = target_dir / "features" / f"{symbol}.csv"
            df.to_csv(csv_file, index=False)
            
            # è½¬æ¢ä¸ºæ ‡å‡†qlibæ ¼å¼
            success = self._convert_to_qlib_format(df, stock_dir, symbol)
            
            if success:
                logger.info(f"    âœ“ {symbol} ä¸‹è½½æˆåŠŸ: {len(df)} æ¡è®°å½• (CSV + äºŒè¿›åˆ¶)")
                return True
            else:
                logger.error(f"    âŒ {symbol} è½¬æ¢æ ¼å¼å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"    âŒ {symbol} ä¸‹è½½å¤±è´¥: {e}")
            return False

    def _download_hk_stock_data(self, symbol: str, start_date: str, end_date: str, 
                              target_dir: Path, incremental_update: bool = False):
        """ä¸‹è½½å•åªæ¸¯è‚¡æ•°æ®å¹¶è½¬æ¢ä¸ºæ ‡å‡†qlibæ ¼å¼"""
        if yf is None:
            logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½æ¸¯è‚¡æ•°æ®")
            return False
            
        # æ¸¯è‚¡ä»£ç è½¬æ¢ï¼š0700.HK -> 0700_hkï¼ˆç”¨äºç›®å½•åå’Œæ–‡ä»¶åï¼‰
        symbol_qlib = symbol.replace('.HK', '_hk').lower()
        stock_dir = target_dir / "features" / symbol_qlib
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
            existing_data = None
            csv_file = target_dir / "features" / f"{symbol}.csv"
            if incremental_update and csv_file.exists():
                existing_data = pd.read_csv(csv_file)
                if 'Date' in existing_data.columns:
                    existing_data['Date'] = pd.to_datetime(existing_data['Date'], utc=True).dt.tz_convert(None)
                    latest_date = existing_data['Date'].max()
                    logger.info(f"    ğŸ“ å‘ç°ç°æœ‰æ•°æ®: {symbol} ({len(existing_data)} æ¡è®°å½•)")
                    logger.info(f"        ç°æœ‰æ•°æ®èŒƒå›´: {existing_data['Date'].min()} åˆ° {latest_date}")
                    
                    # è°ƒæ•´å¼€å§‹æ—¥æœŸä¸ºæœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©
                    new_start = (latest_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
                    if new_start >= end_date:
                        logger.info(f"    âœ… {symbol} æ•°æ®å·²æ˜¯æœ€æ–°")
                        return self._convert_to_qlib_format(existing_data, stock_dir, symbol)
                    else:
                        logger.info(f"    ğŸ”„ {symbol} éœ€è¦è¡¥å……æ•°æ®: {new_start} åˆ° {end_date}")
                        start_date = new_start
            
            # ä¸‹è½½æ•°æ®
            ticker = yf.Ticker(symbol)
            data = ticker.history(start=start_date, end=end_date, interval="1d")
            
            if data.empty:
                logger.warning(f"    âŒ {symbol} æ²¡æœ‰è·å–åˆ°æ•°æ®")
                return False
            
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼
            df = data.reset_index()
            
            # å¤„ç†æ¸¯è‚¡çš„æ—¶åŒºä¿¡æ¯ï¼ˆç§»é™¤æ—¶åŒºï¼Œä¿ç•™æ—¥æœŸï¼‰
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date']).dt.date
                df['Date'] = pd.to_datetime(df['Date'])
            
            # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œåˆå¹¶æ•°æ®
            if existing_data is not None and not existing_data.empty:
                df = pd.concat([existing_data, df], ignore_index=True)
                df = df.drop_duplicates(subset=['Date'], keep='last')
                df = df.sort_values('Date').reset_index(drop=True)
            
            logger.info(f"    ğŸ“Š {symbol} æ–°æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # ä¿å­˜CSVæ–‡ä»¶ï¼ˆç”¨äºåç»­å¢é‡æ›´æ–°ï¼‰
            csv_file = target_dir / "features" / f"{symbol}.csv"
            df.to_csv(csv_file, index=False)
            
            # è½¬æ¢ä¸ºæ ‡å‡†qlibæ ¼å¼
            success = self._convert_to_qlib_format(df, stock_dir, symbol)
            
            if success:
                logger.info(f"    âœ“ {symbol} ä¸‹è½½æˆåŠŸ: {len(df)} æ¡è®°å½• (CSV + äºŒè¿›åˆ¶)")
                return True
            else:
                logger.error(f"    âŒ {symbol} è½¬æ¢æ ¼å¼å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"    âŒ {symbol} ä¸‹è½½å¤±è´¥: {e}")
            return False

    def _download_cn_stock_data(self, symbol: str, start_date: str, end_date: str, 
                              target_dir: Path, incremental_update: bool = False):
        """ä¸‹è½½å•åªAè‚¡æ•°æ®å¹¶è½¬æ¢ä¸ºæ ‡å‡†qlibæ ¼å¼"""
        if yf is None:
            logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½Aè‚¡æ•°æ®")
            return False
            
        # Aè‚¡ä»£ç è½¬æ¢ï¼š600000.SS -> 600000_ssï¼ˆç”¨äºç›®å½•åå’Œæ–‡ä»¶åï¼‰
        symbol_qlib = symbol.replace('.SS', '_ss').replace('.SZ', '_sz').lower()
        stock_dir = target_dir / "features" / symbol_qlib
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
            existing_data = None
            csv_file = target_dir / "features" / f"{symbol}.csv"
            if incremental_update and csv_file.exists():
                existing_data = pd.read_csv(csv_file)
                if 'Date' in existing_data.columns:
                    existing_data['Date'] = pd.to_datetime(existing_data['Date'], utc=True).dt.tz_convert(None)
                    latest_date = existing_data['Date'].max()
                    logger.info(f"    ğŸ“ å‘ç°ç°æœ‰æ•°æ®: {symbol} ({len(existing_data)} æ¡è®°å½•)")
                    logger.info(f"        ç°æœ‰æ•°æ®èŒƒå›´: {existing_data['Date'].min()} åˆ° {latest_date}")
                    
                    # è°ƒæ•´å¼€å§‹æ—¥æœŸä¸ºæœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©
                    new_start = (latest_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
                    if new_start >= end_date:
                        logger.info(f"    âœ… {symbol} æ•°æ®å·²æ˜¯æœ€æ–°")
                        return self._convert_to_qlib_format(existing_data, stock_dir, symbol)
                    else:
                        logger.info(f"    ğŸ”„ {symbol} éœ€è¦è¡¥å……æ•°æ®: {new_start} åˆ° {end_date}")
                        start_date = new_start
            
            # ä¸‹è½½æ•°æ®
            ticker = yf.Ticker(symbol)
            data = ticker.history(start=start_date, end=end_date, interval="1d")
            
            if data.empty:
                logger.warning(f"    âŒ {symbol} æ²¡æœ‰è·å–åˆ°æ•°æ®")
                return False
            
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼
            df = data.reset_index()
            
            # å¤„ç†Aè‚¡çš„æ—¶åŒºä¿¡æ¯ï¼ˆç§»é™¤æ—¶åŒºï¼Œä¿ç•™æ—¥æœŸï¼‰
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date']).dt.date
                df['Date'] = pd.to_datetime(df['Date'])
            
            # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œåˆå¹¶æ•°æ®
            if existing_data is not None and not existing_data.empty:
                df = pd.concat([existing_data, df], ignore_index=True)
                df = df.drop_duplicates(subset=['Date'], keep='last')
                df = df.sort_values('Date').reset_index(drop=True)
            
            logger.info(f"    ğŸ“Š {symbol} æ–°æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # ä¿å­˜CSVæ–‡ä»¶ï¼ˆç”¨äºåç»­å¢é‡æ›´æ–°ï¼‰
            csv_file = target_dir / "features" / f"{symbol}.csv"
            df.to_csv(csv_file, index=False)
            
            # è½¬æ¢ä¸ºæ ‡å‡†qlibæ ¼å¼
            success = self._convert_to_qlib_format(df, stock_dir, symbol)
            
            if success:
                logger.info(f"    âœ“ {symbol} ä¸‹è½½æˆåŠŸ: {len(df)} æ¡è®°å½• (CSV + äºŒè¿›åˆ¶)")
                return True
            else:
                logger.error(f"    âŒ {symbol} è½¬æ¢æ ¼å¼å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"    âŒ {symbol} ä¸‹è½½å¤±è´¥: {e}")
            return False

    def _convert_to_qlib_format(self, df: pd.DataFrame, stock_dir: Path, symbol: str):
        """å°†DataFrameè½¬æ¢ä¸ºæ ‡å‡†qlibæ ¼å¼"""
        try:
            # åˆ›å»ºè‚¡ç¥¨ç›®å½•
            stock_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤„ç†æ—¥æœŸç´¢å¼•
            if 'Date' in df.columns:
                # ç¡®ä¿æ—¥æœŸæ²¡æœ‰æ—¶åŒºä¿¡æ¯
                df['Date'] = pd.to_datetime(df['Date'])
                if df['Date'].dt.tz is not None:
                    df['Date'] = df['Date'].dt.tz_convert(None)
                df.set_index('Date', inplace=True)
            
            # è®¡ç®—é¢å¤–ç‰¹å¾
            df['Change'] = df['Close'].pct_change().fillna(0)
            df['Factor'] = 1.0  # å¤æƒå› å­ç®€åŒ–ä¸º1
            
            # å®šä¹‰ç‰¹å¾æ˜ å°„
            feature_mapping = {
                'open': 'Open',
                'high': 'High', 
                'low': 'Low',
                'close': 'Close',
                'volume': 'Volume',
                'change': 'Change',
                'factor': 'Factor'
            }
            
            # ç”ŸæˆäºŒè¿›åˆ¶æ–‡ä»¶
            for feature_name, column_name in feature_mapping.items():
                if column_name in df.columns:
                    values = df[column_name].values.astype(np.float32)
                    
                    # æ›¿æ¢æ— ç©·å¤§å’ŒNaNä¸º0
                    values = np.nan_to_num(values, nan=0.0, posinf=0.0, neginf=0.0)
                    
                    # å†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶
                    bin_file = stock_dir / f"{feature_name}.day.bin"
                    with open(bin_file, 'wb') as f:
                        for value in values:
                            f.write(struct.pack('<f', value))  # å°ç«¯åºfloat32
            
            return True
            
        except Exception as e:
            logger.error(f"è½¬æ¢æ ¼å¼å¤±è´¥ {symbol}: {e}")
            return False

    def qlib_data(
        self,
        name="qlib_data",
        target_dir="~/.qlib/qlib_data/cn_data",
        version=None,
        interval="1d",
        region="cn",
        delete_old=True,
        exists_skip=False,
        trading_date=None,
        end_date=None,
        incremental_update=False,
        cn_realtime=False,
    ):
        """
        å¢å¼ºç‰ˆæ•°æ®ä¸‹è½½æ–¹æ³•
        
        Parameters
        ----------
        target_dir: str
            data save directory
        name: str
            dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data
        version: str
            data version, value from [v1, ...], by default None(use script to specify version)
        interval: str
            data freq, value from [1d], by default 1d
        region: str
            data region, value from [cn, us, hk], by default cn
        delete_old: bool
            delete an existing directory, by default True
        exists_skip: bool
            exists skip, by default False
        trading_date: str
            start date for data download (YYYYMMDD format)
        end_date: str
            end date for data download (YYYYMMDD format)
        incremental_update: bool
            enable incremental update mode, by default False
        cn_realtime: bool
            use real-time download for China stocks (like US/HK), by default False (use official data package)
        """
        
        if exists_skip and exists_qlib_data(target_dir):
            logger.warning(
                f"Data already exists: {target_dir}, the data download will be skipped\n"
                f"\tIf downloading is required: `exists_skip=False` or `change target_dir`"
            )
            return

        # å¤„ç†æ—¥æœŸå‚æ•°
        if trading_date:
            try:
                start_date = pd.to_datetime(str(trading_date), format='%Y%m%d').strftime('%Y-%m-%d')
            except:
                logger.warning(f"æ— æ•ˆçš„trading_dateæ ¼å¼: {trading_date}ï¼Œä½¿ç”¨é»˜è®¤æ—¥æœŸ: 19900101")
                start_date = "1990-01-01"
        else:
            start_date = "1990-01-01"
            
        if end_date:
            try:
                end_date_formatted = pd.to_datetime(str(end_date), format='%Y%m%d').strftime('%Y-%m-%d')
            except:
                logger.warning(f"æ— æ•ˆçš„end_dateæ ¼å¼: {end_date}ï¼Œä½¿ç”¨é»˜è®¤æ—¥æœŸ: 20241231")
                end_date_formatted = "2024-12-31"
        else:
            end_date_formatted = "2024-12-31"

        target_path = Path(target_dir).expanduser()
        target_path.mkdir(parents=True, exist_ok=True)

        if region.lower() == "us":
            # ç¾è‚¡æ•°æ®å¤„ç†
            logger.info("============================================================")
            logger.info("ç¾è‚¡æ•°æ®å®æ—¶ä¸‹è½½å·¥å…·")
            logger.info("============================================================")
            logger.info(f"è¾“å‡ºç›®å½•: {target_dir}")
            logger.info(f"æ—¶é—´èŒƒå›´: {trading_date or '19900101'} åˆ° {end_date or '20241231'} (YYYYMMDDæ ¼å¼)")
            logger.info(f"yfinanceæ ¼å¼: {start_date} åˆ° {end_date_formatted}")
            logger.info(f"æ•°æ®é—´éš”: {interval}")
            logger.info(f"å¢é‡æ›´æ–°æ¨¡å¼: {'æ˜¯' if incremental_update else 'å¦'}")
            logger.info(f"è‡ªåŠ¨è½¬æ¢æ ¼å¼: æ˜¯")
            logger.info("============================================================")
            
            # è·å–ç¾è‚¡åˆ—è¡¨
            stocks = self._get_us_stocks_list()
            logger.info(f"å‡†å¤‡ä¸‹è½½ {len(stocks)} åªç¾è‚¡çš„æ•°æ®...")
            
            # åˆ›å»ºç›®å½•ç»“æ„
            features_dir = target_path / "features"
            features_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºäº¤æ˜“æ—¥å†
            self._create_us_calendar(start_date, end_date_formatted, target_path)
            
            # åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨
            self._create_us_instruments(stocks, target_path)
            
            # ä¸‹è½½è‚¡ç¥¨æ•°æ®
            success_count = 0
            for i, stock in enumerate(stocks, 1):
                logger.info(f"[{i}/{len(stocks)}] æ­£åœ¨ä¸‹è½½ {stock}...")
                
                success = self._download_us_stock_data(
                    stock, start_date, end_date_formatted, target_path, incremental_update
                )
                
                if success:
                    success_count += 1
                
                # æ¯ä¸‹è½½10åªè‚¡ç¥¨ä¼‘æ¯5ç§’
                if i % 10 == 0:
                    logger.info(f"    å·²å®Œæˆ {i}/{len(stocks)}ï¼Œä¼‘æ¯ 5 ç§’...")
                    time.sleep(5)
                else:
                    time.sleep(1.5)  # åŸºæœ¬å»¶è¿Ÿ
            
            logger.info(f"âœ… ç¾è‚¡æ•°æ®ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {success_count}/{len(stocks)}")
            
        elif region.lower() == "hk":
            # æ¸¯è‚¡æ•°æ®å¤„ç†
            logger.info("============================================================")
            logger.info("æ¸¯è‚¡æ•°æ®å®æ—¶ä¸‹è½½å·¥å…·")
            logger.info("============================================================")
            logger.info(f"è¾“å‡ºç›®å½•: {target_dir}")
            logger.info(f"æ—¶é—´èŒƒå›´: {trading_date or '20200101'} åˆ° {end_date or '20241231'} (YYYYMMDDæ ¼å¼)")
            logger.info(f"yfinanceæ ¼å¼: {start_date} åˆ° {end_date_formatted}")
            logger.info(f"æ•°æ®é—´éš”: {interval}")
            logger.info(f"å¢é‡æ›´æ–°æ¨¡å¼: {'æ˜¯' if incremental_update else 'å¦'}")
            logger.info(f"è‡ªåŠ¨è½¬æ¢æ ¼å¼: æ˜¯")
            logger.info("============================================================")
            
            # è·å–æ¸¯è‚¡åˆ—è¡¨
            stocks = self._get_hk_stocks_list()
            logger.info(f"å‡†å¤‡ä¸‹è½½ {len(stocks)} åªæ¸¯è‚¡çš„æ•°æ®...")
            
            # åˆ›å»ºç›®å½•ç»“æ„
            features_dir = target_path / "features"
            features_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºäº¤æ˜“æ—¥å†
            self._create_hk_calendar(start_date, end_date_formatted, target_path)
            
            # åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨
            self._create_hk_instruments(stocks, target_path)
            
            # ä¸‹è½½è‚¡ç¥¨æ•°æ®
            success_count = 0
            for i, stock in enumerate(stocks, 1):
                logger.info(f"[{i}/{len(stocks)}] æ­£åœ¨ä¸‹è½½ {stock}...")
                
                success = self._download_hk_stock_data(
                    stock, start_date, end_date_formatted, target_path, incremental_update
                )
                
                if success:
                    success_count += 1
                
                # æ¯ä¸‹è½½10åªè‚¡ç¥¨ä¼‘æ¯5ç§’
                if i % 10 == 0:
                    logger.info(f"    å·²å®Œæˆ {i}/{len(stocks)}ï¼Œä¼‘æ¯ 5 ç§’...")
                    time.sleep(5)
                else:
                    time.sleep(1.5)  # åŸºæœ¬å»¶è¿Ÿ
            
            logger.info(f"âœ… æ¸¯è‚¡æ•°æ®ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {success_count}/{len(stocks)}")
            
        elif region.lower() == "cn" and cn_realtime:
            # ä¸­å›½æ•°æ®å®æ—¶ä¸‹è½½å¤„ç†
            logger.info("============================================================")
            logger.info("ä¸­å›½è‚¡ç¥¨æ•°æ®å®æ—¶ä¸‹è½½å·¥å…·")
            logger.info("============================================================")
            logger.info(f"è¾“å‡ºç›®å½•: {target_dir}")
            logger.info(f"æ—¶é—´èŒƒå›´: {trading_date or '20200101'} åˆ° {end_date or '20241231'} (YYYYMMDDæ ¼å¼)")
            logger.info(f"yfinanceæ ¼å¼: {start_date} åˆ° {end_date_formatted}")
            logger.info(f"æ•°æ®é—´éš”: {interval}")
            logger.info(f"å¢é‡æ›´æ–°æ¨¡å¼: {'æ˜¯' if incremental_update else 'å¦'}")
            logger.info(f"è‡ªåŠ¨è½¬æ¢æ ¼å¼: æ˜¯")
            logger.info("============================================================")
            
            # è·å–Aè‚¡åˆ—è¡¨
            stocks = self._get_cn_stocks_list()
            logger.info(f"å‡†å¤‡ä¸‹è½½ {len(stocks)} åªAè‚¡çš„æ•°æ®...")
            
            # åˆ›å»ºç›®å½•ç»“æ„
            features_dir = target_path / "features"
            features_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºäº¤æ˜“æ—¥å†
            self._create_cn_calendar(start_date, end_date_formatted, target_path)
            
            # åˆ›å»ºè‚¡ç¥¨åˆ—è¡¨
            self._create_cn_instruments(stocks, target_path)
            
            # ä¸‹è½½è‚¡ç¥¨æ•°æ®
            success_count = 0
            for i, stock in enumerate(stocks, 1):
                logger.info(f"[{i}/{len(stocks)}] æ­£åœ¨ä¸‹è½½ {stock}...")
                
                success = self._download_cn_stock_data(
                    stock, start_date, end_date_formatted, target_path, incremental_update
                )
                
                if success:
                    success_count += 1
                
                # æ¯ä¸‹è½½10åªè‚¡ç¥¨ä¼‘æ¯5ç§’
                if i % 10 == 0:
                    logger.info(f"    å·²å®Œæˆ {i}/{len(stocks)}ï¼Œä¼‘æ¯ 5 ç§’...")
                    time.sleep(5)
                else:
                    time.sleep(1.5)  # åŸºæœ¬å»¶è¿Ÿ
            
            logger.info(f"âœ… Aè‚¡æ•°æ®ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {success_count}/{len(stocks)}")
            
        else:
            # ä¸­å›½æ•°æ®å¤„ç†ï¼ˆä½¿ç”¨åŸæœ‰é€»è¾‘ï¼‰
            logger.info("============================================================")
            logger.info("ä¸­å›½è‚¡ç¥¨æ•°æ®å®æ—¶ä¸‹è½½å·¥å…·")
            logger.info("============================================================")
            logger.info(f"è¾“å‡ºç›®å½•: {target_dir}")
            logger.info(f"æ—¶é—´èŒƒå›´: {trading_date or '20200101'} åˆ° {end_date or '20241231'} (YYYYMMDDæ ¼å¼)")
            logger.info(f"yfinanceæ ¼å¼: {start_date} åˆ° {end_date_formatted}")
            logger.info(f"æ•°æ®é—´éš”: {interval}")
            logger.info(f"å¢é‡æ›´æ–°æ¨¡å¼: {'æ˜¯' if incremental_update else 'å¦'}")
            logger.info(f"è‡ªåŠ¨è½¬æ¢æ ¼å¼: æ˜¯")
            logger.info("============================================================")
            
            # ä½¿ç”¨æ ‡å‡†qlibä¸‹è½½é€»è¾‘
            qlib_version = ".".join(re.findall(r"(\d+)\.+", qlib.__version__))

            def _get_file_name_with_version(qlib_version, dataset_version):
                dataset_version = "v2" if dataset_version is None else dataset_version
                file_name_with_version = f"{dataset_version}/{name}_{region.lower()}_{interval.lower()}_{qlib_version}.zip"
                return file_name_with_version

            file_name = _get_file_name_with_version(qlib_version, dataset_version=version)
            if not self.check_dataset(file_name):
                file_name = _get_file_name_with_version("latest", dataset_version=version)
            self.download_data(file_name.lower(), target_dir, delete_old)

    def download_financial_data(
        self,
        target_dir="~/.qlib/financial_data",
        region="us",
        data_types=None,
        stock_symbols=None,
        max_stocks=None,
        max_workers=5,
        save_format="csv",
        include_ratios=True,
    ):
        """
        ä¸‹è½½è´¢åŠ¡æ•°æ®ï¼šåŸºæœ¬é¢ä¿¡æ¯ã€è´¢åŠ¡æŠ¥è¡¨ã€èµ„äº§è´Ÿå€ºè¡¨ã€ç°é‡‘æµé‡è¡¨ç­‰
        
        Parameters
        ----------
        target_dir: str
            è´¢åŠ¡æ•°æ®ä¿å­˜ç›®å½•
        region: str  
            å¸‚åœºåŒºåŸŸï¼Œæ”¯æŒ ['us', 'hk', 'cn']
        data_types: list
            è¦ä¸‹è½½çš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒ:
            ['info', 'financials', 'balance_sheet', 'cashflow', 'dividends', 
             'splits', 'recommendations', 'institutional_holders', 'earnings']
        stock_symbols: list
            æŒ‡å®šè‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨
        max_stocks: int or None
            æœ€å¤§è‚¡ç¥¨æ•°é‡é™åˆ¶ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸‹è½½æ‰€æœ‰å¯ç”¨è‚¡ç¥¨
            é»˜è®¤ä¸ºNone(ä¸‹è½½æ‰€æœ‰)ï¼Œå»ºè®®é¦–æ¬¡ä½¿ç”¨æ—¶è®¾ç½®è¾ƒå°å€¼è¿›è¡Œæµ‹è¯•
        max_workers: int
            å¹¶è¡Œä¸‹è½½çº¿ç¨‹æ•°
        save_format: str
            ä¿å­˜æ ¼å¼ ['csv', 'pickle', 'json']
        include_ratios: bool
            æ˜¯å¦è®¡ç®—è´¢åŠ¡æ¯”ç‡
        """
        
        if yf is None:
            logger.error("yfinanceæœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½è´¢åŠ¡æ•°æ®")
            return False
            
        # é»˜è®¤æ•°æ®ç±»å‹
        if data_types is None:
            data_types = ['info', 'financials', 'balance_sheet', 'cashflow', 'dividends']
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        if stock_symbols is None:
            if region.lower() == "us":
                all_stocks = self._get_us_stocks_list()
                stock_symbols = all_stocks[:max_stocks] if max_stocks else all_stocks
            elif region.lower() == "hk":
                all_stocks = self._get_hk_stocks_list()
                stock_symbols = all_stocks[:max_stocks] if max_stocks else all_stocks
            elif region.lower() == "cn":
                all_stocks = self._get_cn_stocks_list()
                stock_symbols = all_stocks[:max_stocks] if max_stocks else all_stocks
            else:
                logger.error(f"ä¸æ”¯æŒçš„å¸‚åœºåŒºåŸŸ: {region}")
                return False
        else:
            # å¦‚æœç”¨æˆ·æä¾›äº†è‚¡ç¥¨åˆ—è¡¨ï¼Œä»å¯åº”ç”¨max_stocksé™åˆ¶
            if max_stocks and len(stock_symbols) > max_stocks:
                stock_symbols = stock_symbols[:max_stocks]
                logger.info(f"åº”ç”¨max_stocksé™åˆ¶ï¼Œå°†ä¸‹è½½å‰{max_stocks}åªè‚¡ç¥¨")
        
        target_path = Path(target_dir).expanduser()
        target_path.mkdir(parents=True, exist_ok=True)
        
        logger.info("=" * 60)
        logger.info(f"è´¢åŠ¡æ•°æ®ä¸‹è½½å·¥å…· - {region.upper()}å¸‚åœº")
        logger.info("=" * 60)
        logger.info(f"ç›®æ ‡ç›®å½•: {target_dir}")
        logger.info(f"æ•°æ®ç±»å‹: {data_types}")
        logger.info(f"è‚¡ç¥¨æ•°é‡: {len(stock_symbols)}")
        logger.info(f"å¹¶è¡Œçº¿ç¨‹: {max_workers}")
        logger.info(f"ä¿å­˜æ ¼å¼: {save_format}")
        logger.info(f"è®¡ç®—æ¯”ç‡: {include_ratios}")
        logger.info("=" * 60)
        
        # ä¸ºæ¯ç§æ•°æ®ç±»å‹åˆ›å»ºç›®å½•
        for data_type in data_types:
            (target_path / data_type).mkdir(exist_ok=True)
        
        # å¹¶è¡Œä¸‹è½½è´¢åŠ¡æ•°æ®
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import time
        
        def download_single_stock_financial(symbol):
            """ä¸‹è½½å•åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®"""
            try:
                # å¤„ç†è‚¡ç¥¨ä»£ç æ ¼å¼
                if region.lower() == "hk" and not symbol.endswith(".HK"):
                    ticker_symbol = f"{symbol}.HK"
                elif region.lower() == "cn" and not (symbol.endswith(".SS") or symbol.endswith(".SZ")):
                    # ç®€å•åˆ¤æ–­ï¼Œå®é™…åº”è¯¥æ ¹æ®äº¤æ˜“æ‰€æ¥ç¡®å®š
                    ticker_symbol = f"{symbol}.SS"
                else:
                    ticker_symbol = symbol
                
                stock = yf.Ticker(ticker_symbol)
                results = {}
                
                for data_type in data_types:
                    try:
                        if data_type == 'info':
                            data = stock.info
                            if data:
                                df = pd.DataFrame([data])
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                results[data_type] = df
                        
                        elif data_type == 'financials':
                            data = stock.financials
                            if not data.empty:
                                df = data.T  # è½¬ç½®ä½¿æ—¥æœŸæˆä¸ºè¡Œ
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                results[data_type] = df
                        
                        elif data_type == 'balance_sheet':
                            data = stock.balance_sheet
                            if not data.empty:
                                df = data.T
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                results[data_type] = df
                        
                        elif data_type == 'cashflow':
                            data = stock.cashflow
                            if not data.empty:
                                df = data.T
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                results[data_type] = df
                        
                        elif data_type == 'dividends':
                            data = stock.dividends
                            if not data.empty:
                                df = data.to_frame('Dividend')
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                df.reset_index(inplace=True)
                                results[data_type] = df
                        
                        elif data_type == 'splits':
                            data = stock.splits
                            if not data.empty:
                                df = data.to_frame('Split')
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                df.reset_index(inplace=True)
                                results[data_type] = df
                        
                        elif data_type == 'recommendations':
                            data = stock.recommendations
                            if data is not None and not data.empty:
                                df = data.copy()
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                results[data_type] = df
                        
                        elif data_type == 'institutional_holders':
                            data = stock.institutional_holders
                            if data is not None and not data.empty:
                                df = data.copy()
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                results[data_type] = df
                        
                        elif data_type == 'earnings':
                            data = stock.earnings
                            if not data.empty:
                                df = data.copy()
                                df['Symbol'] = symbol
                                df['UpdateTime'] = pd.Timestamp.now()
                                df.reset_index(inplace=True)
                                results[data_type] = df
                    
                    except Exception as e:
                        logger.warning(f"è·å– {symbol} çš„ {data_type} æ•°æ®å¤±è´¥: {e}")
                        continue
                
                # è®¡ç®—è´¢åŠ¡æ¯”ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if include_ratios and 'info' in results and 'financials' in results and 'balance_sheet' in results:
                    try:
                        ratios_df = self._calculate_financial_ratios(
                            results.get('info'),
                            results.get('financials'), 
                            results.get('balance_sheet'),
                            symbol
                        )
                        if ratios_df is not None:
                            results['financial_ratios'] = ratios_df
                    except Exception as e:
                        logger.warning(f"è®¡ç®— {symbol} è´¢åŠ¡æ¯”ç‡å¤±è´¥: {e}")
                
                return symbol, results
                
            except Exception as e:
                logger.error(f"ä¸‹è½½ {symbol} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
                return symbol, {}
        
        # æ‰§è¡Œå¹¶è¡Œä¸‹è½½
        success_count = 0
        total_files_saved = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {executor.submit(download_single_stock_financial, symbol): symbol 
                              for symbol in stock_symbols}
            
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    symbol, results = future.result()
                    
                    if results:
                        # ä¿å­˜æ•°æ®
                        files_saved = self._save_financial_data(results, target_path, symbol, save_format)
                        total_files_saved += files_saved
                        success_count += 1
                        logger.info(f"âœ… {symbol}: æˆåŠŸä¿å­˜ {files_saved} ä¸ªæ–‡ä»¶")
                    else:
                        logger.warning(f"âŒ {symbol}: æœªè·å–åˆ°ä»»ä½•æ•°æ®")
                
                except Exception as e:
                    logger.error(f"âŒ {symbol}: å¤„ç†å¤±è´¥ - {e}")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(0.5)
        
        logger.info("=" * 60)
        logger.info(f"âœ… è´¢åŠ¡æ•°æ®ä¸‹è½½å®Œæˆï¼")
        logger.info(f"æˆåŠŸè‚¡ç¥¨: {success_count}/{len(stock_symbols)}")
        logger.info(f"æ€»æ–‡ä»¶æ•°: {total_files_saved}")
        logger.info(f"æ•°æ®ç›®å½•: {target_path}")
        logger.info("=" * 60)
        
        return True

    def _calculate_financial_ratios(self, info_df, financials_df, balance_sheet_df, symbol):
        """è®¡ç®—è´¢åŠ¡æ¯”ç‡"""
        try:
            if info_df is None or financials_df is None or balance_sheet_df is None:
                return None
            
            ratios = {}
            
            # ä»infoè·å–åŸºæœ¬ä¿¡æ¯
            if not info_df.empty:
                market_cap = info_df.get('marketCap', {}).iloc[0] if 'marketCap' in info_df.columns else None
                shares_outstanding = info_df.get('sharesOutstanding', {}).iloc[0] if 'sharesOutstanding' in info_df.columns else None
                book_value = info_df.get('bookValue', {}).iloc[0] if 'bookValue' in info_df.columns else None
                current_price = info_df.get('currentPrice', {}).iloc[0] if 'currentPrice' in info_df.columns else None
                
                # åŸºæœ¬æ¯”ç‡
                if market_cap:
                    ratios['MarketCap'] = market_cap
                if current_price and book_value and book_value > 0:
                    ratios['PriceToBook'] = current_price / book_value
                if shares_outstanding:
                    ratios['SharesOutstanding'] = shares_outstanding
            
            # ä»è´¢åŠ¡æŠ¥è¡¨è®¡ç®—æ¯”ç‡
            if not financials_df.empty:
                # è·å–æœ€æ–°å¹´åº¦æ•°æ®
                latest_financials = financials_df.iloc[0] if len(financials_df) > 0 else None
                
                if latest_financials is not None:
                    total_revenue = latest_financials.get('Total Revenue')
                    net_income = latest_financials.get('Net Income')
                    
                    if total_revenue and net_income and total_revenue > 0:
                        ratios['NetProfitMargin'] = net_income / total_revenue
                    
                    if market_cap and total_revenue and total_revenue > 0:
                        ratios['PriceToSales'] = market_cap / total_revenue
                    
                    if market_cap and net_income and net_income > 0:
                        ratios['PriceToEarnings'] = market_cap / net_income
            
            # ä»èµ„äº§è´Ÿå€ºè¡¨è®¡ç®—æ¯”ç‡  
            if not balance_sheet_df.empty:
                latest_balance = balance_sheet_df.iloc[0] if len(balance_sheet_df) > 0 else None
                
                if latest_balance is not None:
                    total_assets = latest_balance.get('Total Assets')
                    total_debt = latest_balance.get('Total Debt')
                    stockholder_equity = latest_balance.get('Stockholders Equity')
                    current_assets = latest_balance.get('Current Assets')
                    current_liabilities = latest_balance.get('Current Liabilities')
                    
                    # èµ„äº§è´Ÿå€ºæ¯”ç‡
                    if total_assets and total_debt and total_assets > 0:
                        ratios['DebtToAssets'] = total_debt / total_assets
                    
                    if stockholder_equity and total_debt and stockholder_equity > 0:
                        ratios['DebtToEquity'] = total_debt / stockholder_equity
                    
                    # æµåŠ¨æ¯”ç‡
                    if current_assets and current_liabilities and current_liabilities > 0:
                        ratios['CurrentRatio'] = current_assets / current_liabilities
                    
                    # æ‰˜å®¾Qå€¼
                    if market_cap and total_assets and total_assets > 0:
                        ratios['TobinsQ'] = market_cap / total_assets
            
            if ratios:
                df = pd.DataFrame([ratios])
                df['Symbol'] = symbol
                df['CalculationDate'] = pd.Timestamp.now()
                return df
            
            return None
            
        except Exception as e:
            logger.error(f"è®¡ç®— {symbol} è´¢åŠ¡æ¯”ç‡å¤±è´¥: {e}")
            return None

    def _save_financial_data(self, results, target_path, symbol, save_format):
        """ä¿å­˜è´¢åŠ¡æ•°æ®"""
        files_saved = 0
        
        for data_type, df in results.items():
            if df is None or df.empty:
                continue
                
            try:
                # åˆ›å»ºæ–‡ä»¶è·¯å¾„
                file_dir = target_path / data_type
                file_dir.mkdir(exist_ok=True)
                
                if save_format.lower() == 'csv':
                    file_path = file_dir / f"{symbol}.csv"
                    df.to_csv(file_path, index=True)
                elif save_format.lower() == 'pickle':
                    file_path = file_dir / f"{symbol}.pkl"
                    df.to_pickle(file_path)
                elif save_format.lower() == 'json':
                    file_path = file_dir / f"{symbol}.json"
                    df.to_json(file_path, orient='records', date_format='iso')
                else:
                    logger.warning(f"ä¸æ”¯æŒçš„ä¿å­˜æ ¼å¼: {save_format}")
                    continue
                
                files_saved += 1
                
            except Exception as e:
                logger.error(f"ä¿å­˜ {symbol} çš„ {data_type} æ•°æ®å¤±è´¥: {e}")
        
        return files_saved

    def download_fundamental_analysis_data(
        self,
        target_dir="~/.qlib/fundamental_data", 
        region="us",
        analysis_types=None,
        stock_symbols=None,
        include_technical_indicators=True,
        time_periods=None,
    ):
        """
        ä¸‹è½½åŸºæœ¬é¢åˆ†ææ•°æ®ï¼ˆæ•´åˆç‰ˆï¼‰
        
        Parameters
        ----------
        target_dir: str
            æ•°æ®ä¿å­˜ç›®å½•
        region: str
            å¸‚åœºåŒºåŸŸ
        analysis_types: list
            åˆ†æç±»å‹ ['valuation', 'profitability', 'liquidity', 'leverage', 'efficiency']
        stock_symbols: list
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        include_technical_indicators: bool
            æ˜¯å¦åŒ…å«æŠ€æœ¯æŒ‡æ ‡
        time_periods: list
            æ—¶é—´å‘¨æœŸ ['quarterly', 'annual']
        """
        
        if analysis_types is None:
            analysis_types = ['valuation', 'profitability', 'liquidity', 'leverage']
        
        if time_periods is None:
            time_periods = ['quarterly', 'annual']
        
        # è·å–åŸºç¡€è´¢åŠ¡æ•°æ®
        basic_data_types = ['info', 'financials', 'balance_sheet', 'cashflow']
        success = self.download_financial_data(
            target_dir=target_dir,
            region=region,
            data_types=basic_data_types,
            stock_symbols=stock_symbols,
            include_ratios=True
        )
        
        if not success:
            return False
        
        # è¿›è¡ŒåŸºæœ¬é¢åˆ†æ
        return self._perform_fundamental_analysis(
            target_dir, analysis_types, time_periods, include_technical_indicators
        )

    def _perform_fundamental_analysis(self, target_dir, analysis_types, time_periods, include_technical):
        """æ‰§è¡ŒåŸºæœ¬é¢åˆ†æ"""
        try:
            target_path = Path(target_dir).expanduser()
            analysis_dir = target_path / "analysis_results"
            analysis_dir.mkdir(exist_ok=True)
            
            logger.info("å¼€å§‹åŸºæœ¬é¢åˆ†æ...")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„åˆ†æé€»è¾‘
            # ä¾‹å¦‚ï¼šè®¡ç®—å„ç§è´¢åŠ¡æ¯”ç‡ã€è¶‹åŠ¿åˆ†æã€åŒè¡Œæ¯”è¾ƒç­‰
            
            logger.info("âœ… åŸºæœ¬é¢åˆ†æå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åŸºæœ¬é¢åˆ†æå¤±è´¥: {e}")
            return False


# ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™åŸæ¥çš„GetDataç±»å
GetData = GetDataEnhanced 