#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import time
import pandas as pd
from pathlib import Path
from loguru import logger
import argparse

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from qlib_indicators import QlibIndicatorsEnhancedCalculator

class BatchIndicatorCalculator:
    """
    æ‰¹å¤„ç†æŒ‡æ ‡è®¡ç®—å™¨
    å°†å¤§é‡è‚¡ç¥¨åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡ºå’Œç´¢å¼•å†²çª
    """
    
    def __init__(self, data_dir: str = None, batch_size: int = 20, max_workers: int = 8):
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # åˆ›å»ºè®¡ç®—å™¨
        if data_dir:
            self.calculator = QlibIndicatorsEnhancedCalculator(
                data_dir=data_dir,
                enable_parallel=True,
                max_workers=max_workers
            )
        else:
            self.calculator = QlibIndicatorsEnhancedCalculator(
                enable_parallel=True,
                max_workers=max_workers
            )
        
        logger.info(f"æ‰¹å¤„ç†é…ç½®: æ‰¹æ¬¡å¤§å°={batch_size}, çº¿ç¨‹æ•°={max_workers}")
    
    def get_stock_batches(self, max_stocks: int = None):
        """å°†è‚¡ç¥¨åˆ†æ‰¹"""
        stocks = self.calculator.get_available_stocks()
        
        if max_stocks:
            stocks = stocks[:max_stocks]
        
        # åˆ†æ‰¹
        batches = []
        for i in range(0, len(stocks), self.batch_size):
            batch = stocks[i:i + self.batch_size]
            batches.append(batch)
        
        logger.info(f"æ€»è‚¡ç¥¨æ•°: {len(stocks)}, åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡")
        return batches
    
    def process_single_batch(self, batch_stocks, batch_num, total_batches):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_stocks)} åªè‚¡ç¥¨)")
        
        start_time = time.time()
        
        try:
            # ä¸´æ—¶ä¿®æ”¹è‚¡ç¥¨åˆ—è¡¨
            original_get_stocks = self.calculator.get_available_stocks
            self.calculator.get_available_stocks = lambda: batch_stocks
            
            # è®¡ç®—æŒ‡æ ‡
            result = self.calculator.calculate_all_indicators()
            
            # æ¢å¤åŸå§‹æ–¹æ³•
            self.calculator.get_available_stocks = original_get_stocks
            
            elapsed_time = time.time() - start_time
            
            if not result.empty:
                logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {len(result)} è¡Œ, {len(result.columns)-1} ä¸ªæŒ‡æ ‡ (è€—æ—¶: {elapsed_time:.2f}s)")
                return result
            else:
                logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_num} ç»“æœä¸ºç©º")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
            return None
    
    def merge_batch_results(self, batch_results):
        """å®‰å…¨åˆå¹¶æ‰¹æ¬¡ç»“æœ"""
        logger.info(f"å¼€å§‹åˆå¹¶ {len(batch_results)} ä¸ªæ‰¹æ¬¡ç»“æœ...")
        
        if not batch_results:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ‰¹æ¬¡ç»“æœå¯ä»¥åˆå¹¶")
            return pd.DataFrame()
        
        try:
            # æœ€å®‰å…¨çš„é€ä¸ªåˆå¹¶æ–¹å¼
            combined_df = None
            total_rows = 0
            
            for i, batch_df in enumerate(batch_results):
                if batch_df is None or batch_df.empty:
                    continue
                
                # å½»åº•æ¸…ç†DataFrame
                clean_df = batch_df.copy()
                clean_df = clean_df.reset_index(drop=True)
                
                # ç¡®ä¿åˆ—åä¸€è‡´
                if combined_df is not None:
                    # ç»Ÿä¸€åˆ—ç»“æ„
                    all_cols = sorted(set(combined_df.columns) | set(clean_df.columns))
                    
                    # ä¸ºç¼ºå¤±çš„åˆ—æ·»åŠ NaN
                    for col in all_cols:
                        if col not in combined_df.columns:
                            combined_df[col] = pd.NA
                        if col not in clean_df.columns:
                            clean_df[col] = pd.NA
                    
                    # é‡æ–°æ’åºåˆ—
                    combined_df = combined_df[all_cols]
                    clean_df = clean_df[all_cols]
                
                if combined_df is None:
                    combined_df = clean_df
                else:
                    # ä½¿ç”¨æœ€åŸºæœ¬çš„appendæ–¹å¼
                    combined_df = pd.concat([combined_df, clean_df], ignore_index=True, sort=False)
                
                total_rows += len(clean_df)
                logger.info(f"åˆå¹¶æ‰¹æ¬¡ {i+1}: +{len(clean_df)} è¡Œ (æ€»è®¡: {total_rows} è¡Œ)")
            
            if combined_df is not None and not combined_df.empty:
                # å»é‡
                initial_rows = len(combined_df)
                combined_df = combined_df.drop_duplicates()
                final_rows = len(combined_df)
                
                if initial_rows != final_rows:
                    logger.info(f"å»é‡: {initial_rows} -> {final_rows} è¡Œ")
                
                logger.info(f"âœ… åˆå¹¶å®Œæˆ: {final_rows} è¡Œ, {len(combined_df.columns)-1} ä¸ªæŒ‡æ ‡")
                return combined_df
            else:
                logger.error("åˆå¹¶åç»“æœä¸ºç©º")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"åˆå¹¶æ‰¹æ¬¡ç»“æœå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def run_batch_calculation(self, max_stocks: int = None, output_file: str = "batch_indicators.csv"):
        """è¿è¡Œæ‰¹å¤„ç†è®¡ç®—"""
        logger.info("ğŸš€ å¼€å§‹æ‰¹å¤„ç†æŒ‡æ ‡è®¡ç®—")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # è·å–è‚¡ç¥¨æ‰¹æ¬¡
        batches = self.get_stock_batches(max_stocks)
        
        if not batches:
            logger.error("æ²¡æœ‰è‚¡ç¥¨éœ€è¦å¤„ç†")
            return False
        
        # å¤„ç†å„ä¸ªæ‰¹æ¬¡
        batch_results = []
        successful_batches = 0
        
        for i, batch in enumerate(batches, 1):
            try:
                result = self.process_single_batch(batch, i, len(batches))
                if result is not None and not result.empty:
                    batch_results.append(result)
                    successful_batches += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = i / len(batches) * 100
                    logger.info(f"ğŸ“Š æ•´ä½“è¿›åº¦: {progress:.1f}% ({i}/{len(batches)} æ‰¹æ¬¡)")
                    
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i} æ— æœ‰æ•ˆç»“æœ")
                    
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {i} å¤„ç†å¼‚å¸¸: {e}")
                continue
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        if batch_results:
            final_result = self.merge_batch_results(batch_results)
            
            if not final_result.empty:
                # ä¿å­˜ç»“æœ
                try:
                    output_path = Path(output_file)
                    final_result.to_csv(output_path, index=False, encoding='utf-8-sig')
                    
                    total_time = time.time() - start_time
                    
                    logger.info("=" * 60)
                    logger.info("ğŸ‰ æ‰¹å¤„ç†å®Œæˆï¼")
                    logger.info(f"âœ… æˆåŠŸæ‰¹æ¬¡: {successful_batches}/{len(batches)}")
                    logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {final_result['Symbol'].nunique()}")
                    logger.info(f"ğŸ“ˆ æ€»æŒ‡æ ‡æ•°: {len(final_result.columns)-1}")
                    logger.info(f"ğŸ“‹ æ€»æ•°æ®è¡Œæ•°: {len(final_result)}")
                    logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
                    logger.info(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_path.absolute()}")
                    logger.info("=" * 60)
                    
                    return True
                    
                except Exception as e:
                    logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
                    return False
            else:
                logger.error("æœ€ç»ˆåˆå¹¶ç»“æœä¸ºç©º")
                return False
        else:
            logger.error("æ²¡æœ‰æˆåŠŸçš„æ‰¹æ¬¡ç»“æœ")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ‰¹å¤„ç†QlibæŒ‡æ ‡è®¡ç®—å™¨ - åˆ†æ‰¹å¤„ç†å¤§é‡è‚¡ç¥¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  # é»˜è®¤é…ç½® (æ¯æ‰¹20åªè‚¡ç¥¨ï¼Œ8ä¸ªçº¿ç¨‹)
  python batch_calculator.py
  
  # è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°å’Œçº¿ç¨‹æ•°
  python batch_calculator.py --batch-size 10 --max-workers 4
  
  # å¤„ç†å‰100åªè‚¡ç¥¨
  python batch_calculator.py --max-stocks 100 --batch-size 25
  
  # å°æ‰¹æ¬¡å¤„ç†ï¼ˆå†…å­˜å—é™ç¯å¢ƒï¼‰
  python batch_calculator.py --batch-size 5 --max-workers 2
        '''
    )
    
    parser.add_argument('--data-dir', help='Qlibæ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--max-stocks', type=int, help='æœ€å¤§è‚¡ç¥¨æ•°é‡')
    parser.add_argument('--batch-size', type=int, default=20, help='æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡ (é»˜è®¤: 20)')
    parser.add_argument('--max-workers', type=int, default=8, help='æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤: 8)')
    parser.add_argument('--output', default='batch_indicators.csv', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='æ—¥å¿—çº§åˆ«')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        lambda msg: print(msg, end=""),
        level=args.log_level,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>"
    )
    
    try:
        # åˆ›å»ºæ‰¹å¤„ç†è®¡ç®—å™¨
        batch_calc = BatchIndicatorCalculator(
            data_dir=args.data_dir,
            batch_size=args.batch_size,
            max_workers=args.max_workers
        )
        
        # è¿è¡Œè®¡ç®—
        success = batch_calc.run_batch_calculation(
            max_stocks=args.max_stocks,
            output_file=args.output
        )
        
        if success:
            logger.info("ğŸ¯ æ‰¹å¤„ç†è®¡ç®—æˆåŠŸå®Œæˆï¼")
        else:
            logger.error("âŒ æ‰¹å¤„ç†è®¡ç®—å¤±è´¥")
            
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 