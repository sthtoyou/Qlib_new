#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import pandas as pd
import numpy as np
import talib
import struct
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from loguru import logger
import warnings
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
from functools import partial
import multiprocessing
import csv
import gc
import logging
import json
import hashlib
from datetime import datetime, timedelta
import shutil

warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=FutureWarning)


class QlibIndicatorsEnhancedCalculator:
    """
    å¢å¼ºç‰ˆQlibæŒ‡æ ‡è®¡ç®—å™¨
    é›†æˆAlpha158ã€Alpha360æŒ‡æ ‡ä½“ç³»å’Œå¤šç§æŠ€æœ¯åˆ†ææŒ‡æ ‡
    æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—ã€æŒ‡æ ‡å»é‡åŠŸèƒ½å’Œå¢é‡è®¡ç®—
    """
    
    # å­—æ®µä¸­æ–‡æ ‡ç­¾æ˜ å°„
    FIELD_LABELS = {
        # åŸºç¡€å­—æ®µ
        'Date': 'æ—¥æœŸ',
        'Symbol': 'è‚¡ç¥¨ä»£ç ',
        
        # OHLCVåŸºç¡€æ•°æ®
        'Open': 'å¼€ç›˜ä»·',
        'High': 'æœ€é«˜ä»·', 
        'Low': 'æœ€ä½ä»·',
        'Close': 'æ”¶ç›˜ä»·',
        'Volume': 'æˆäº¤é‡',
        
        # æ³¢åŠ¨ç‡æŒ‡æ ‡
        'RealizedVolatility_20': '20æ—¥å·²å®ç°æ³¢åŠ¨ç‡',
        'NegativeSemiDeviation_20': '20æ—¥è´ŸåŠåå·®',
        'ContinuousVolatility_20': '20æ—¥è¿ç»­æ³¢åŠ¨ç‡',
        'PositiveSemiDeviation_20': '20æ—¥æ­£åŠåå·®',
        'Volatility_10': '10æ—¥æ³¢åŠ¨ç‡',
        'Volatility_30': '30æ—¥æ³¢åŠ¨ç‡',
        'Volatility_60': '60æ—¥æ³¢åŠ¨ç‡',
        
        # èœ¡çƒ›å›¾æ¨¡å¼
        'CDL2CROWS': 'ä¸¤åªä¹Œé¸¦',
        'CDL3BLACKCROWS': 'ä¸‰åªä¹Œé¸¦',
        'CDL3INSIDE': 'ä¸‰å†…éƒ¨ä¸Šæ¶¨å’Œä¸‹è·Œ',
        'CDL3LINESTRIKE': 'ä¸‰çº¿æ‰“å‡»',
        'CDL3OUTSIDE': 'ä¸‰å¤–éƒ¨ä¸Šæ¶¨å’Œä¸‹è·Œ',
        'CDL3STARSINSOUTH': 'å—æ–¹ä¸‰æ˜Ÿ',
        'CDL3WHITESOLDIERS': 'ä¸‰ç™½å…µ',
        'CDLABANDONEDBABY': 'å¼ƒå©´',
        'CDLADVANCEBLOCK': 'å¤§æ•Œå½“å‰',
        'CDLBELTHOLD': 'æ‰è…°å¸¦çº¿',
        'CDLBREAKAWAY': 'è„±ç¦»',
        'CDLCLOSINGMARUBOZU': 'æ”¶ç›˜ç¼ºå½±çº¿',
        'CDLCONCEALBABYSWALL': 'è—å©´åæ²¡',
        'CDLCOUNTERATTACK': 'åå‡»çº¿',
        'CDLDARKCLOUDCOVER': 'ä¹Œäº‘å‹é¡¶',
        'CDLDOJI': 'åå­—',
        'CDLDOJISTAR': 'åå­—æ˜Ÿ',
        'CDLDRAGONFLYDOJI': 'èœ»èœ“åå­—',
        'CDLENGULFING': 'åå™¬æ¨¡å¼',
        'CDLEVENINGDOJISTAR': 'åå­—æš®æ˜Ÿ',
        'CDLEVENINGSTAR': 'æš®æ˜Ÿ',
        'CDLGAPSIDESIDEWHITE': 'å‘ä¸Šè·³ç©ºå¹¶åˆ—é˜³çº¿',
        'CDLGRAVESTONEDOJI': 'å¢“ç¢‘åå­—',
        'CDLHAMMER': 'é”¤å­çº¿',
        'CDLHANGINGMAN': 'ä¸ŠåŠçº¿',
        'CDLHARAMI': 'æ¯å­çº¿',
        'CDLHARAMICROSS': 'åå­—å­•çº¿',
        'CDLHIGHWAVE': 'é£é«˜æµªå¤§çº¿',
        'CDLHIKKAKE': 'é™·é˜±',
        'CDLHIKKAKEMOD': 'ä¿®æ­£é™·é˜±',
        'CDLHOMINGPIGEON': 'å®¶é¸½',
        'CDLIDENTICAL3CROWS': 'ä¸‰èƒèƒä¹Œé¸¦',
        'CDLINNECK': 'é¢ˆå†…çº¿',
        'CDLINVERTEDHAMMER': 'å€’é”¤å­çº¿',
        'CDLKICKING': 'åå†²å½¢æ€',
        'CDLKICKINGBYLENGTH': 'ç”±è¾ƒé•¿ç¼ºå½±çº¿å†³å®šçš„åå†²å½¢æ€',
        'CDLLADDERBOTTOM': 'æ¢¯åº•',
        'CDLLONGLEGGEDDOJI': 'é•¿è„šåå­—',
        'CDLLONGLINE': 'é•¿èœ¡çƒ›çº¿',
        'CDLMARUBOZU': 'å…‰å¤´å…‰è„š/ç¼ºå½±çº¿',
        'CDLMATCHINGLOW': 'ç›¸åŒä½ä»·',
        'CDLMATHOLD': 'é“ºå«',
        'CDLMORNINGDOJISTAR': 'åå­—æ™¨æ˜Ÿ',
        'CDLMORNINGSTAR': 'æ™¨æ˜Ÿ',
        'CDLONNECK': 'é¢ˆä¸Šçº¿',
        'CDLPIERCING': 'åˆºç©¿å½¢æ€',
        'CDLRICKSHAWMAN': 'é»„åŒ…è½¦å¤«',
        'CDLRISEFALL3METHODS': 'ä¸Šå‡å’Œä¸‹é™ä¸‰æ³•',
        'CDLSEPARATINGLINES': 'åˆ†ç¦»çº¿',
        'CDLSHOOTINGSTAR': 'å°„å‡»ä¹‹æ˜Ÿ',
        'CDLSHORTLINE': 'çŸ­èœ¡çƒ›çº¿',
        'CDLSPINNINGTOP': 'é™€èº',
        'CDLSTALLEDPATTERN': 'åœé¡¿å½¢æ€',
        'CDLSTICKSANDWICH': 'æ¡å½¢ä¸‰æ˜æ²»',
        'CDLTAKURI': 'æ¢æ°´ç«¿',
        'CDLTASUKIGAP': 'è·³ç©ºå¹¶åˆ—é˜´é˜³çº¿',
        'CDLTHRUSTING': 'æ’å…¥',
        'CDLTRISTAR': 'ä¸‰æ˜Ÿ',
        'CDLUNIQUE3RIVER': 'å¥‡ç‰¹ä¸‰æ²³åºŠ',
        'CDLUPSIDEGAP2CROWS': 'ä¸Šå‡è·³ç©ºä¸¤ä¹Œé¸¦',
        'CDLXSIDEGAP3METHODS': 'ä¸Šå‡/ä¸‹é™è·³ç©ºä¸‰æ³•',
        
        # ç§»åŠ¨å¹³å‡çº¿
        'SMA_5': '5æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'SMA_10': '10æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'SMA_20': '20æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'SMA_50': '50æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'EMA_5': '5æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'EMA_10': '10æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'EMA_20': '20æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'EMA_50': '50æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'DEMA_20': '20æ—¥åŒé‡æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'TEMA_20': '20æ—¥ä¸‰é‡æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'KAMA_30': '30æ—¥è‡ªé€‚åº”ç§»åŠ¨å¹³å‡',
        'WMA_20': '20æ—¥åŠ æƒç§»åŠ¨å¹³å‡',
        
        # MACDæŒ‡æ ‡
        'MACD': 'MACDä¸»çº¿',
        'MACD_Signal': 'MACDä¿¡å·çº¿',
        'MACD_Histogram': 'MACDæŸ±çŠ¶å›¾',
        'MACDEXT': 'MACDæ‰©å±•',
        'MACDFIX': 'MACDå›ºå®š',
        
        # åŠ¨é‡æŒ‡æ ‡
        'RSI_14': '14æ—¥ç›¸å¯¹å¼ºå¼±æŒ‡æ•°',
        'CCI_14': '14æ—¥å•†å“é€šé“æŒ‡æ•°',
        'CMO_14': '14æ—¥é’±å¾·åŠ¨é‡æ‘†åŠ¨æŒ‡æ ‡',
        'MFI_14': '14æ—¥èµ„é‡‘æµé‡æŒ‡æ•°',
        'WILLR_14': '14æ—¥å¨å»‰æŒ‡æ ‡',
        'ULTOSC': 'ç»ˆææ‘†åŠ¨æŒ‡æ ‡',
        
        # è¶‹åŠ¿æŒ‡æ ‡
        'ADX_14': '14æ—¥å¹³å‡è¶‹å‘æŒ‡æ•°',
        'ADXR_14': '14æ—¥å¹³å‡è¶‹å‘æŒ‡æ•°è¯„çº§',
        'APO': 'ä»·æ ¼éœ‡è¡æŒ‡æ ‡',
        'AROON_DOWN': 'é˜¿éš†ä¸‹é™',
        'AROON_UP': 'é˜¿éš†ä¸Šå‡',
        'AROONOSC_14': '14æ—¥é˜¿éš†éœ‡è¡æŒ‡æ ‡',
        'BOP': 'å¹³è¡¡éœ‡è¡æŒ‡æ ‡',
        'DX_14': '14æ—¥æ–¹å‘è¿åŠ¨æŒ‡æ•°',
        'MINUS_DI_14': '14æ—¥è´Ÿå‘åŠ¨æ€æŒ‡æ ‡',
        'MINUS_DM_14': '14æ—¥è´Ÿå‘åŠ¨æ€åŠ¨é‡',
        'PLUS_DI_14': '14æ—¥æ­£å‘åŠ¨æ€æŒ‡æ ‡',
        'PLUS_DM_14': '14æ—¥æ­£å‘åŠ¨æ€åŠ¨é‡',
        'PPO': 'ä»·æ ¼éœ‡è¡ç™¾åˆ†æ¯”',
        'TRIX_30': '30æ—¥ä¸‰é‡æŒ‡æ•°å¹³æ»‘ç§»åŠ¨å¹³å‡',
        
        # ä»·æ ¼åŠ¨é‡
        'MOM_10': '10æ—¥åŠ¨é‡',
        'ROC_10': '10æ—¥å˜åŠ¨ç‡',
        'ROCP_10': '10æ—¥å˜åŠ¨ç‡ç™¾åˆ†æ¯”',
        'ROCR_10': '10æ—¥å˜åŠ¨ç‡æ¯”ç‡',
        'ROCR100_10': '10æ—¥å˜åŠ¨ç‡æ¯”ç‡100',
        
        # å¸ƒæ—å¸¦
        'BB_Upper': 'å¸ƒæ—å¸¦ä¸Šè½¨',
        'BB_Middle': 'å¸ƒæ—å¸¦ä¸­è½¨',
        'BB_Lower': 'å¸ƒæ—å¸¦ä¸‹è½¨',
        
        # éšæœºæŒ‡æ ‡
        'STOCH_K': 'éšæœºæŒ‡æ ‡Kå€¼',
        'STOCH_D': 'éšæœºæŒ‡æ ‡Då€¼',
        'STOCHF_K': 'å¿«é€ŸéšæœºæŒ‡æ ‡Kå€¼',
        'STOCHF_D': 'å¿«é€ŸéšæœºæŒ‡æ ‡Då€¼',
        'STOCHRSI_K': 'éšæœºRSIæŒ‡æ ‡Kå€¼',
        'STOCHRSI_D': 'éšæœºRSIæŒ‡æ ‡Då€¼',
        
        # æ³¢åŠ¨ç‡æŒ‡æ ‡
        'ATR_14': '14æ—¥å¹³å‡çœŸå®æ³¢å¹…',
        'NATR_14': '14æ—¥å½’ä¸€åŒ–å¹³å‡çœŸå®æ³¢å¹…',
        'TRANGE': 'çœŸå®æ³¢å¹…',
        
        # æˆäº¤é‡æŒ‡æ ‡
        'OBV': 'èƒ½é‡æ½®',
        'AD': 'ç´¯ç§¯/æ´¾å‘çº¿',
        'ADOSC': 'ç´¯ç§¯/æ´¾å‘éœ‡è¡æŒ‡æ ‡',
        
        # å¸Œå°”ä¼¯ç‰¹å˜æ¢æŒ‡æ ‡
        'HT_DCPERIOD': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢ä¸»å¯¼å‘¨æœŸ',
        'HT_DCPHASE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢ä¸»å¯¼å‘¨æœŸç›¸ä½',
        'HT_INPHASE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢åŒç›¸åˆ†é‡',
        'HT_QUADRATURE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢æ­£äº¤åˆ†é‡',
        'HT_SINE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢æ­£å¼¦æ³¢',
        'HT_LEADSINE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢é¢†å…ˆæ­£å¼¦æ³¢',
        'HT_TRENDMODE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢è¶‹åŠ¿æ¨¡å¼',
        'HT_TRENDLINE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢è¶‹åŠ¿çº¿',
        
        # ä»·æ ¼æŒ‡æ ‡
        'AVGPRICE': 'å¹³å‡ä»·æ ¼',
        'MEDPRICE': 'ä¸­é—´ä»·æ ¼',
        'TYPPRICE': 'å…¸å‹ä»·æ ¼',
        'WCLPRICE': 'åŠ æƒæ”¶ç›˜ä»·',
        'MIDPOINT': 'ä¸­ç‚¹',
        'MIDPRICE': 'ä¸­é—´ä»·æ ¼',
        'MAMA': 'MESAè‡ªé€‚åº”ç§»åŠ¨å¹³å‡',
        'FAMA': 'è·Ÿéšè‡ªé€‚åº”ç§»åŠ¨å¹³å‡',
        
        # ç»Ÿè®¡æŒ‡æ ‡
        'LINEARREG': 'çº¿æ€§å›å½’',
        'LINEARREG_ANGLE': 'çº¿æ€§å›å½’è§’åº¦',
        'LINEARREG_INTERCEPT': 'çº¿æ€§å›å½’æˆªè·',
        'LINEARREG_SLOPE': 'çº¿æ€§å›å½’æ–œç‡',
        'STDDEV': 'æ ‡å‡†å·®',
        'TSF': 'æ—¶é—´åºåˆ—é¢„æµ‹',
        'VAR': 'æ–¹å·®',
        'MAXINDEX': 'æœ€å¤§å€¼ç´¢å¼•',
        'MININDEX': 'æœ€å°å€¼ç´¢å¼•',
        
        # è´¢åŠ¡æŒ‡æ ‡
        'PriceToBookRatio': 'å¸‚å‡€ç‡',
        'MarketCap': 'å¸‚å€¼',
        'PERatio': 'å¸‚ç›ˆç‡',
        'PriceToSalesRatio': 'å¸‚é”€ç‡',
        'ROE': 'å‡€èµ„äº§æ”¶ç›Šç‡',
        'ROA': 'æ€»èµ„äº§æ”¶ç›Šç‡',
        'ProfitMargins': 'åˆ©æ¶¦ç‡',
        'QuickRatio': 'é€ŸåŠ¨æ¯”ç‡',
        'DebtToEquity': 'èµ„äº§è´Ÿå€ºç‡',
        'TobinsQ': 'æ‰˜å®¾Qå€¼',
        'DailyTurnover': 'æ—¥æ¢æ‰‹ç‡',
        'turnover_c1d': '1æ—¥ç´¯è®¡æ¢æ‰‹ç‡',
        'turnover_c5d': '5æ—¥ç´¯è®¡æ¢æ‰‹ç‡',
        'turnover_m5d': '5æ—¥ç§»åŠ¨æ¢æ‰‹ç‡',
        'turnover_c10d': '10æ—¥ç´¯è®¡æ¢æ‰‹ç‡',
        'turnover_m10d': '10æ—¥ç§»åŠ¨æ¢æ‰‹ç‡',
        'turnover_c20d': '20æ—¥ç´¯è®¡æ¢æ‰‹ç‡',
        'turnover_m20d': '20æ—¥ç§»åŠ¨æ¢æ‰‹ç‡',
        'turnover_c30d': '30æ—¥ç´¯è®¡æ¢æ‰‹ç‡',
        'turnover_m30d': '30æ—¥ç§»åŠ¨æ¢æ‰‹ç‡',
        'CurrentRatio': 'æµåŠ¨æ¯”ç‡',
    }
    
    @classmethod
    def _generate_alpha360_labels(cls):
        """ç”ŸæˆAlpha360æŒ‡æ ‡çš„ä¸­æ–‡æ ‡ç­¾"""
        alpha360_labels = {}
        
        # ä¸ºAlpha360æŒ‡æ ‡ç”Ÿæˆæ ‡ç­¾
        for field in ['CLOSE', 'OPEN', 'HIGH', 'LOW', 'VWAP', 'VOLUME']:
            for i in range(60):
                field_name = f'ALPHA360_{field}{i}'
                if field == 'CLOSE':
                    alpha360_labels[field_name] = f'Alpha360æ”¶ç›˜ä»·{i}æ—¥å‰'
                elif field == 'OPEN':
                    alpha360_labels[field_name] = f'Alpha360å¼€ç›˜ä»·{i}æ—¥å‰'
                elif field == 'HIGH':
                    alpha360_labels[field_name] = f'Alpha360æœ€é«˜ä»·{i}æ—¥å‰'
                elif field == 'LOW':
                    alpha360_labels[field_name] = f'Alpha360æœ€ä½ä»·{i}æ—¥å‰'
                elif field == 'VWAP':
                    alpha360_labels[field_name] = f'Alpha360æˆäº¤é‡åŠ æƒå¹³å‡ä»·{i}æ—¥å‰'
                elif field == 'VOLUME':
                    alpha360_labels[field_name] = f'Alpha360æˆäº¤é‡{i}æ—¥å‰'
        
        return alpha360_labels
    
    @classmethod
    def _generate_alpha158_labels(cls):
        """ç”ŸæˆAlpha158æŒ‡æ ‡çš„ä¸­æ–‡æ ‡ç­¾"""
        alpha158_labels = {}
        
        # åŸºç¡€Kçº¿æŒ‡æ ‡
        alpha158_labels.update({
            'ALPHA158_KMID': 'Kçº¿ä¸­ç‚¹ä»·æ ¼',
            'ALPHA158_KLEN': 'Kçº¿é•¿åº¦',
            'ALPHA158_KMID2': 'Kçº¿ä¸­ç‚¹ä»·æ ¼å¹³æ–¹',
            'ALPHA158_KUP': 'Kçº¿ä¸Šå½±çº¿',
            'ALPHA158_KUP2': 'Kçº¿ä¸Šå½±çº¿å¹³æ–¹',
            'ALPHA158_KLOW': 'Kçº¿ä¸‹å½±çº¿',
            'ALPHA158_KLOW2': 'Kçº¿ä¸‹å½±çº¿å¹³æ–¹',
            'ALPHA158_KSFT': 'Kçº¿åç§»',
            'ALPHA158_KSFT2': 'Kçº¿åç§»å¹³æ–¹',
            'ALPHA158_OPEN0': 'å½“æ—¥å¼€ç›˜ä»·',
            'ALPHA158_HIGH0': 'å½“æ—¥æœ€é«˜ä»·',
            'ALPHA158_LOW0': 'å½“æ—¥æœ€ä½ä»·',
            'ALPHA158_VWAP0': 'å½“æ—¥æˆäº¤é‡åŠ æƒå¹³å‡ä»·',
            'ALPHA158_VOLUME0': 'å½“æ—¥æˆäº¤é‡',
        })
        
        # æ”¶ç›Šç‡æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_ROC{period}'] = f'{period}æ—¥æ”¶ç›Šç‡'
        
        # ç§»åŠ¨å¹³å‡æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_MA{period}'] = f'{period}æ—¥ç§»åŠ¨å¹³å‡'
        
        # æ ‡å‡†å·®æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_STD{period}'] = f'{period}æ—¥æ ‡å‡†å·®'
        
        # è´å¡”å€¼æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_BETA{period}'] = f'{period}æ—¥è´å¡”å€¼'
        
        # Rå¹³æ–¹æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_RSQR{period}'] = f'{period}æ—¥Rå¹³æ–¹'
        
        # æ®‹å·®æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_RESI{period}'] = f'{period}æ—¥çº¿æ€§å›å½’æ®‹å·®'
        
        # æœ€å¤§å€¼æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_MAX{period}'] = f'{period}æ—¥æœ€å¤§å€¼'
        
        # æœ€å°å€¼æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_MIN{period}'] = f'{period}æ—¥æœ€å°å€¼'
        
        # å››åˆ†ä½æ•°æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_QTLU{period}'] = f'{period}æ—¥ä¸Šå››åˆ†ä½æ•°'
            alpha158_labels[f'ALPHA158_QTLD{period}'] = f'{period}æ—¥ä¸‹å››åˆ†ä½æ•°'
        
        # æ’åæŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_RANK{period}'] = f'{period}æ—¥æ’å'
        
        # RSVæŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_RSV{period}'] = f'{period}æ—¥RSV'
        
        # ä½ç½®æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_IMAX{period}'] = f'{period}æ—¥æœ€é«˜ä»·ä½ç½®æŒ‡æ•°'
            alpha158_labels[f'ALPHA158_IMIN{period}'] = f'{period}æ—¥æœ€ä½ä»·ä½ç½®æŒ‡æ•°'
            alpha158_labels[f'ALPHA158_IMXD{period}'] = f'{period}æ—¥æœ€é«˜æœ€ä½ä»·ä½ç½®å·®'
        
        # ç›¸å…³æ€§æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_CORR{period}'] = f'{period}æ—¥ç›¸å…³æ€§'
            alpha158_labels[f'ALPHA158_CORD{period}'] = f'{period}æ—¥ç›¸å…³æ€§å·®åˆ†'
        
        # è®¡æ•°æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_CNTP{period}'] = f'{period}æ—¥ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹'
            alpha158_labels[f'ALPHA158_CNTN{period}'] = f'{period}æ—¥ä¸‹è·Œå¤©æ•°æ¯”ä¾‹'
            alpha158_labels[f'ALPHA158_CNTD{period}'] = f'{period}æ—¥æ¶¨è·Œå¤©æ•°å·®'
        
        # æ±‚å’ŒæŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_SUMP{period}'] = f'{period}æ—¥æ€»æ”¶ç›Šæ¯”ä¾‹'
            alpha158_labels[f'ALPHA158_SUMN{period}'] = f'{period}æ—¥æ€»æŸå¤±æ¯”ä¾‹'
            alpha158_labels[f'ALPHA158_SUMD{period}'] = f'{period}æ—¥æ”¶ç›ŠæŸå¤±å·®æ¯”ä¾‹'
        
        # æˆäº¤é‡æŒ‡æ ‡
        for period in [5, 10, 20, 30, 60]:
            alpha158_labels[f'ALPHA158_VMA{period}'] = f'{period}æ—¥æˆäº¤é‡ç§»åŠ¨å¹³å‡'
            alpha158_labels[f'ALPHA158_VSTD{period}'] = f'{period}æ—¥æˆäº¤é‡æ ‡å‡†å·®'
            alpha158_labels[f'ALPHA158_WVMA{period}'] = f'{period}æ—¥æˆäº¤é‡åŠ æƒä»·æ ¼æ³¢åŠ¨ç‡'
            alpha158_labels[f'ALPHA158_VSUMP{period}'] = f'{period}æ—¥æˆäº¤é‡ä¸Šå‡æ¯”ä¾‹'
            alpha158_labels[f'ALPHA158_VSUMN{period}'] = f'{period}æ—¥æˆäº¤é‡ä¸‹é™æ¯”ä¾‹'
            alpha158_labels[f'ALPHA158_VSUMD{period}'] = f'{period}æ—¥æˆäº¤é‡æ¶¨è·Œå·®æ¯”ä¾‹'
        
        return alpha158_labels
    
    def get_field_labels(self, columns):
        """è·å–å­—æ®µçš„ä¸­æ–‡æ ‡ç­¾"""
        # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
        all_labels = self.FIELD_LABELS.copy()
        all_labels.update(self._generate_alpha360_labels())
        all_labels.update(self._generate_alpha158_labels())
        
        # è¿”å›æŒ‡å®šåˆ—çš„ä¸­æ–‡æ ‡ç­¾
        labels = []
        for col in columns:
            labels.append(all_labels.get(col, col))
        return labels
    
    def __init__(self, data_dir: str = r"D:\stk_data\trd\us_data", financial_data_dir: str = None, 
                 max_workers: int = None, enable_parallel: bool = True,
                 cache_dir: str = "indicator_cache", enable_incremental: bool = False,
                 start_date: str = None, end_date: str = None, recent_days: int = None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆæŒ‡æ ‡è®¡ç®—å™¨
        
        Parameters:
        -----------
        data_dir : str
            æ•°æ®ç›®å½•è·¯å¾„
        financial_data_dir : str
            è´¢åŠ¡æ•°æ®ç›®å½•è·¯å¾„
        max_workers : int
            æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        enable_parallel : bool
            æ˜¯å¦å¯ç”¨å¹¶è¡Œè®¡ç®—
        cache_dir : str
            ç¼“å­˜ç›®å½•ï¼Œç”¨äºå¢é‡è®¡ç®—
        enable_incremental : bool
            æ˜¯å¦å¯ç”¨å¢é‡è®¡ç®—æ¨¡å¼
        start_date : str
            è®¡ç®—å¼€å§‹æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
        end_date : str
            è®¡ç®—ç»“æŸæ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
        recent_days : int
            è®¡ç®—æœ€è¿‘Nå¤©çš„æ•°æ®
        """
        self.data_dir = Path(data_dir)
        self.features_dir = self.data_dir / "features"
        self.output_dir = self.data_dir
        self.financial_data_dir = Path(financial_data_dir) if financial_data_dir else None
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.enable_parallel = enable_parallel
        self.enable_incremental = enable_incremental
        
        # æ—¶é—´çª—å£è®¾ç½®
        self.start_date = None
        self.end_date = None
        self._setup_time_window(start_date, end_date, recent_days)
        
        # å¢é‡è®¡ç®—ç›¸å…³
        if self.enable_incremental:
            self.cache_dir = Path(cache_dir)
            self.cache_dir.mkdir(exist_ok=True)
            
            # ç¼“å­˜æ–‡ä»¶è·¯å¾„
            self.metadata_file = self.cache_dir / "metadata.json"
            self.stock_status_file = self.cache_dir / "stock_status.json"
            self.data_hashes_file = self.cache_dir / "data_hashes.json"
            self.date_ranges_file = self.cache_dir / "date_ranges.json"
            self.output_backup_dir = self.cache_dir / "output_backups"
            self.output_backup_dir.mkdir(exist_ok=True)
            
            # åŠ è½½æˆ–åˆå§‹åŒ–å…ƒæ•°æ®
            self.metadata = self._load_metadata()
            self.stock_status = self._load_stock_status()
            self.data_hashes = self._load_data_hashes()
            self.date_ranges = self._load_date_ranges()
            
            logger.info(f"å¢é‡è®¡ç®—æ¨¡å¼å·²å¯ç”¨ï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
        
        # æŒ‡æ ‡ç¼“å­˜
        self._indicators_cache = {}
        self._indicators_cache_lock = threading.Lock()
        
        # è´¢åŠ¡æ•°æ®ç¼“å­˜
        self._financial_data_cache = {}
        self._financial_data_cache_lock = threading.Lock()
        
        # çº¿ç¨‹æœ¬åœ°å­˜å‚¨
        self._local = threading.local()
        
        # åŠ è½½è´¢åŠ¡æ•°æ®
        if self.financial_data_dir:
            self._load_financial_data()
        
        logger.info(f"å¢å¼ºç‰ˆæŒ‡æ ‡è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ (å¹¶è¡Œ: {enable_parallel}, å¢é‡: {enable_incremental})")
        
    def _setup_time_window(self, start_date: str, end_date: str, recent_days: int):
        """è®¾ç½®æ—¶é—´çª—å£"""
        try:
            if recent_days is not None:
                # ä½¿ç”¨æœ€è¿‘Nå¤©
                if start_date or end_date:
                    logger.warning("åŒæ—¶æŒ‡å®šäº†recent_dayså’Œstart_date/end_dateï¼Œä¼˜å…ˆä½¿ç”¨recent_days")
                
                self.end_date = pd.Timestamp.now().normalize()
                self.start_date = self.end_date - pd.Timedelta(days=recent_days)
                logger.info(f"ğŸ“… æ—¶é—´çª—å£è®¾ç½®: æœ€è¿‘{recent_days}å¤© ({self.start_date.strftime('%Y-%m-%d')} è‡³ {self.end_date.strftime('%Y-%m-%d')})")
                
            elif start_date or end_date:
                # ä½¿ç”¨æŒ‡å®šçš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ
                if start_date:
                    self.start_date = pd.to_datetime(start_date)
                    logger.info(f"ğŸ“… å¼€å§‹æ—¥æœŸ: {self.start_date.strftime('%Y-%m-%d')}")
                    
                if end_date:
                    self.end_date = pd.to_datetime(end_date)
                    logger.info(f"ğŸ“… ç»“æŸæ—¥æœŸ: {self.end_date.strftime('%Y-%m-%d')}")
                    
                if self.start_date and self.end_date:
                    if self.start_date >= self.end_date:
                        raise ValueError("å¼€å§‹æ—¥æœŸå¿…é¡»æ—©äºç»“æŸæ—¥æœŸ")
                    days_span = (self.end_date - self.start_date).days
                    logger.info(f"ğŸ“… æ—¶é—´çª—å£: {days_span}å¤© ({self.start_date.strftime('%Y-%m-%d')} è‡³ {self.end_date.strftime('%Y-%m-%d')})")
                    
            else:
                # æ²¡æœ‰æŒ‡å®šæ—¶é—´çª—å£ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
                logger.info("ğŸ“… æ—¶é—´çª—å£: å…¨éƒ¨æ•°æ®")
                
        except Exception as e:
            logger.error(f"æ—¶é—´çª—å£è®¾ç½®å¤±è´¥: {e}")
            raise ValueError(f"æ— æ•ˆçš„æ—¶é—´çª—å£å‚æ•°: {e}")
    
    def _apply_time_window_filter(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ¹æ®æ—¶é—´çª—å£è¿‡æ»¤æ•°æ®"""
        if df.empty:
            return df
            
        if self.start_date is None and self.end_date is None:
            return df
            
        try:
            # ç¡®ä¿DataFrameæœ‰æ—¥æœŸç´¢å¼•æˆ–Dateåˆ—
            if 'Date' in df.columns:
                # å¦‚æœæœ‰Dateåˆ—ï¼Œä½¿ç”¨Dateåˆ—è¿›è¡Œè¿‡æ»¤
                df_filtered = df.copy()
                date_series = pd.to_datetime(df_filtered['Date'])
                
                if self.start_date is not None:
                    mask_start = date_series >= self.start_date
                    df_filtered = df_filtered[mask_start]
                    
                if self.end_date is not None:
                    mask_end = date_series <= self.end_date
                    df_filtered = df_filtered[mask_end]
                    
            elif isinstance(df.index, pd.DatetimeIndex):
                # å¦‚æœç´¢å¼•æ˜¯æ—¥æœŸç´¢å¼•ï¼Œç›´æ¥ä½¿ç”¨ç´¢å¼•è¿‡æ»¤
                df_filtered = df.copy()
                
                if self.start_date is not None:
                    df_filtered = df_filtered[df_filtered.index >= self.start_date]
                    
                if self.end_date is not None:
                    df_filtered = df_filtered[df_filtered.index <= self.end_date]
                    
            else:
                logger.warning("æ•°æ®æ²¡æœ‰æ—¥æœŸç´¢å¼•æˆ–Dateåˆ—ï¼Œæ— æ³•åº”ç”¨æ—¶é—´çª—å£è¿‡æ»¤")
                return df
                
            if not df_filtered.empty:
                original_len = len(df)
                filtered_len = len(df_filtered)
                logger.debug(f"æ—¶é—´çª—å£è¿‡æ»¤: {original_len} -> {filtered_len} è¡Œ")
                
            return df_filtered
            
        except Exception as e:
            logger.error(f"æ—¶é—´çª—å£è¿‡æ»¤å¤±è´¥: {e}")
            return df

    # å¢é‡è®¡ç®—ç›¸å…³æ–¹æ³•
    def _load_metadata(self) -> Dict:
        """åŠ è½½å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return {
            "last_update": None,
            "total_stocks": 0,
            "processed_stocks": 0,
            "version": "2.0",
            "output_file": None,
            "last_output_backup": None
        }
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _load_stock_status(self) -> Dict:
        """åŠ è½½è‚¡ç¥¨çŠ¶æ€"""
        if self.stock_status_file.exists():
            try:
                with open(self.stock_status_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½è‚¡ç¥¨çŠ¶æ€å¤±è´¥: {e}")
        
        return {}
    
    def _save_stock_status(self):
        """ä¿å­˜è‚¡ç¥¨çŠ¶æ€"""
        try:
            with open(self.stock_status_file, 'w', encoding='utf-8') as f:
                json.dump(self.stock_status, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜è‚¡ç¥¨çŠ¶æ€å¤±è´¥: {e}")
    
    def _load_data_hashes(self) -> Dict:
        """åŠ è½½æ•°æ®å“ˆå¸Œå€¼"""
        if self.data_hashes_file.exists():
            try:
                with open(self.data_hashes_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½æ•°æ®å“ˆå¸Œå¤±è´¥: {e}")
        
        return {}
    
    def _save_data_hashes(self):
        """ä¿å­˜æ•°æ®å“ˆå¸Œå€¼"""
        try:
            with open(self.data_hashes_file, 'w', encoding='utf-8') as f:
                json.dump(self.data_hashes, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å“ˆå¸Œå¤±è´¥: {e}")
    
    def _load_date_ranges(self) -> Dict:
        """åŠ è½½æ—¥æœŸèŒƒå›´ä¿¡æ¯"""
        if self.date_ranges_file.exists():
            try:
                with open(self.date_ranges_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½æ—¥æœŸèŒƒå›´å¤±è´¥: {e}")
        
        return {}
    
    def _save_date_ranges(self):
        """ä¿å­˜æ—¥æœŸèŒƒå›´ä¿¡æ¯"""
        try:
            with open(self.date_ranges_file, 'w', encoding='utf-8') as f:
                json.dump(self.date_ranges, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜æ—¥æœŸèŒƒå›´å¤±è´¥: {e}")
    
    def _calculate_data_hash(self, symbol: str, date_range: Optional[Tuple[str, str]] = None) -> str:
        """è®¡ç®—è‚¡ç¥¨æ•°æ®çš„å“ˆå¸Œå€¼"""
        try:
            data = self.read_qlib_binary_data(symbol)
            if data is None or data.empty:
                return ""
            
            # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œåªè®¡ç®—è¯¥èŒƒå›´å†…çš„æ•°æ®å“ˆå¸Œ
            if date_range:
                start_date, end_date = date_range
                data = data[(data.index >= start_date) & (data.index <= end_date)]
                if data.empty:
                    return ""
            
            # è®¡ç®—å…³é”®å­—æ®µçš„å“ˆå¸Œå€¼
            key_fields = ['Open', 'High', 'Low', 'Close', 'Volume']
            hash_data = data[key_fields].fillna(0).values.tobytes()
            return hashlib.md5(hash_data).hexdigest()
            
        except Exception as e:
            logger.warning(f"è®¡ç®— {symbol} æ•°æ®å“ˆå¸Œå¤±è´¥: {e}")
            return ""
    
    def _get_stock_date_range(self, symbol: str) -> Tuple[Optional[str], Optional[str]]:
        """è·å–è‚¡ç¥¨çš„æ•°æ®æ—¥æœŸèŒƒå›´"""
        try:
            data = self.read_qlib_binary_data(symbol)
            if data is None or data.empty:
                return None, None
            
            return str(data.index.min()), str(data.index.max())
            
        except Exception as e:
            logger.warning(f"è·å– {symbol} æ—¥æœŸèŒƒå›´å¤±è´¥: {e}")
            return None, None
    
    def _is_data_changed(self, symbol: str, date_range: Optional[Tuple[str, str]] = None) -> bool:
        """æ£€æŸ¥è‚¡ç¥¨æ•°æ®æ˜¯å¦å‘ç”Ÿå˜åŒ–"""
        current_hash = self._calculate_data_hash(symbol, date_range)
        
        # æ„å»ºå“ˆå¸Œé”®
        if date_range:
            hash_key = f"{symbol}_{date_range[0]}_{date_range[1]}"
        else:
            hash_key = symbol
        
        previous_hash = self.data_hashes.get(hash_key, "")
        
        if current_hash != previous_hash:
            self.data_hashes[hash_key] = current_hash
            return True
        
        return False
    
    def _get_stock_last_update(self, symbol: str) -> Optional[str]:
        """è·å–è‚¡ç¥¨æœ€åæ›´æ–°æ—¶é—´"""
        return self.stock_status.get(symbol, {}).get('last_update')
    
    def _update_stock_status(self, symbol: str, success: bool, rows: int = 0, 
                           date_range: Optional[Tuple[str, str]] = None):
        """
        æ›´æ–°è‚¡ç¥¨çŠ¶æ€
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        success : bool
            æ˜¯å¦æˆåŠŸ
        rows : int
            å¤„ç†çš„è¡Œæ•°
        date_range : Optional[Tuple[str, str]]
            æ—¥æœŸèŒƒå›´ (start_date, end_date)
        """
        if symbol not in self.stock_status:
            self.stock_status[symbol] = {}
        
        self.stock_status[symbol].update({
            'last_update': datetime.now().isoformat(),
            'success': success,
            'rows': rows,
            'indicators_count': 695  # é¢„æœŸçš„æŒ‡æ ‡æ•°é‡
        })
        
        # æ›´æ–°æ—¥æœŸèŒƒå›´ä¿¡æ¯
        if date_range:
            self.date_ranges[symbol] = {
                'start_date': date_range[0],
                'end_date': date_range[1],
                'last_update': datetime.now().isoformat()
            }
    
    def _needs_update(self, symbol: str, force_update: bool = False, 
                     date_range: Optional[Tuple[str, str]] = None) -> Tuple[bool, str]:
        """
        åˆ¤æ–­è‚¡ç¥¨æ˜¯å¦éœ€è¦æ›´æ–°
        åŸºäº"Stock X Date X Indicator"ç»´åº¦è¿›è¡Œå¢é‡åˆ¤æ–­
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        force_update : bool
            æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        date_range : Optional[Tuple[str, str]]
            æ—¥æœŸèŒƒå›´ (start_date, end_date)
            
        Returns:
        --------
        Tuple[bool, str]: (æ˜¯å¦éœ€è¦æ›´æ–°, åŸå› )
        """
        if force_update:
            return True, "å¼ºåˆ¶æ›´æ–°"
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å‘ç”Ÿå˜åŒ–ï¼ˆåŸºäºæ•°æ®å“ˆå¸Œï¼‰
        if self._is_data_changed(symbol, date_range):
            return True, "æ•°æ®å‘ç”Ÿå˜åŒ–"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ—¥æœŸèŒƒå›´
        if date_range:
            current_range = self.date_ranges.get(symbol, {})
            current_start = current_range.get('start_date')
            current_end = current_range.get('end_date')
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦æ‰©å±•
            if current_start is None or current_end is None:
                return True, "é¦–æ¬¡è·å–æ—¥æœŸèŒƒå›´"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®ï¼ˆæ—¥æœŸèŒƒå›´æ‰©å±•ï¼‰
            if date_range[0] < current_start:
                return True, f"æ—¥æœŸèŒƒå›´æ‰©å±•ï¼ˆå¼€å§‹æ—¥æœŸ: {date_range[0]} < {current_start}ï¼‰"
            
            if date_range[1] > current_end:
                return True, f"æ—¥æœŸèŒƒå›´æ‰©å±•ï¼ˆç»“æŸæ—¥æœŸ: {date_range[1]} > {current_end}ï¼‰"
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦æ”¶ç¼©ï¼ˆå¯èƒ½æ˜¯æ•°æ®ä¿®æ­£ï¼‰
            if date_range[0] > current_start or date_range[1] < current_end:
                return True, f"æ—¥æœŸèŒƒå›´å˜åŒ–ï¼ˆå¯èƒ½æ˜¯æ•°æ®ä¿®æ­£ï¼‰"
        
        # æ£€æŸ¥æ˜¯å¦ä»æœªå¤„ç†è¿‡
        if symbol not in self.stock_status:
            return True, "é¦–æ¬¡å¤„ç†"
        
        # æ£€æŸ¥ä¸Šæ¬¡å¤„ç†æ˜¯å¦æˆåŠŸ
        if not self.stock_status[symbol].get('success', False):
            return True, "ä¸Šæ¬¡å¤„ç†å¤±è´¥"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æŒ‡æ ‡æ•°é‡å˜åŒ–ï¼ˆå¯èƒ½ç®—æ³•æ›´æ–°ï¼‰
        current_indicators_count = self.stock_status[symbol].get('indicators_count', 0)
        expected_indicators_count = 695  # é¢„æœŸçš„æŒ‡æ ‡æ•°é‡
        if current_indicators_count != expected_indicators_count:
            return True, f"æŒ‡æ ‡æ•°é‡å˜åŒ–ï¼ˆå½“å‰: {current_indicators_count}, é¢„æœŸ: {expected_indicators_count}ï¼‰"
        
        # æ£€æŸ¥æœ€åæ›´æ–°æ—¶é—´ï¼ˆå¯é€‰ï¼šåŸºäºæ—¶é—´é—´éš”çš„æ›´æ–°ï¼‰
        last_update = self.stock_status[symbol].get('last_update')
        if last_update:
            try:
                last_update_time = datetime.fromisoformat(last_update)
                days_since_update = (datetime.now() - last_update_time).days
                # å¦‚æœè¶…è¿‡30å¤©æ²¡æœ‰æ›´æ–°ï¼Œå»ºè®®é‡æ–°è®¡ç®—ï¼ˆå¯é€‰ç­–ç•¥ï¼‰
                if days_since_update > 30:
                    return True, f"é•¿æ—¶é—´æœªæ›´æ–°ï¼ˆ{days_since_update}å¤©ï¼‰"
            except Exception:
                pass
        
        return False, "æ— éœ€æ›´æ–°"
    
    def _backup_output_file(self, output_file: str) -> str:
        """å¤‡ä»½è¾“å‡ºæ–‡ä»¶"""
        if not os.path.exists(output_file):
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = self.output_backup_dir / f"backup_{timestamp}_{Path(output_file).name}"
        
        try:
            shutil.copy2(output_file, backup_file)
            logger.info(f"âœ… å¤‡ä»½æ–‡ä»¶: {backup_file}")
            return str(backup_file)
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _merge_with_existing_output(self, new_data: pd.DataFrame, output_file: str) -> pd.DataFrame:
        """
        ä¸ç°æœ‰è¾“å‡ºæ–‡ä»¶åˆå¹¶
        åŸºäº"Stock X Date X Indicator"ç»´åº¦è¿›è¡Œæ™ºèƒ½åˆå¹¶
        
        Parameters:
        -----------
        new_data : pd.DataFrame
            æ–°è®¡ç®—çš„æ•°æ®
        output_file : str
            ç°æœ‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
        --------
        pd.DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        if not os.path.exists(output_file):
            return new_data
        
        try:
            # è¯»å–ç°æœ‰æ•°æ®ï¼šå…ˆè¯»å–å­—æ®µåï¼Œç„¶åè·³è¿‡ä¸­æ–‡æ ‡ç­¾è¡Œè¯»å–æ•°æ®
            # ç¬¬ä¸€æ­¥ï¼šè¯»å–å­—æ®µå
            with open(output_file, 'r', encoding='utf-8-sig') as f:
                header_line = f.readline().strip()
                column_names = header_line.split(',')
            
            # ç¬¬äºŒæ­¥ï¼šè¯»å–æ•°æ®ï¼ˆè·³è¿‡å­—æ®µåå’Œä¸­æ–‡æ ‡ç­¾è¡Œï¼‰
            existing_data = pd.read_csv(output_file, skiprows=2, names=column_names, low_memory=False)
            
            logger.info(f"ğŸ“‹ ç°æœ‰æ•°æ®: {len(existing_data)} è¡Œ")
            logger.info(f"ğŸ“‹ æ–°æ•°æ®: {len(new_data)} è¡Œ")
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_columns = ['Symbol', 'Date']
            missing_columns = []
            for col in required_columns:
                if col not in existing_data.columns:
                    missing_columns.append(f"ç°æœ‰æ•°æ®ç¼ºå°‘: {col}")
                if col not in new_data.columns:
                    missing_columns.append(f"æ–°æ•°æ®ç¼ºå°‘: {col}")
            
            if missing_columns:
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {', '.join(missing_columns)}")
                logger.info(f"ğŸ“‹ ç°æœ‰æ•°æ®åˆ—: {list(existing_data.columns[:10])}...")
                logger.info(f"ğŸ“‹ æ–°æ•°æ®åˆ—: {list(new_data.columns[:10])}...")
                # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œä¿ç•™ç°æœ‰æ•°æ®è€Œä¸æ˜¯è¦†ç›–
                logger.warning("ğŸ”„ åˆå¹¶å¤±è´¥ï¼Œä¿ç•™ç°æœ‰æ•°æ®")
                return existing_data
            
            # åˆ›å»ºå¤åˆé”®ï¼šè‚¡ç¥¨+æ—¥æœŸï¼ˆåŸºäº"Stock X Date X Indicator"ç»´åº¦ï¼‰
            existing_data['composite_key'] = existing_data['Symbol'].astype(str) + '_' + existing_data['Date'].astype(str)
            new_data['composite_key'] = new_data['Symbol'].astype(str) + '_' + new_data['Date'].astype(str)
            
            # ç»Ÿè®¡é‡å¤è®°å½•
            duplicate_keys = set(existing_data['composite_key']) & set(new_data['composite_key'])
            logger.info(f"ğŸ”„ å‘ç°é‡å¤è®°å½•: {len(duplicate_keys)} æ¡")
            
            # ç§»é™¤ç°æœ‰æ•°æ®ä¸­çš„é‡å¤è®°å½•ï¼ˆæ–°æ•°æ®ä¼˜å…ˆï¼‰
            existing_data = existing_data[~existing_data['composite_key'].isin(new_data['composite_key'])]
            
            # åˆå¹¶æ•°æ®
            combined_data = pd.concat([existing_data.drop('composite_key', axis=1), 
                                     new_data.drop('composite_key', axis=1)], 
                                    ignore_index=True, sort=False)
            
            # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç æ’åº
            if 'Date' in combined_data.columns and 'Symbol' in combined_data.columns:
                combined_data = combined_data.sort_values(['Date', 'Symbol']).reset_index(drop=True)
            
            logger.info(f"âœ… åˆå¹¶å®Œæˆ: æ–°å¢ {len(new_data)} è¡Œï¼Œä¿ç•™ {len(existing_data)} è¡Œï¼Œæ€»è®¡ {len(combined_data)} è¡Œ")
            
            return combined_data
            
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶æ•°æ®å¤±è´¥: {e}")
            logger.warning("ğŸ”„ åˆå¹¶å¤±è´¥ï¼Œå°è¯•è¯»å–ç°æœ‰æ•°æ®")
            try:
                if os.path.exists(output_file):
                    # æ­£ç¡®è¯»å–å¸¦æœ‰åˆ—åçš„ç°æœ‰æ•°æ®
                    with open(output_file, 'r', encoding='utf-8-sig') as f:
                        header_line = f.readline().strip()
                        column_names = header_line.split(',')
                    existing_data = pd.read_csv(output_file, skiprows=2, names=column_names, low_memory=False)
                    if not existing_data.empty:
                        logger.info(f"âœ… æˆåŠŸè¯»å–ç°æœ‰æ•°æ®: {len(existing_data)} è¡Œ")
                        return existing_data
                    else:
                        logger.warning("âš ï¸ ç°æœ‰æ–‡ä»¶ä¸ºç©º")
                        return new_data if not new_data.empty else pd.DataFrame()
                else:
                    logger.warning("âš ï¸ ç°æœ‰æ–‡ä»¶ä¸å­˜åœ¨")
                    return new_data if not new_data.empty else pd.DataFrame()
            except Exception as read_error:
                logger.error(f"âŒ è¯»å–ç°æœ‰æ•°æ®ä¹Ÿå¤±è´¥: {read_error}")
                return new_data if not new_data.empty else pd.DataFrame()
    
    def _load_financial_data(self):
        """åŠ è½½è´¢åŠ¡æ•°æ®åˆ°å†…å­˜ç¼“å­˜"""
        try:
            if self.financial_data_dir is None:
                logger.warning("æœªæŒ‡å®šè´¢åŠ¡æ•°æ®ç›®å½•ï¼Œå°†ä½¿ç”¨ä¼°ç®—å€¼")
                return
                
            logger.info(f"æ­£åœ¨åŠ è½½è´¢åŠ¡æ•°æ®ä»: {self.financial_data_dir}")
            
            # åŠ è½½å„ç§è´¢åŠ¡æ•°æ®
            data_types = ['info', 'financials', 'balance_sheet', 'cashflow', 'dividends', 'financial_ratios']
            
            for data_type in data_types:
                data_path = self.financial_data_dir / data_type
                if data_path.exists():
                    self._financial_data_cache[data_type] = {}
                    
                    # è¯»å–CSVæ–‡ä»¶
                    csv_files = list(data_path.glob("*.csv"))
                    if csv_files:
                        for csv_file in csv_files:
                            symbol = csv_file.stem.upper()
                            try:
                                df = pd.read_csv(csv_file, index_col=0)
                                self._financial_data_cache[data_type][symbol] = df
                            except Exception as e:
                                logger.warning(f"Failed to load {data_type} for {symbol}: {e}")
                        
                        logger.info(f"âœ… åŠ è½½ {data_type} æ•°æ®: {len(self._financial_data_cache[data_type])} åªè‚¡ç¥¨")
                    else:
                        logger.warning(f"ğŸ“ {data_type} ç›®å½•ä¸ºç©º")
                else:
                    logger.warning(f"ğŸ“ è´¢åŠ¡æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
                    
        except Exception as e:
            logger.error(f"åŠ è½½è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            self._financial_data_cache = {}
    
    def read_qlib_binary_data(self, symbol: str) -> Optional[pd.DataFrame]:
        """è¯»å–QlibäºŒè¿›åˆ¶æ•°æ®"""
        symbol_dir = self.features_dir / symbol.lower()
        
        if not symbol_dir.exists():
            return None
        
        features = ['open', 'high', 'low', 'close', 'volume']
        data_dict = {}
        
        try:
            for feature in features:
                bin_file = symbol_dir / f"{feature}.day.bin"
                if bin_file.exists():
                    with open(bin_file, 'rb') as f:
                        values = []
                        while True:
                            data = f.read(4)
                            if not data:
                                break
                            value = struct.unpack('<f', data)[0]
                            if not np.isnan(value) and not np.isinf(value):
                                values.append(value)
                            else:
                                values.append(0.0)
                        data_dict[feature.title()] = values
            
            if not data_dict:
                return None
            
            # Ensure all arrays have the same length
            lengths = [len(v) for v in data_dict.values()]
            if len(set(lengths)) > 1:
                min_length = min(lengths)
                for key in data_dict:
                    data_dict[key] = data_dict[key][:min_length]
            
            df = pd.DataFrame(data_dict)
            
            # Read actual calendar dates from qlib
            calendar_file = self.data_dir / "calendars" / "day.txt"
            if calendar_file.exists():
                with open(calendar_file, 'r') as f:
                    calendar_dates = [line.strip() for line in f.readlines()]
                calendar_dates = [pd.to_datetime(date) for date in calendar_dates if date.strip()]
                
                # The data in qlib is typically aligned with the calendar in reverse order
                if len(df) <= len(calendar_dates):
                    dates = calendar_dates[-len(df):]
                else:
                    # If data is longer than calendar, extend backwards
                    latest_date = calendar_dates[-1] if calendar_dates else pd.to_datetime('2025-06-27')
                    all_dates = pd.bdate_range(end=latest_date, periods=len(df), freq='B')
                    dates = all_dates.tolist()
            else:
                # Fallback: generate business days ending at a reasonable date
                end_date = pd.to_datetime('2025-06-27')
                dates = pd.bdate_range(end=end_date, periods=len(df), freq='B')
            
            df.index = pd.DatetimeIndex(dates)
            
            # åº”ç”¨æ—¶é—´çª—å£è¿‡æ»¤
            df = self._apply_time_window_filter(df)
            
            return df
            
        except Exception as e:
            logger.warning(f"Failed to read binary data {symbol}: {e}")
            return None
    
    def get_available_stocks(self) -> List[str]:
        """è·å–å¯ç”¨è‚¡ç¥¨åˆ—è¡¨"""
        stocks = []
        
        if not self.features_dir.exists():
            logger.error(f"Features directory does not exist: {self.features_dir}")
            return stocks
        
        for stock_dir in self.features_dir.iterdir():
            if stock_dir.is_dir():
                required_files = ['open.day.bin', 'high.day.bin', 'low.day.bin', 'close.day.bin', 'volume.day.bin']
                if all((stock_dir / file).exists() for file in required_files):
                    symbol = stock_dir.name.upper()
                    stocks.append(symbol)
        
        logger.info(f"Found {len(stocks)} stocks data")
        return sorted(stocks)
    
    def get_financial_data(self, symbol: str, data_type: str) -> Optional[pd.DataFrame]:
        """è·å–è´¢åŠ¡æ•°æ®"""
        try:
            # å°è¯•å¤šç§ç¬¦å·æ ¼å¼
            symbol_variants = [
                symbol,                        # åŸå§‹ç¬¦å·
                symbol.replace('_', '.'),      # ä¸‹åˆ’çº¿è½¬ç‚¹å· (0002_HK -> 0002.HK)
                symbol.replace('.', '_'),      # ç‚¹å·è½¬ä¸‹åˆ’çº¿ (0002.HK -> 0002_HK)
                symbol.upper(),                # å¤§å†™
                symbol.replace('_HK', '.HK'),  # ç‰¹å®šè½¬æ¢
                symbol.replace('.HK', '_HK')   # ç‰¹å®šè½¬æ¢
            ]
            
            if data_type in self._financial_data_cache:
                for variant in symbol_variants:
                    if variant in self._financial_data_cache[data_type]:
                        return self._financial_data_cache[data_type][variant]
            return None
        except Exception as e:
            logger.warning(f"Failed to get financial data for {symbol}, {data_type}: {e}")
            return None
    
    def _safe_divide(self, a, b, fill_value=0.0):
        """å®‰å…¨é™¤æ³•æ“ä½œï¼Œé¿å…é™¤é›¶é”™è¯¯"""
        return np.where(np.abs(b) > 1e-12, a / b, fill_value)
    
    def _get_calculated_indicators(self):
        """è·å–çº¿ç¨‹æœ¬åœ°çš„æŒ‡æ ‡é›†åˆ"""
        if not hasattr(self._local, 'calculated_indicators'):
            self._local.calculated_indicators = set()
        return self._local.calculated_indicators
    
    def _add_indicator(self, indicators: dict, name: str, values):
        """æ·»åŠ æŒ‡æ ‡åˆ°å­—å…¸ä¸­ï¼Œé¿å…é‡å¤ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        calculated_indicators = self._get_calculated_indicators()
        if name not in calculated_indicators:
            indicators[name] = values
            calculated_indicators.add(name)
        else:
            logger.debug(f"æŒ‡æ ‡ {name} å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤è®¡ç®—")
    
    def _reset_indicators_cache(self):
        """é‡ç½®çº¿ç¨‹æœ¬åœ°çš„æŒ‡æ ‡ç¼“å­˜"""
        if hasattr(self._local, 'calculated_indicators'):
            self._local.calculated_indicators.clear()
    
    def calculate_all_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡ï¼ˆå…±çº¦60ä¸ªï¼‰"""
        if data.empty or len(data) < 50:
            logger.warning("Insufficient data for calculating technical indicators")
            return pd.DataFrame()
        
        try:
            indicators = {}
            
            # Clean data and convert to numpy arrays for talib
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            volume = data['Volume'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0).values
            
            # 1. Moving Averages (ç§»åŠ¨å¹³å‡çº¿ç±») - 12ä¸ª
            self._add_indicator(indicators, 'SMA_5', talib.SMA(close, timeperiod=5))
            self._add_indicator(indicators, 'SMA_10', talib.SMA(close, timeperiod=10))
            self._add_indicator(indicators, 'SMA_20', talib.SMA(close, timeperiod=20))
            self._add_indicator(indicators, 'SMA_50', talib.SMA(close, timeperiod=50))
            
            self._add_indicator(indicators, 'EMA_5', talib.EMA(close, timeperiod=5))
            self._add_indicator(indicators, 'EMA_10', talib.EMA(close, timeperiod=10))
            self._add_indicator(indicators, 'EMA_20', talib.EMA(close, timeperiod=20))
            self._add_indicator(indicators, 'EMA_50', talib.EMA(close, timeperiod=50))
            
            self._add_indicator(indicators, 'DEMA_20', talib.DEMA(close, timeperiod=20))
            self._add_indicator(indicators, 'TEMA_20', talib.TEMA(close, timeperiod=20))
            self._add_indicator(indicators, 'KAMA_30', talib.KAMA(close, timeperiod=30))
            self._add_indicator(indicators, 'WMA_20', talib.WMA(close, timeperiod=20))
            
            # 2. MACD Family - 3ä¸ª
            indicators['MACD'], indicators['MACD_Signal'], indicators['MACD_Histogram'] = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
            indicators['MACDEXT'], _, _ = talib.MACDEXT(close, fastperiod=12, slowperiod=26, signalperiod=9)
            indicators['MACDFIX'], _, _ = talib.MACDFIX(close, signalperiod=9)
            
            # 3. Momentum Oscillators (åŠ¨é‡æŒ¯è¡å™¨) - 6ä¸ª
            indicators['RSI_14'] = talib.RSI(close, timeperiod=14)
            indicators['CCI_14'] = talib.CCI(high, low, close, timeperiod=14)
            indicators['CMO_14'] = talib.CMO(close, timeperiod=14)
            indicators['MFI_14'] = talib.MFI(high, low, close, volume, timeperiod=14)
            indicators['WILLR_14'] = talib.WILLR(high, low, close, timeperiod=14)
            indicators['ULTOSC'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)
            
            # 4. Trend Indicators (è¶‹åŠ¿æŒ‡æ ‡) - 13ä¸ª
            indicators['ADX_14'] = talib.ADX(high, low, close, timeperiod=14)
            indicators['ADXR_14'] = talib.ADXR(high, low, close, timeperiod=14)
            indicators['APO'] = talib.APO(close, fastperiod=12, slowperiod=26)
            indicators['AROON_DOWN'], indicators['AROON_UP'] = talib.AROON(high, low, timeperiod=14)
            indicators['AROONOSC_14'] = talib.AROONOSC(high, low, timeperiod=14)
            indicators['BOP'] = talib.BOP(open_price, high, low, close)
            indicators['DX_14'] = talib.DX(high, low, close, timeperiod=14)
            indicators['MINUS_DI_14'] = talib.MINUS_DI(high, low, close, timeperiod=14)
            indicators['MINUS_DM_14'] = talib.MINUS_DM(high, low, timeperiod=14)
            indicators['PLUS_DI_14'] = talib.PLUS_DI(high, low, close, timeperiod=14)
            indicators['PLUS_DM_14'] = talib.PLUS_DM(high, low, timeperiod=14)
            indicators['PPO'] = talib.PPO(close, fastperiod=12, slowperiod=26)
            indicators['TRIX_30'] = talib.TRIX(close, timeperiod=30)
            
            # 5. Momentum Indicators (åŠ¨é‡æŒ‡æ ‡) - 5ä¸ª
            indicators['MOM_10'] = talib.MOM(close, timeperiod=10)
            indicators['ROC_10'] = talib.ROC(close, timeperiod=10)
            indicators['ROCP_10'] = talib.ROCP(close, timeperiod=10)
            indicators['ROCR_10'] = talib.ROCR(close, timeperiod=10)
            indicators['ROCR100_10'] = talib.ROCR100(close, timeperiod=10)
            
            # 6. Bollinger Bands - 3ä¸ª
            indicators['BB_Upper'], indicators['BB_Middle'], indicators['BB_Lower'] = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2)
            
            # 7. Stochastic (éšæœºæŒ‡æ ‡) - 6ä¸ª
            indicators['STOCH_K'], indicators['STOCH_D'] = talib.STOCH(high, low, close, fastk_period=14, slowk_period=3, slowd_period=3)
            indicators['STOCHF_K'], indicators['STOCHF_D'] = talib.STOCHF(high, low, close, fastk_period=14, fastd_period=3)
            indicators['STOCHRSI_K'], indicators['STOCHRSI_D'] = talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3)
            
            # 8. Volatility Indicators (æ³¢åŠ¨ç‡æŒ‡æ ‡) - 3ä¸ª
            indicators['ATR_14'] = talib.ATR(high, low, close, timeperiod=14)
            indicators['NATR_14'] = talib.NATR(high, low, close, timeperiod=14)
            indicators['TRANGE'] = talib.TRANGE(high, low, close)
            
            # 9. Volume Indicators (æˆäº¤é‡æŒ‡æ ‡) - 3ä¸ª
            indicators['OBV'] = talib.OBV(close, volume)
            indicators['AD'] = talib.AD(high, low, close, volume)
            indicators['ADOSC'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)
            
            # 10. Hilbert Transform (å¸Œå°”ä¼¯ç‰¹å˜æ¢) - 7ä¸ª
            indicators['HT_DCPERIOD'] = talib.HT_DCPERIOD(close)
            indicators['HT_DCPHASE'] = talib.HT_DCPHASE(close)
            indicators['HT_INPHASE'], indicators['HT_QUADRATURE'] = talib.HT_PHASOR(close)
            indicators['HT_SINE'], indicators['HT_LEADSINE'] = talib.HT_SINE(close)
            indicators['HT_TRENDMODE'] = talib.HT_TRENDMODE(close)
            indicators['HT_TRENDLINE'] = talib.HT_TRENDLINE(close)
            
            # 11. Math Transform (æ•°å­¦å˜æ¢) - 8ä¸ª
            indicators['AVGPRICE'] = talib.AVGPRICE(open_price, high, low, close)
            indicators['MEDPRICE'] = talib.MEDPRICE(high, low)
            indicators['TYPPRICE'] = talib.TYPPRICE(high, low, close)
            indicators['WCLPRICE'] = talib.WCLPRICE(high, low, close)
            indicators['MIDPOINT'] = talib.MIDPOINT(close, timeperiod=14)
            indicators['MIDPRICE'] = talib.MIDPRICE(high, low, timeperiod=14)
            indicators['MAMA'], indicators['FAMA'] = talib.MAMA(close)
            
            # 12. Statistical Functions (ç»Ÿè®¡å‡½æ•°) - 7ä¸ª
            indicators['LINEARREG'] = talib.LINEARREG(close, timeperiod=14)
            indicators['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(close, timeperiod=14)
            indicators['LINEARREG_INTERCEPT'] = talib.LINEARREG_INTERCEPT(close, timeperiod=14)
            indicators['LINEARREG_SLOPE'] = talib.LINEARREG_SLOPE(close, timeperiod=14)
            indicators['STDDEV'] = talib.STDDEV(close, timeperiod=30)
            indicators['TSF'] = talib.TSF(close, timeperiod=14)
            indicators['VAR'] = talib.VAR(close, timeperiod=30)
            
            # 13. Min/Max Functions - 2ä¸ª
            indicators['MAXINDEX'] = talib.MAXINDEX(close, timeperiod=30)
            indicators['MININDEX'] = talib.MININDEX(close, timeperiod=30)
            
            # è½¬æ¢ä¸ºDataFrame
            indicators_df = pd.DataFrame(indicators, index=data.index)
            
            logger.info(f"è®¡ç®—äº† {len(indicators)} ä¸ªæŠ€æœ¯æŒ‡æ ‡")
            return indicators_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_alpha158_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—Alpha158æŒ‡æ ‡ä½“ç³» (158ä¸ªæŒ‡æ ‡)
        åŒ…æ‹¬KBARæŒ‡æ ‡ã€ä»·æ ¼æŒ‡æ ‡ã€æˆäº¤é‡æŒ‡æ ‡ã€æ»šåŠ¨æŠ€æœ¯æŒ‡æ ‡
        """
        if data.empty or len(data) < 60:
            logger.warning("æ•°æ®ä¸è¶³ä»¥è®¡ç®—Alpha158æŒ‡æ ‡")
            return pd.DataFrame()
        
        try:
            indicators = {}
            
            # æ¸…ç†æ•°æ®
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            volume = data['Volume'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0).values
            
            # è®¡ç®—VWAP
            vwap = self._safe_divide(
                np.convolve(close * volume, np.ones(1), 'same'),
                np.convolve(volume, np.ones(1), 'same')
            )
            
            # 1. KBARæŒ‡æ ‡ (9ä¸ª)
            self._add_indicator(indicators, 'ALPHA158_KMID', self._safe_divide(close - open_price, open_price))
            self._add_indicator(indicators, 'ALPHA158_KLEN', self._safe_divide(high - low, open_price))
            self._add_indicator(indicators, 'ALPHA158_KMID2', self._safe_divide(close - open_price, high - low + 1e-12))
            self._add_indicator(indicators, 'ALPHA158_KUP', self._safe_divide(high - np.maximum(open_price, close), open_price))
            self._add_indicator(indicators, 'ALPHA158_KUP2', self._safe_divide(high - np.maximum(open_price, close), high - low + 1e-12))
            self._add_indicator(indicators, 'ALPHA158_KLOW', self._safe_divide(np.minimum(open_price, close) - low, open_price))
            self._add_indicator(indicators, 'ALPHA158_KLOW2', self._safe_divide(np.minimum(open_price, close) - low, high - low + 1e-12))
            self._add_indicator(indicators, 'ALPHA158_KSFT', self._safe_divide(2 * close - high - low, open_price))
            self._add_indicator(indicators, 'ALPHA158_KSFT2', self._safe_divide(2 * close - high - low, high - low + 1e-12))
            
            # 2. ä»·æ ¼æŒ‡æ ‡ (æ ‡å‡†åŒ–åˆ°æ”¶ç›˜ä»·)
            features = ['OPEN', 'HIGH', 'LOW', 'VWAP']
            for feature in features:
                if feature == 'OPEN':
                    price_values = open_price
                elif feature == 'HIGH':
                    price_values = high
                elif feature == 'LOW':
                    price_values = low
                elif feature == 'VWAP':
                    price_values = vwap
                
                self._add_indicator(indicators, f'ALPHA158_{feature}0', self._safe_divide(price_values, close))
            
            # 3. æˆäº¤é‡æŒ‡æ ‡
            self._add_indicator(indicators, 'ALPHA158_VOLUME0', self._safe_divide(volume, volume + 1e-12))
            
            # 4. æ»šåŠ¨æŠ€æœ¯æŒ‡æ ‡
            windows = [5, 10, 20, 30, 60]
            
            # ROC - Rate of Change
            for d in windows:
                ref_close = np.roll(close, d)
                self._add_indicator(indicators, f'ALPHA158_ROC{d}', self._safe_divide(ref_close, close))
            
            # MA - Simple Moving Average
            for d in windows:
                ma_values = pd.Series(close).rolling(window=d, min_periods=1).mean().values
                self._add_indicator(indicators, f'ALPHA158_MA{d}', self._safe_divide(ma_values, close))
            
            # STD - Standard Deviation
            for d in windows:
                std_values = pd.Series(close).rolling(window=d, min_periods=1).std().fillna(0).values
                self._add_indicator(indicators, f'ALPHA158_STD{d}', self._safe_divide(std_values, close))
            
            # BETA - Slope
            for d in windows:
                beta_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    x = np.arange(d)
                    y = close[i-d+1:i+1]
                    if len(y) > 1:
                        try:
                            slope = np.polyfit(x, y, 1)[0]
                            beta_values[i] = slope
                        except:
                            beta_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_BETA{d}', self._safe_divide(beta_values, close))
            
            # RSQR - R-square
            for d in windows:
                rsqr_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    x = np.arange(d)
                    y = close[i-d+1:i+1]
                    if len(y) > 1:
                        try:
                            correlation_matrix = np.corrcoef(x, y)
                            rsqr_values[i] = correlation_matrix[0, 1] ** 2 if not np.isnan(correlation_matrix[0, 1]) else 0
                        except:
                            rsqr_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_RSQR{d}', rsqr_values)
            
            # MAX/MIN
            for d in windows:
                max_values = pd.Series(high).rolling(window=d, min_periods=1).max().values
                min_values = pd.Series(low).rolling(window=d, min_periods=1).min().values
                self._add_indicator(indicators, f'ALPHA158_MAX{d}', self._safe_divide(max_values, close))
                self._add_indicator(indicators, f'ALPHA158_MIN{d}', self._safe_divide(min_values, close))
            
            # QTLU/QTLD - Quantiles
            for d in windows:
                qtlu_values = pd.Series(close).rolling(window=d, min_periods=1).quantile(0.8).values
                qtld_values = pd.Series(close).rolling(window=d, min_periods=1).quantile(0.2).values
                self._add_indicator(indicators, f'ALPHA158_QTLU{d}', self._safe_divide(qtlu_values, close))
                self._add_indicator(indicators, f'ALPHA158_QTLD{d}', self._safe_divide(qtld_values, close))
            
            # RANK - Percentile rank
            for d in windows:
                rank_values = pd.Series(close).rolling(window=d, min_periods=1).rank(pct=True).values
                self._add_indicator(indicators, f'ALPHA158_RANK{d}', rank_values)
            
            # RSV - Relative Strength Value
            for d in windows:
                min_low = pd.Series(low).rolling(window=d, min_periods=1).min().values
                max_high = pd.Series(high).rolling(window=d, min_periods=1).max().values
                self._add_indicator(indicators, f'ALPHA158_RSV{d}', self._safe_divide(close - min_low, max_high - min_low + 1e-12))
            
            # RESI - Linear Regression Residual
            for d in windows:
                resi_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    x = np.arange(d)
                    y = close[i-d+1:i+1]
                    if len(y) > 1:
                        try:
                            slope, intercept = np.polyfit(x, y, 1)
                            predicted = slope * (d-1) + intercept
                            resi_values[i] = y[-1] - predicted
                        except:
                            resi_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_RESI{d}', self._safe_divide(resi_values, close))
            
            # IMAX - Index of Maximum
            for d in windows:
                imax_values = np.zeros_like(close)
                for i in range(d, len(high)):
                    window_high = high[i-d+1:i+1]
                    if len(window_high) > 0:
                        imax_values[i] = (len(window_high) - 1 - np.argmax(window_high)) / d
                self._add_indicator(indicators, f'ALPHA158_IMAX{d}', imax_values)
            
            # IMIN - Index of Minimum  
            for d in windows:
                imin_values = np.zeros_like(close)
                for i in range(d, len(low)):
                    window_low = low[i-d+1:i+1]
                    if len(window_low) > 0:
                        imin_values[i] = (len(window_low) - 1 - np.argmin(window_low)) / d
                self._add_indicator(indicators, f'ALPHA158_IMIN{d}', imin_values)
            
            # IMXD - Index Max - Index Min Difference
            for d in windows:
                imxd_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_high = high[i-d+1:i+1] 
                    window_low = low[i-d+1:i+1]
                    if len(window_high) > 0 and len(window_low) > 0:
                        idx_max = len(window_high) - 1 - np.argmax(window_high)
                        idx_min = len(window_low) - 1 - np.argmin(window_low)
                        imxd_values[i] = (idx_max - idx_min) / d
                self._add_indicator(indicators, f'ALPHA158_IMXD{d}', imxd_values)
            
            # CORR - Correlation between close and log(volume)
            for d in windows:
                corr_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_close = close[i-d+1:i+1]
                    window_volume = volume[i-d+1:i+1]
                    if len(window_close) > 1:
                        try:
                            log_volume = np.log(window_volume + 1)
                            if np.std(window_close) > 1e-8 and np.std(log_volume) > 1e-8:
                                corr_values[i] = np.corrcoef(window_close, log_volume)[0, 1]
                        except:
                            corr_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_CORR{d}', corr_values)
            
            # CORD - Correlation between price change and volume change
            for d in windows:
                cord_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    if i >= d:
                        close_change = close[i-d+2:i+1] / close[i-d+1:i]
                        volume_change = np.log((volume[i-d+2:i+1] / (volume[i-d+1:i] + 1e-12)) + 1)
                        if len(close_change) > 1:
                            try:
                                if np.std(close_change) > 1e-8 and np.std(volume_change) > 1e-8:
                                    cord_values[i] = np.corrcoef(close_change, volume_change)[0, 1]
                            except:
                                cord_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_CORD{d}', cord_values)
            
            # CNTP - Count of Positive returns
            for d in windows:
                cntp_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_returns = close[i-d+2:i+1] > close[i-d+1:i]
                    if len(window_returns) > 0:
                        cntp_values[i] = np.mean(window_returns)
                self._add_indicator(indicators, f'ALPHA158_CNTP{d}', cntp_values)
            
            # CNTN - Count of Negative returns
            for d in windows:
                cntn_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_returns = close[i-d+2:i+1] < close[i-d+1:i]
                    if len(window_returns) > 0:
                        cntn_values[i] = np.mean(window_returns)
                self._add_indicator(indicators, f'ALPHA158_CNTN{d}', cntn_values)
            
            # CNTD - Count Difference (CNTP - CNTN)
            for d in windows:
                cntd_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_pos = close[i-d+2:i+1] > close[i-d+1:i]
                    window_neg = close[i-d+2:i+1] < close[i-d+1:i]
                    if len(window_pos) > 0:
                        cntd_values[i] = np.mean(window_pos) - np.mean(window_neg)
                self._add_indicator(indicators, f'ALPHA158_CNTD{d}', cntd_values)
            
            # SUMP - Sum of Positive returns ratio
            for d in windows:
                sump_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    changes = close[i-d+2:i+1] - close[i-d+1:i]
                    if len(changes) > 0:
                        positive_sum = np.sum(np.maximum(changes, 0))
                        total_abs_sum = np.sum(np.abs(changes))
                        sump_values[i] = self._safe_divide(positive_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_SUMP{d}', sump_values)
            
            # SUMN - Sum of Negative returns ratio  
            for d in windows:
                sumn_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    changes = close[i-d+2:i+1] - close[i-d+1:i]
                    if len(changes) > 0:
                        negative_sum = np.sum(np.maximum(-changes, 0))
                        total_abs_sum = np.sum(np.abs(changes))
                        sumn_values[i] = self._safe_divide(negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_SUMN{d}', sumn_values)
            
            # SUMD - Sum Difference (SUMP - SUMN)
            for d in windows:
                sumd_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    changes = close[i-d+2:i+1] - close[i-d+1:i]
                    if len(changes) > 0:
                        positive_sum = np.sum(np.maximum(changes, 0))
                        negative_sum = np.sum(np.maximum(-changes, 0))
                        total_abs_sum = np.sum(np.abs(changes))
                        sumd_values[i] = self._safe_divide(positive_sum - negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_SUMD{d}', sumd_values)
            
            # VMA - Volume Moving Average
            for d in windows:
                vma_values = pd.Series(volume).rolling(window=d, min_periods=1).mean().values
                self._add_indicator(indicators, f'ALPHA158_VMA{d}', self._safe_divide(vma_values, volume + 1e-12))
            
            # VSTD - Volume Standard Deviation
            for d in windows:
                vstd_values = pd.Series(volume).rolling(window=d, min_periods=1).std().fillna(0).values
                self._add_indicator(indicators, f'ALPHA158_VSTD{d}', self._safe_divide(vstd_values, volume + 1e-12))
            
            # WVMA - Weighted Volume Moving Average (price change volatility weighted by volume)
            for d in windows:
                wvma_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    price_changes = np.abs(close[i-d+2:i+1] / close[i-d+1:i] - 1)
                    weights = volume[i-d+2:i+1]
                    if len(price_changes) > 0:
                        weighted_changes = price_changes * weights
                        mean_weighted = np.mean(weighted_changes)
                        std_weighted = np.std(weighted_changes)
                        wvma_values[i] = self._safe_divide(std_weighted, mean_weighted + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_WVMA{d}', wvma_values)
            
            # VSUMP - Volume Sum Positive ratio
            for d in windows:
                vsump_values = np.zeros_like(close)
                for i in range(d, len(volume)):
                    vol_changes = volume[i-d+2:i+1] - volume[i-d+1:i] 
                    if len(vol_changes) > 0:
                        positive_sum = np.sum(np.maximum(vol_changes, 0))
                        total_abs_sum = np.sum(np.abs(vol_changes))
                        vsump_values[i] = self._safe_divide(positive_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_VSUMP{d}', vsump_values)
            
            # VSUMN - Volume Sum Negative ratio
            for d in windows:
                vsumn_values = np.zeros_like(close)
                for i in range(d, len(volume)):
                    vol_changes = volume[i-d+2:i+1] - volume[i-d+1:i]
                    if len(vol_changes) > 0:
                        negative_sum = np.sum(np.maximum(-vol_changes, 0))
                        total_abs_sum = np.sum(np.abs(vol_changes))
                        vsumn_values[i] = self._safe_divide(negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_VSUMN{d}', vsumn_values)
            
            # VSUMD - Volume Sum Difference (VSUMP - VSUMN)
            for d in windows:
                vsumd_values = np.zeros_like(close)
                for i in range(d, len(volume)):
                    vol_changes = volume[i-d+2:i+1] - volume[i-d+1:i]
                    if len(vol_changes) > 0:
                        positive_sum = np.sum(np.maximum(vol_changes, 0))
                        negative_sum = np.sum(np.maximum(-vol_changes, 0))
                        total_abs_sum = np.sum(np.abs(vol_changes))
                        vsumd_values[i] = self._safe_divide(positive_sum - negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_VSUMD{d}', vsumd_values)
            
            # è½¬æ¢ä¸ºDataFrame
            indicators_df = pd.DataFrame(indicators, index=data.index)
            
            logger.info(f"è®¡ç®—äº†Alpha158æŒ‡æ ‡ä½“ç³»: {len(indicators)} ä¸ªæŒ‡æ ‡")
            return indicators_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—Alpha158æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_alpha360_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—Alpha360æŒ‡æ ‡ä½“ç³» (360ä¸ªæŒ‡æ ‡)
        åŒ…æ‹¬è¿‡å»60å¤©çš„æ ‡å‡†åŒ–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
        """
        if data.empty or len(data) < 60:
            logger.warning("æ•°æ®ä¸è¶³ä»¥è®¡ç®—Alpha360æŒ‡æ ‡")
            return pd.DataFrame()
        
        try:
            indicators = {}
            
            # æ¸…ç†æ•°æ®
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            volume = data['Volume'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0).values
            
            # è®¡ç®—VWAP
            vwap = self._safe_divide(
                np.convolve(close * volume, np.ones(1), 'same'),
                np.convolve(volume, np.ones(1), 'same')
            )
            
            # Alpha360: è¿‡å»60å¤©çš„ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®ï¼Œé™¤ä»¥å½“å‰æ”¶ç›˜ä»·æ ‡å‡†åŒ–
            # 1. CLOSE æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_CLOSE{i}', self._safe_divide(close, close))
                else:
                    ref_close = np.roll(close, i)
                    self._add_indicator(indicators, f'ALPHA360_CLOSE{i}', self._safe_divide(ref_close, close))
            
            # 2. OPEN æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_OPEN{i}', self._safe_divide(open_price, close))
                else:
                    ref_open = np.roll(open_price, i)
                    self._add_indicator(indicators, f'ALPHA360_OPEN{i}', self._safe_divide(ref_open, close))
            
            # 3. HIGH æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_HIGH{i}', self._safe_divide(high, close))
                else:
                    ref_high = np.roll(high, i)
                    self._add_indicator(indicators, f'ALPHA360_HIGH{i}', self._safe_divide(ref_high, close))
            
            # 4. LOW æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_LOW{i}', self._safe_divide(low, close))
                else:
                    ref_low = np.roll(low, i)
                    self._add_indicator(indicators, f'ALPHA360_LOW{i}', self._safe_divide(ref_low, close))
            
            # 5. VWAP æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_VWAP{i}', self._safe_divide(vwap, close))
                else:
                    ref_vwap = np.roll(vwap, i)
                    self._add_indicator(indicators, f'ALPHA360_VWAP{i}', self._safe_divide(ref_vwap, close))
            
            # 6. VOLUME æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_VOLUME{i}', self._safe_divide(volume, volume + 1e-12))
                else:
                    ref_volume = np.roll(volume, i)
                    self._add_indicator(indicators, f'ALPHA360_VOLUME{i}', self._safe_divide(ref_volume, volume + 1e-12))
            
            # è½¬æ¢ä¸ºDataFrame
            indicators_df = pd.DataFrame(indicators, index=data.index)
            
            logger.info(f"è®¡ç®—äº†Alpha360æŒ‡æ ‡ä½“ç³»: {len(indicators)} ä¸ªæŒ‡æ ‡")
            return indicators_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—Alpha360æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_candlestick_patterns(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—èœ¡çƒ›å›¾å½¢æ€æŒ‡æ ‡ï¼ˆå…±61ä¸ªï¼‰"""
        if data.empty or len(data) < 10:
            logger.warning("Insufficient data for calculating candlestick patterns")
            return pd.DataFrame()
        
        try:
            patterns = {}
            
            # Clean data
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            
            # æ‰€æœ‰61ä¸ªèœ¡çƒ›å›¾å½¢æ€
            candle_patterns = [
                'CDL2CROWS', 'CDL3BLACKCROWS', 'CDL3INSIDE', 'CDL3LINESTRIKE', 'CDL3OUTSIDE',
                'CDL3STARSINSOUTH', 'CDL3WHITESOLDIERS', 'CDLABANDONEDBABY', 'CDLADVANCEBLOCK',
                'CDLBELTHOLD', 'CDLBREAKAWAY', 'CDLCLOSINGMARUBOZU', 'CDLCONCEALBABYSWALL',
                'CDLCOUNTERATTACK', 'CDLDARKCLOUDCOVER', 'CDLDOJI', 'CDLDOJISTAR', 'CDLDRAGONFLYDOJI',
                'CDLENGULFING', 'CDLEVENINGDOJISTAR', 'CDLEVENINGSTAR', 'CDLGAPSIDESIDEWHITE',
                'CDLGRAVESTONEDOJI', 'CDLHAMMER', 'CDLHANGINGMAN', 'CDLHARAMI', 'CDLHARAMICROSS',
                'CDLHIGHWAVE', 'CDLHIKKAKE', 'CDLHIKKAKEMOD', 'CDLHOMINGPIGEON', 'CDLIDENTICAL3CROWS',
                'CDLINNECK', 'CDLINVERTEDHAMMER', 'CDLKICKING', 'CDLKICKINGBYLENGTH', 'CDLLADDERBOTTOM',
                'CDLLONGLEGGEDDOJI', 'CDLLONGLINE', 'CDLMARUBOZU', 'CDLMATCHINGLOW', 'CDLMATHOLD',
                'CDLMORNINGDOJISTAR', 'CDLMORNINGSTAR', 'CDLONNECK', 'CDLPIERCING', 'CDLRICKSHAWMAN',
                'CDLRISEFALL3METHODS', 'CDLSEPARATINGLINES', 'CDLSHOOTINGSTAR', 'CDLSHORTLINE',
                'CDLSPINNINGTOP', 'CDLSTALLEDPATTERN', 'CDLSTICKSANDWICH', 'CDLTAKURI', 'CDLTASUKIGAP',
                'CDLTHRUSTING', 'CDLTRISTAR', 'CDLUNIQUE3RIVER', 'CDLUPSIDEGAP2CROWS', 'CDLXSIDEGAP3METHODS'
            ]
            
            for pattern in candle_patterns:
                try:
                    patterns[pattern] = getattr(talib, pattern)(open_price, high, low, close)
                except Exception as e:
                    logger.warning(f"Failed to calculate {pattern}: {e}")
                    patterns[pattern] = np.zeros(len(data))
            
            patterns_df = pd.DataFrame(patterns, index=data.index)
            
            logger.info(f"è®¡ç®—äº† {len(patterns)} ä¸ªèœ¡çƒ›å›¾å½¢æ€æŒ‡æ ‡")
            return patterns_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—èœ¡çƒ›å›¾å½¢æ€å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_financial_indicators(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """è®¡ç®—è´¢åŠ¡æŒ‡æ ‡å’Œæ¢æ‰‹ç‡ï¼ˆçº¦15ä¸ªï¼‰- ä½¿ç”¨ä¼°ç®—å€¼æ›¿ä»£ç¼ºå¤±æ•°æ®"""
        try:
            result_data = data.copy()
            
            # é¢„å…ˆåˆå§‹åŒ–æ‰€æœ‰è´¢åŠ¡æŒ‡æ ‡åˆ—
            financial_columns = [
                'PriceToBookRatio', 'MarketCap', 'PERatio', 'PriceToSalesRatio',
                'ROE', 'ROA', 'ProfitMargins', 'CurrentRatio', 'QuickRatio', 
                'DebtToEquity', 'TobinsQ', 'DailyTurnover', 
                'turnover_c1d', 'turnover_c5d', 'turnover_c10d', 'turnover_c20d', 'turnover_c30d',
                'turnover_m5d', 'turnover_m10d', 'turnover_m20d', 'turnover_m30d'
            ]
            
            # è·å–åŸºæœ¬ä¿¡æ¯æ•°æ®
            info_data = self.get_financial_data(symbol, 'info')
            balance_sheet_data = self.get_financial_data(symbol, 'balance_sheet')
            
            # å¦‚æœæœ‰çœŸå®è´¢åŠ¡æ•°æ®ï¼Œä½¿ç”¨çœŸå®æ•°æ®
            if info_data is not None and not info_data.empty:
                result_data = self._calculate_real_financial_indicators(result_data, info_data, balance_sheet_data)
            else:
                # å¦åˆ™ä½¿ç”¨åŸºäºä»·æ ¼å’Œæˆäº¤é‡çš„ä¼°ç®—æŒ‡æ ‡
                result_data = self._calculate_estimated_financial_indicators(result_data, symbol)
            
            # ç¡®ä¿æ‰€æœ‰è´¢åŠ¡æŒ‡æ ‡åˆ—éƒ½å­˜åœ¨ä¸”æœ‰é»˜è®¤å€¼
            result_data = self._ensure_financial_columns_exist(result_data, symbol)
            
            logger.info(f"âœ… å®Œæˆè´¢åŠ¡æŒ‡æ ‡è®¡ç®— (åŒ…å«ä¼°ç®—å€¼)")
            return result_data
            
        except Exception as e:
            logger.error(f"è®¡ç®—è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿè¦ç¡®ä¿åˆ—å­˜åœ¨
            for col in financial_columns:
                if col not in data.columns:
                    data[col] = np.nan
            return data
    
    def _calculate_real_financial_indicators(self, data: pd.DataFrame, info_data: pd.DataFrame, balance_sheet_data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨çœŸå®è´¢åŠ¡æ•°æ®è®¡ç®—æŒ‡æ ‡"""
        result_data = data.copy()
        
        try:
            # è·å–è´¢åŠ¡æ•°æ®çš„ç¬¬ä¸€è¡Œï¼ˆé€šå¸¸åŒ…å«æœ€æ–°æ•°æ®ï¼‰
            info_row = info_data.iloc[0] if not info_data.empty else pd.Series()
            
            # 1. å¸‚å‡€ç‡ (Price to Book Ratio)
            if 'priceToBook' in info_row.index and not pd.isna(info_row['priceToBook']):
                # ç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„å¸‚å‡€ç‡
                result_data['PriceToBookRatio'] = float(info_row['priceToBook'])
            elif 'bookValue' in info_row.index and not pd.isna(info_row['bookValue']):
                book_value = float(info_row['bookValue'])
                if book_value > 0:
                    result_data['PriceToBookRatio'] = result_data['Close'] / book_value
            
            # 2. å¸‚å€¼ (Market Cap)
            if 'marketCap' in info_row.index and not pd.isna(info_row['marketCap']):
                result_data['MarketCap'] = float(info_row['marketCap'])
            elif 'sharesOutstanding' in info_row.index and not pd.isna(info_row['sharesOutstanding']):
                shares_outstanding = float(info_row['sharesOutstanding'])
                if shares_outstanding > 0:
                    result_data['MarketCap'] = result_data['Close'] * shares_outstanding
            
            # 3. å¸‚ç›ˆç‡ (PE Ratio)
            if 'trailingPE' in info_row.index and not pd.isna(info_row['trailingPE']):
                result_data['PERatio'] = float(info_row['trailingPE'])
            elif 'forwardPE' in info_row.index and not pd.isna(info_row['forwardPE']):
                result_data['PERatio'] = float(info_row['forwardPE'])
            
            # 4. å¸‚é”€ç‡ (Price to Sales Ratio)
            if 'priceToSalesTrailing12Months' in info_row.index and not pd.isna(info_row['priceToSalesTrailing12Months']):
                result_data['PriceToSalesRatio'] = float(info_row['priceToSalesTrailing12Months'])
            
            # 5. å‡€èµ„äº§æ”¶ç›Šç‡ (ROE)
            if 'returnOnEquity' in info_row.index and not pd.isna(info_row['returnOnEquity']):
                result_data['ROE'] = float(info_row['returnOnEquity'])
            
            # 6. èµ„äº§æ”¶ç›Šç‡ (ROA)
            if 'returnOnAssets' in info_row.index and not pd.isna(info_row['returnOnAssets']):
                result_data['ROA'] = float(info_row['returnOnAssets'])
            
            # 7. åˆ©æ¶¦ç‡
            if 'profitMargins' in info_row.index and not pd.isna(info_row['profitMargins']):
                result_data['ProfitMargins'] = float(info_row['profitMargins'])
            
            # 8. æµåŠ¨æ¯”ç‡ (å¦‚æœæœ‰èµ„äº§è´Ÿå€ºè¡¨æ•°æ®)
            if balance_sheet_data is not None and not balance_sheet_data.empty:
                balance_row = balance_sheet_data.iloc[0]
                if 'currentRatio' in balance_row.index and not pd.isna(balance_row['currentRatio']):
                    result_data['CurrentRatio'] = float(balance_row['currentRatio'])
                elif 'Total Current Assets' in balance_row.index and 'Total Current Liabilities' in balance_row.index:
                    current_assets = balance_row.get('Total Current Assets', 0)
                    current_liabilities = balance_row.get('Total Current Liabilities', 1)
                    if current_liabilities > 0:
                        result_data['CurrentRatio'] = current_assets / current_liabilities
            
            # 9. é€ŸåŠ¨æ¯”ç‡
            if 'quickRatio' in info_row.index and not pd.isna(info_row['quickRatio']):
                result_data['QuickRatio'] = float(info_row['quickRatio'])
            else:
                # ä¼°ç®—ä¸ºæµåŠ¨æ¯”ç‡çš„80%
                if 'CurrentRatio' in result_data.columns:
                    result_data['QuickRatio'] = result_data['CurrentRatio'] * 0.8
            
            # 10. èµ„äº§è´Ÿå€ºç‡
            if 'debtToEquity' in info_row.index and not pd.isna(info_row['debtToEquity']):
                result_data['DebtToEquity'] = float(info_row['debtToEquity'])
            elif 'totalDebt' in info_row.index and 'marketCap' in info_row.index:
                total_debt = info_row.get('totalDebt', 0)
                market_cap = info_row.get('marketCap', 1)
                if market_cap > 0:
                    result_data['DebtToEquity'] = total_debt / market_cap
            
            # 11. æ‰˜å®¾Qå€¼
            if 'enterpriseValue' in info_row.index and balance_sheet_data is not None:
                enterprise_value = info_row.get('enterpriseValue', None)
                if enterprise_value and not balance_sheet_data.empty:
                    balance_row = balance_sheet_data.iloc[0]
                    total_assets = balance_row.get('Total Assets', balance_row.get('totalAssets', None))
                    if total_assets and total_assets > 0:
                        result_data['TobinsQ'] = enterprise_value / total_assets
            
            # 12. æ¢æ‰‹ç‡è®¡ç®—
            if 'floatShares' in info_row.index and not pd.isna(info_row['floatShares']):
                float_shares = float(info_row['floatShares'])
                if float_shares > 0:
                    result_data = self._calculate_real_turnover_indicators(result_data, float_shares)
            elif 'sharesOutstanding' in info_row.index and not pd.isna(info_row['sharesOutstanding']):
                # ä½¿ç”¨æ€»è‚¡æœ¬ä½œä¸ºæµé€šè‚¡çš„æ›¿ä»£
                shares_outstanding = float(info_row['sharesOutstanding'])
                if shares_outstanding > 0:
                    result_data = self._calculate_real_turnover_indicators(result_data, shares_outstanding)
            
            logger.info("âœ… ä½¿ç”¨çœŸå®è´¢åŠ¡æ•°æ®è®¡ç®—å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨çœŸå®è´¢åŠ¡æ•°æ®è®¡ç®—å¤±è´¥: {e}")
            # å¦‚æœçœŸå®æ•°æ®è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°ä¼°ç®—æ–¹æ³•
            result_data = self._calculate_estimated_financial_indicators(result_data, "UNKNOWN")
        
        return result_data
    
    def _calculate_real_turnover_indicators(self, data: pd.DataFrame, shares_count: float) -> pd.DataFrame:
        """åŸºäºçœŸå®æµé€šè‚¡æ•°è®¡ç®—æ¢æ‰‹ç‡æŒ‡æ ‡"""
        try:
            result_data = data.copy()
            
            # è®¡ç®—æ—¥æ¢æ‰‹ç‡
            result_data['DailyTurnover'] = result_data['Volume'] / shares_count
            
            # è®¡ç®—ä¸åŒçª—å£çš„ç´¯è®¡å’Œå¹³å‡æ¢æ‰‹ç‡
            windows = [1, 5, 10, 20, 30]
            for window in windows:
                result_data[f'turnover_c{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).sum()
                if window > 1:
                    result_data[f'turnover_m{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).mean()
            
            return result_data
            
        except Exception as e:
            logger.error(f"è®¡ç®—çœŸå®æ¢æ‰‹ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            return data
    
    def _calculate_estimated_financial_indicators(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """åŸºäºä»·æ ¼å’Œæˆäº¤é‡æ•°æ®ä¼°ç®—è´¢åŠ¡æŒ‡æ ‡"""
        result_data = data.copy()
        
        # è·å–åŸºç¡€æ•°æ®
        close = result_data['Close'].values
        volume = result_data['Volume'].values
        high = result_data['High'].values
        low = result_data['Low'].values
        
        # 1. ä¼°ç®—å¸‚å€¼ (å‡è®¾æµé€šè‚¡ä¸ºå¹³å‡æˆäº¤é‡çš„æŸä¸ªå€æ•°)
        avg_volume = np.mean(volume[volume > 0]) if len(volume[volume > 0]) > 0 else 1000000
        estimated_shares = avg_volume * 50  # å‡è®¾å¹³å‡æˆäº¤é‡æ˜¯æµé€šè‚¡çš„1/50
        result_data['MarketCap'] = close * estimated_shares
        
        # 2. ä¼°ç®—å¸‚å‡€ç‡ (åŸºäºä»·æ ¼æ³¢åŠ¨æ€§ï¼Œé«˜æ³¢åŠ¨æ€§é€šå¸¸å¯¹åº”é«˜PB)
        price_volatility = pd.Series(close).rolling(20).std().fillna(0) / pd.Series(close).rolling(20).mean().fillna(1)
        result_data['PriceToBookRatio'] = 1.0 + price_volatility * 3  # åŸºå‡†1å€ï¼Œæ³¢åŠ¨æ€§æ¯å¢åŠ 1ï¼ŒPBå¢åŠ 3
        
        # 3. ä¼°ç®—å¸‚ç›ˆç‡ (åŸºäºä»·æ ¼è¶‹åŠ¿ï¼Œä¸Šæ¶¨è¶‹åŠ¿å¯¹åº”é«˜PE)
        price_trend = pd.Series(close).rolling(20).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0).fillna(0)
        base_pe = 15  # åŸºå‡†PE
        result_data['PERatio'] = base_pe + (price_trend / np.mean(close) * 1000)
        result_data['PERatio'] = np.clip(result_data['PERatio'], 5, 50)  # é™åˆ¶åœ¨åˆç†èŒƒå›´
        
        # 4. ä¼°ç®—å¸‚é”€ç‡ (åŸºäºæˆäº¤é‡æ´»è·ƒåº¦)
        volume_series = pd.Series(volume, index=result_data.index)
        volume_activity = volume_series.rolling(20).mean().fillna(avg_volume) / avg_volume
        result_data['PriceToSalesRatio'] = 1.0 + volume_activity * 2  # æˆäº¤æ´»è·ƒå¯¹åº”é«˜PS
        
        # 5. ä¼°ç®—ROE (åŸºäºæ”¶ç›Šç‡)
        returns = pd.Series(close).pct_change(20).fillna(0)
        result_data['ROE'] = np.clip(returns * 4, -0.3, 0.5)  # å¹´åŒ–æ”¶ç›Šç‡ä½œä¸ºROEçš„ä»£ç†
        
        # 6. ä¼°ç®—ROA (é€šå¸¸æ¯”ROEä½)
        result_data['ROA'] = result_data['ROE'] * 0.6
        
        # 7. ä¼°ç®—åˆ©æ¶¦ç‡ (åŸºäºä»·æ ¼ç¨³å®šæ€§)
        price_stability = 1 / (1 + price_volatility)
        result_data['ProfitMargins'] = price_stability * 0.1  # ç¨³å®šçš„è‚¡ç¥¨å‡è®¾æœ‰æ›´å¥½çš„åˆ©æ¶¦ç‡
        
        # 8. ä¼°ç®—æµåŠ¨æ¯”ç‡ (åŸºäºæˆäº¤é‡æµåŠ¨æ€§)
        volume_series = pd.Series(volume, index=result_data.index)
        liquidity = volume_series.rolling(5).mean() / volume_series.rolling(20).mean()
        result_data['CurrentRatio'] = 1.0 + liquidity.fillna(1) * 0.5
        
        # 9. ä¼°ç®—é€ŸåŠ¨æ¯”ç‡ (é€šå¸¸æ¯”æµåŠ¨æ¯”ç‡ä½)
        result_data['QuickRatio'] = result_data['CurrentRatio'] * 0.8
        
        # 10. ä¼°ç®—èµ„äº§è´Ÿå€ºç‡ (åŸºäºæ³¢åŠ¨æ€§ï¼Œé«˜æ³¢åŠ¨å¯èƒ½æ„å‘³ç€é«˜æ æ†)
        result_data['DebtToEquity'] = price_volatility * 2
        
        # 11. ä¼°ç®—æ‰˜å®¾Qå€¼
        market_value_ratio = (high + low) / (2 * close)  # ç›¸å¯¹ä¼°å€¼
        market_value_ratio = pd.Series(market_value_ratio, index=result_data.index).fillna(1)
        result_data['TobinsQ'] = market_value_ratio
        
        # 12. ä¼°ç®—æ¢æ‰‹ç‡æŒ‡æ ‡
        result_data = self._calculate_estimated_turnover_indicators(result_data, estimated_shares)
        
        logger.info(f"ğŸ”® ä½¿ç”¨ä¼°ç®—æ–¹æ³•è®¡ç®—è´¢åŠ¡æŒ‡æ ‡ (åŸºäºä»·æ ¼å’Œæˆäº¤é‡)")
        return result_data
    
    def _calculate_estimated_turnover_indicators(self, data: pd.DataFrame, estimated_shares: float) -> pd.DataFrame:
        """åŸºäºä¼°ç®—æµé€šè‚¡è®¡ç®—æ¢æ‰‹ç‡æŒ‡æ ‡"""
        try:
            result_data = data.copy()
            
            # è®¡ç®—æ—¥æ¢æ‰‹ç‡
            result_data['DailyTurnover'] = result_data['Volume'] / estimated_shares
            
            # è®¡ç®—ä¸åŒçª—å£çš„ç´¯è®¡å’Œå¹³å‡æ¢æ‰‹ç‡
            windows = [1, 5, 10, 20, 30]
            for window in windows:
                result_data[f'turnover_c{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).sum()
                if window > 1:
                    result_data[f'turnover_m{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).mean()
            
            logger.info("âœ… å®Œæˆæ¢æ‰‹ç‡æŒ‡æ ‡ä¼°ç®—")
            return result_data
            
        except Exception as e:
            logger.error(f"ä¼°ç®—æ¢æ‰‹ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            return data
    
    def _ensure_financial_columns_exist(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç¡®ä¿æ‰€æœ‰è´¢åŠ¡æŒ‡æ ‡åˆ—éƒ½å­˜åœ¨ä¸”æœ‰åˆç†çš„é»˜è®¤å€¼"""
        result_data = data.copy()
        
        # å®šä¹‰æ‰€æœ‰å¿…éœ€çš„è´¢åŠ¡æŒ‡æ ‡åˆ—å’Œå…¶é»˜è®¤å€¼
        required_columns = {
            'PriceToBookRatio': 1.5,    # é»˜è®¤å¸‚å‡€ç‡
            'MarketCap': None,          # å¸‚å€¼éœ€è¦è®¡ç®—
            'PERatio': 15.0,            # é»˜è®¤å¸‚ç›ˆç‡
            'PriceToSalesRatio': 2.0,   # é»˜è®¤å¸‚é”€ç‡
            'ROE': 0.1,                 # é»˜è®¤10%çš„ROE
            'ROA': 0.05,                # é»˜è®¤5%çš„ROA
            'ProfitMargins': 0.08,      # é»˜è®¤8%çš„åˆ©æ¶¦ç‡
            'CurrentRatio': 1.2,        # é»˜è®¤æµåŠ¨æ¯”ç‡
            'QuickRatio': 1.0,          # é»˜è®¤é€ŸåŠ¨æ¯”ç‡
            'DebtToEquity': 0.5,        # é»˜è®¤èµ„äº§è´Ÿå€ºç‡
            'TobinsQ': 1.0,             # é»˜è®¤æ‰˜å®¾Qå€¼
            'DailyTurnover': None,      # éœ€è¦è®¡ç®—
            'turnover_c1d': None,       # éœ€è¦è®¡ç®—
            'turnover_c5d': None,       # éœ€è¦è®¡ç®—
            'turnover_c10d': None,      # éœ€è¦è®¡ç®—
            'turnover_c20d': None,      # éœ€è¦è®¡ç®—
            'turnover_c30d': None,      # éœ€è¦è®¡ç®—
            'turnover_m5d': None,       # éœ€è¦è®¡ç®—
            'turnover_m10d': None,      # éœ€è¦è®¡ç®—
            'turnover_m20d': None,      # éœ€è¦è®¡ç®—
            'turnover_m30d': None       # éœ€è¦è®¡ç®—
        }
        
        # ä¸ºç¼ºå¤±çš„åˆ—æ·»åŠ é»˜è®¤å€¼
        for col_name, default_value in required_columns.items():
            # æ£€æŸ¥åˆ—æ˜¯å¦ä¸å­˜åœ¨æˆ–å…¨éƒ¨ä¸ºNaN
            col_missing = col_name not in result_data.columns 
            col_all_nan = not col_missing and result_data[col_name].isna().all()
            col_all_zero = not col_missing and (result_data[col_name] == 0).all()
            
            # å¯¹äºæ¢æ‰‹ç‡æŒ‡æ ‡ï¼Œé¢å¤–æ£€æŸ¥æ˜¯å¦å…¨éƒ¨ä¸º0ï¼ˆè¿™é€šå¸¸è¡¨ç¤ºè®¡ç®—æœ‰é—®é¢˜ï¼‰
            needs_calculation = col_missing or col_all_nan or (col_name.startswith('turnover') and col_all_zero)
            
            if needs_calculation:
                if default_value is not None:
                    result_data[col_name] = default_value
                elif col_name == 'MarketCap':
                    # ä¼°ç®—å¸‚å€¼ï¼šå‡è®¾å¹³å‡è‚¡ä»·ä¸ºå½“å‰è‚¡ä»·ï¼Œæµé€šè‚¡ä¸ºæˆäº¤é‡çš„50å€
                    avg_volume = result_data['Volume'].mean() if 'Volume' in result_data.columns else 1000000
                    estimated_shares = avg_volume * 50
                    result_data['MarketCap'] = result_data['Close'] * estimated_shares
                elif col_name.startswith('turnover'):
                    # åªæœ‰åœ¨æ¢æ‰‹ç‡æŒ‡æ ‡ç¡®å®ç¼ºå¤±æˆ–æœ‰é—®é¢˜æ—¶æ‰é‡æ–°è®¡ç®—
                    logger.warning(f"æ¢æ‰‹ç‡æŒ‡æ ‡ {col_name} ç¼ºå¤±æˆ–å¼‚å¸¸ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
                    
                    # ä¼°ç®—æ¢æ‰‹ç‡ç›¸å…³æŒ‡æ ‡
                    if 'DailyTurnover' not in result_data.columns or result_data['DailyTurnover'].isna().all() or (result_data['DailyTurnover'] == 0).all():
                        avg_volume = result_data['Volume'].mean() if 'Volume' in result_data.columns else 1000000
                        estimated_shares = avg_volume * 50
                        result_data['DailyTurnover'] = result_data['Volume'] / estimated_shares
                    
                    # è®¡ç®—ç´¯è®¡å’Œå¹³å‡æ¢æ‰‹ç‡
                    if col_name.startswith('turnover_c'):
                        window = int(col_name.split('d')[0].split('_c')[1])
                        result_data[col_name] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).sum()
                    elif col_name.startswith('turnover_m'):
                        window = int(col_name.split('d')[0].split('_m')[1])
                        result_data[col_name] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).mean()
        
        return result_data
    

    
    def calculate_volatility_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡ï¼ˆçº¦8ä¸ªï¼‰"""
        try:
            volatility_data = {}
            
            # è®¡ç®—ä»·æ ¼å˜åŒ–
            price_diff = data['Close'].diff()
            log_returns = np.log(data['Close'] / data['Close'].shift(1))
            
            # 1. å·²å®ç°æ³¢åŠ¨ç‡ (20å¤©çª—å£)
            volatility_data['RealizedVolatility_20'] = price_diff.rolling(window=20).std() * np.sqrt(252)
            
            # 2. å·²å®ç°è´ŸåŠå˜å·®
            negative_returns = price_diff[price_diff < 0]
            volatility_data['NegativeSemiDeviation_20'] = negative_returns.rolling(window=20).std() * np.sqrt(252)
            
            # 3. å·²å®ç°è¿ç»­æ³¢åŠ¨ç‡
            volatility_data['ContinuousVolatility_20'] = log_returns.rolling(window=20).std() * np.sqrt(252)
            
            # 4. å·²å®ç°æ­£åŠå˜å·®
            positive_returns = price_diff[price_diff > 0]
            volatility_data['PositiveSemiDeviation_20'] = positive_returns.rolling(window=20).std() * np.sqrt(252)
            
            # 5. ä¸åŒçª—å£çš„æ³¢åŠ¨ç‡
            for window in [10, 30, 60]:
                volatility_data[f'Volatility_{window}'] = price_diff.rolling(window=window).std() * np.sqrt(252)
            
            volatility_df = pd.DataFrame(volatility_data, index=data.index)
            
            logger.info("è®¡ç®—äº†æ³¢åŠ¨ç‡æŒ‡æ ‡")
            return volatility_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_all_indicators_for_stock(self, symbol: str, incremental_start_date: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        ä¸ºå•åªè‚¡ç¥¨è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆæ”¯æŒå¢é‡è®¡ç®—ï¼‰
        
        Parameters:
        -----------
        symbol : str
            è‚¡ç¥¨ä»£ç 
        incremental_start_date : Optional[str]
            å¢é‡è®¡ç®—çš„å¼€å§‹æ—¥æœŸï¼Œå¦‚æœæä¾›åˆ™åªè®¡ç®—è¯¥æ—¥æœŸä¹‹åçš„æ•°æ®
        """
        try:
            # è¯»å–å†å²ä»·æ ¼æ•°æ®
            price_data = self.read_qlib_binary_data(symbol)
            if price_data is None or price_data.empty:
                logger.warning(f"No price data found for {symbol}")
                return None
            
            # å¦‚æœæŒ‡å®šäº†å¢é‡å¼€å§‹æ—¥æœŸï¼Œåªè®¡ç®—è¯¥æ—¥æœŸä¹‹åçš„æ•°æ®
            if incremental_start_date:
                start_date = pd.to_datetime(incremental_start_date)
                price_data = price_data[price_data.index >= start_date]
                if price_data.empty:
                    logger.info(f"{symbol}: å¢é‡æ—¥æœŸ {incremental_start_date} ä¹‹åæ²¡æœ‰æ–°æ•°æ®")
                    return None
                logger.info(f"{symbol}: å¢é‡è®¡ç®— {incremental_start_date} ä¹‹åçš„æ•°æ® ({len(price_data)} è¡Œ)")
            
            # ä½¿ç”¨å¹¶è¡Œè®¡ç®—æˆ–é¡ºåºè®¡ç®—
            if self.enable_parallel:
                return self._calculate_indicators_parallel(symbol, price_data)
            else:
                return self._calculate_indicators_sequential(symbol, price_data)
                
        except Exception as e:
            logger.error(f"âŒ {symbol}: è®¡ç®—æŒ‡æ ‡å¤±è´¥ - {e}")
            return None
    
    def _calculate_indicators_parallel(self, symbol: str, price_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        å¹¶è¡Œè®¡ç®—å•åªè‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ ‡ç±»å‹
        """
        if not self.enable_parallel:
            return self._calculate_indicators_sequential(symbol, price_data)
        
        try:
            logger.info(f"å¼€å§‹å¹¶è¡Œè®¡ç®— {symbol} çš„æ‰€æœ‰æŒ‡æ ‡...")
            start_time = time.time()
            
            # é‡ç½®æŒ‡æ ‡ç¼“å­˜
            self._reset_indicators_cache()
            
            # ä¿å­˜åŸå§‹æ—¥æœŸä¿¡æ¯
            original_dates = price_data.index
            
            # å®šä¹‰å„ç±»æŒ‡æ ‡è®¡ç®—ä»»åŠ¡
            indicator_tasks = [
                ('Alpha158', partial(self.calculate_alpha158_indicators, price_data)),
                ('Alpha360', partial(self.calculate_alpha360_indicators, price_data)),
                ('Technical', partial(self.calculate_all_technical_indicators, price_data)),
                ('Candlestick', partial(self.calculate_candlestick_patterns, price_data)),
                ('Financial', partial(self.calculate_financial_indicators, price_data, symbol)),
                ('Volatility', partial(self.calculate_volatility_indicators, price_data))
            ]
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè®¡ç®—
            results = {}
            failed_tasks = []
            
            with ThreadPoolExecutor(max_workers=min(6, self.max_workers)) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_task = {
                    executor.submit(task_func): task_name 
                    for task_name, task_func in indicator_tasks
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task):
                    task_name = future_to_task[future]
                    try:
                        result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                        if result is not None and not result.empty:
                            results[task_name] = result
                            logger.debug(f"âœ… {symbol} - {task_name}: {result.shape[1]} ä¸ªæŒ‡æ ‡")
                        else:
                            failed_tasks.append(task_name)
                            logger.warning(f"âš ï¸ {symbol} - {task_name}: è®¡ç®—ç»“æœä¸ºç©º")
                    except Exception as e:
                        failed_tasks.append(task_name)
                        logger.error(f"âŒ {symbol} - {task_name}: è®¡ç®—å¤±è´¥ - {e}")
            
            if failed_tasks:
                logger.warning(f"{symbol}: ä»¥ä¸‹æŒ‡æ ‡ç±»å‹è®¡ç®—å¤±è´¥: {failed_tasks}")
            
            # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
            try:
                all_indicators = [price_data]
                all_indicators.extend(results.values())
                
                if all_indicators:
                    # ç¡®ä¿æ‰€æœ‰DataFrameéƒ½æœ‰ä¸€è‡´çš„ç´¢å¼•ï¼Œä½†ä¿ç•™æ—¥æœŸä¿¡æ¯
                    aligned_indicators = []
                    base_length = len(price_data)
                    
                    for df in all_indicators:
                        if df is not None and not df.empty:
                            # é‡ç½®ç´¢å¼•ä½†ä¿ç•™åŸå§‹ç´¢å¼•ä½œä¸ºDateåˆ—ï¼ˆå¦‚æœè¿˜æ²¡æœ‰Dateåˆ—çš„è¯ï¼‰
                            df_reset = df.reset_index()
                            if 'index' in df_reset.columns and 'Date' not in df_reset.columns:
                                df_reset = df_reset.rename(columns={'index': 'Date'})
                            elif 'Date' not in df_reset.columns:
                                # å¦‚æœæ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸ
                                df_reset['Date'] = original_dates[:len(df_reset)]
                            
                            # ç¡®ä¿é•¿åº¦ä¸€è‡´
                            if len(df_reset) != base_length:
                                # é‡æ–°é‡‡æ ·æˆ–æˆªæ–­ä»¥åŒ¹é…åŸºå‡†é•¿åº¦
                                if len(df_reset) > base_length:
                                    df_reset = df_reset.iloc[:base_length]
                                else:
                                    # ç”¨NaNå¡«å……ä¸è¶³çš„è¡Œ
                                    missing_rows = base_length - len(df_reset)
                                    padding_data = {}
                                    for col in df_reset.columns:
                                        if col == 'Date':
                                            # ä¸ºæ—¥æœŸåˆ—ç”Ÿæˆåˆé€‚çš„æ—¥æœŸ
                                            last_date = df_reset['Date'].iloc[-1] if len(df_reset) > 0 else original_dates[-1]
                                            if isinstance(last_date, str):
                                                last_date = pd.to_datetime(last_date)
                                            additional_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=missing_rows)
                                            padding_data[col] = additional_dates
                                        else:
                                            padding_data[col] = [np.nan] * missing_rows
                                    
                                    padding_df = pd.DataFrame(padding_data)
                                    df_reset = pd.concat([df_reset, padding_df], ignore_index=True)
                            aligned_indicators.append(df_reset)
                    
                    # å®‰å…¨åˆå¹¶ - ä¿ç•™Dateåˆ—
                    combined_df = pd.concat(aligned_indicators, axis=1)
                    
                    # å¤„ç†é‡å¤çš„Dateåˆ—
                    if 'Date' in combined_df.columns:
                        date_cols = [col for col in combined_df.columns if col == 'Date']
                        if len(date_cols) > 1:
                            # ä¿ç•™ç¬¬ä¸€ä¸ªDateåˆ—ï¼Œåˆ é™¤å…¶ä½™çš„
                            combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]
                    
                    # ä»è´¢åŠ¡æ•°æ®ä¸­æå–æ–°å¢çš„åˆ—
                    if 'Financial' in results:
                        financial_data = results['Financial']
                        financial_cols = [col for col in financial_data.columns if col not in price_data.columns and col != 'Date']
                        if financial_cols:
                            for col in financial_cols:
                                if col not in combined_df.columns:
                                    # é‡æ–°ç´¢å¼•è´¢åŠ¡æ•°æ®ä»¥åŒ¹é…åŸºå‡†ç´¢å¼•
                                    financial_col_data = financial_data[col].reindex(range(base_length), method='ffill')
                                    combined_df[col] = financial_col_data
                    
                    # æ·»åŠ è‚¡ç¥¨ä»£ç 
                    combined_df['Symbol'] = symbol
                    logger.debug(f"âœ… {symbol}: å·²æ·»åŠ Symbolåˆ—ï¼Œå½“å‰åˆ—æ•°: {len(combined_df.columns)}")
                    
                    # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šDate, Symbol, ç„¶åæ˜¯å…¶ä»–åˆ—
                    cols = []
                    if 'Date' in combined_df.columns:
                        cols.append('Date')
                    cols.append('Symbol')
                    cols.extend([col for col in combined_df.columns if col not in ['Date', 'Symbol']])
                    combined_df = combined_df[cols]
                    
                    # éªŒè¯Symbolåˆ—æ˜¯å¦å­˜åœ¨
                    if 'Symbol' not in combined_df.columns:
                        logger.error(f"âŒ {symbol}: Symbolåˆ—æ·»åŠ å¤±è´¥ï¼å½“å‰åˆ—: {list(combined_df.columns[:10])}...")
                        # å¼ºåˆ¶æ·»åŠ Symbolåˆ—
                        combined_df['Symbol'] = symbol
                    
                    # é‡ç½®æ•°å­—ç´¢å¼•ï¼Œä½†ä¿ç•™Dateåˆ—
                    combined_df = combined_df.reset_index(drop=True)
                    
                    elapsed_time = time.time() - start_time
                    logger.info(f"âœ… {symbol}: å¹¶è¡Œè®¡ç®—å®Œæˆ {len(combined_df.columns)-2} ä¸ªæŒ‡æ ‡ (è€—æ—¶: {elapsed_time:.2f}s)")
                    return combined_df
                    
            except Exception as e:
                logger.error(f"âŒ {symbol}: åˆå¹¶æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯ - {e}")
                # é™çº§åˆ°é¡ºåºè®¡ç®—æ–¹æ³•
                return self._calculate_indicators_sequential(symbol, price_data)
            else:
                logger.error(f"âŒ {symbol}: æ‰€æœ‰æŒ‡æ ‡è®¡ç®—éƒ½å¤±è´¥äº†")
                return None
                
        except Exception as e:
            logger.error(f"âŒ {symbol}: å¹¶è¡Œè®¡ç®—å¤±è´¥ - {e}")
            return self._calculate_indicators_sequential(symbol, price_data)
    
    def _calculate_indicators_sequential(self, symbol: str, price_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        é¡ºåºè®¡ç®—å•åªè‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ ‡ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        try:
            logger.info(f"å¼€å§‹é¡ºåºè®¡ç®— {symbol} çš„æ‰€æœ‰æŒ‡æ ‡...")
            
            # é‡ç½®æŒ‡æ ‡è·Ÿè¸ªå™¨
            self._reset_indicators_cache()
            
            # ä¿å­˜åŸå§‹æ—¥æœŸä¿¡æ¯
            original_dates = price_data.index
            
            # 1. è®¡ç®—Alpha158æŒ‡æ ‡ä½“ç³» (~158ä¸ª)
            alpha158_indicators = self.calculate_alpha158_indicators(price_data)
            
            # 2. è®¡ç®—Alpha360æŒ‡æ ‡ä½“ç³» (~360ä¸ª)
            alpha360_indicators = self.calculate_alpha360_indicators(price_data)
            
            # 3. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (~60ä¸ª)
            technical_indicators = self.calculate_all_technical_indicators(price_data)
            
            # 4. è®¡ç®—èœ¡çƒ›å›¾å½¢æ€ (61ä¸ª)
            candlestick_patterns = self.calculate_candlestick_patterns(price_data)
            
            # 5. è®¡ç®—è´¢åŠ¡æŒ‡æ ‡ (~15ä¸ª)
            financial_data = self.calculate_financial_indicators(price_data, symbol)
            
            # 6. è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡ (~8ä¸ª)
            volatility_indicators = self.calculate_volatility_indicators(price_data)
            
            # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡ï¼ˆç¡®ä¿ç´¢å¼•ä¸€è‡´æ€§å¹¶ä¿ç•™æ—¥æœŸä¿¡æ¯ï¼‰
            base_index = price_data.index
            indicator_dfs = [
                price_data,
                alpha158_indicators,
                alpha360_indicators,
                technical_indicators,
                candlestick_patterns,
                financial_data,  # æ·»åŠ è´¢åŠ¡æ•°æ®
                volatility_indicators
            ]
            
            # é‡æ–°ç´¢å¼•æ‰€æœ‰DataFrameä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼Œå¹¶ä¿ç•™æ—¥æœŸä¿¡æ¯
            aligned_dfs = []
            for df in indicator_dfs:
                if df is not None and not df.empty:
                    if not df.index.equals(base_index):
                        df = df.reindex(base_index, method='ffill')
                    
                    # å°†ç´¢å¼•è½¬æ¢ä¸ºDateåˆ—ï¼ˆå¦‚æœè¿˜ä¸æ˜¯åˆ—çš„è¯ï¼‰
                    df_with_date = df.reset_index()
                    if 'index' in df_with_date.columns and 'Date' not in df_with_date.columns:
                        df_with_date = df_with_date.rename(columns={'index': 'Date'})
                    elif 'Date' not in df_with_date.columns:
                        df_with_date['Date'] = original_dates
                    
                    aligned_dfs.append(df_with_date)
            
            all_indicators = pd.concat(aligned_dfs, axis=1)
            
            # å¤„ç†é‡å¤çš„Dateåˆ—
            if 'Date' in all_indicators.columns:
                date_cols = [col for col in all_indicators.columns if col == 'Date']
                if len(date_cols) > 1:
                    # ä¿ç•™ç¬¬ä¸€ä¸ªDateåˆ—ï¼Œåˆ é™¤å…¶ä½™çš„
                    all_indicators = all_indicators.loc[:, ~all_indicators.columns.duplicated(keep='first')]
            
            # è´¢åŠ¡æ•°æ®å·²ç»åœ¨ä¸»åˆå¹¶ä¸­å¤„ç†ï¼Œæ— éœ€é¢å¤–æ“ä½œ
            
            # æ·»åŠ è‚¡ç¥¨ä»£ç 
            all_indicators['Symbol'] = symbol
            logger.debug(f"âœ… {symbol}: é¡ºåºè®¡ç®—å·²æ·»åŠ Symbolåˆ—ï¼Œå½“å‰åˆ—æ•°: {len(all_indicators.columns)}")
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šDate, Symbol, ç„¶åæ˜¯å…¶ä»–åˆ—
            cols = []
            if 'Date' in all_indicators.columns:
                cols.append('Date')
            cols.append('Symbol')
            cols.extend([col for col in all_indicators.columns if col not in ['Date', 'Symbol']])
            all_indicators = all_indicators[cols]
            
            # éªŒè¯Symbolåˆ—æ˜¯å¦å­˜åœ¨
            if 'Symbol' not in all_indicators.columns:
                logger.error(f"âŒ {symbol}: é¡ºåºè®¡ç®—Symbolåˆ—æ·»åŠ å¤±è´¥ï¼å½“å‰åˆ—: {list(all_indicators.columns[:10])}...")
                # å¼ºåˆ¶æ·»åŠ Symbolåˆ—
                all_indicators['Symbol'] = symbol
            
            # é‡ç½®æ•°å­—ç´¢å¼•ï¼Œä½†ä¿ç•™Dateåˆ—
            all_indicators = all_indicators.reset_index(drop=True)
            
            logger.info(f"âœ… {symbol}: é¡ºåºè®¡ç®—å®Œæˆ {len(all_indicators.columns)-2} ä¸ªæŒ‡æ ‡")
            return all_indicators
            
        except Exception as e:
            logger.error(f"âŒ {symbol}: é¡ºåºè®¡ç®—å¤±è´¥ - {e}")
            return None
    
    def calculate_all_indicators(self, max_stocks: Optional[int] = None) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ ‡ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        stocks = self.get_available_stocks()
        
        if max_stocks:
            stocks = stocks[:max_stocks]
        
        if not stocks:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è‚¡ç¥¨æ•°æ®")
            return pd.DataFrame()
        
        logger.info(f"å¼€å§‹è®¡ç®— {len(stocks)} åªè‚¡ç¥¨çš„æŒ‡æ ‡...")
        start_time = time.time()
        
        if self.enable_parallel and len(stocks) > 1:
            return self._calculate_all_stocks_parallel(stocks)
        else:
            return self._calculate_all_stocks_sequential(stocks)
    
    def _calculate_all_stocks_parallel(self, stocks: List[str]) -> pd.DataFrame:
        """å¹¶è¡Œè®¡ç®—å¤šåªè‚¡ç¥¨çš„æŒ‡æ ‡"""
        logger.info(f"ä½¿ç”¨å¹¶è¡Œæ¨¡å¼è®¡ç®— {len(stocks)} åªè‚¡ç¥¨ (æœ€å¤§çº¿ç¨‹æ•°: {self.max_workers})")
        
        all_results = []
        success_count = 0
        failed_stocks = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰è‚¡ç¥¨çš„è®¡ç®—ä»»åŠ¡
            future_to_symbol = {
                executor.submit(self.calculate_all_indicators_for_stock, symbol): symbol 
                for symbol in stocks
            }
            
            # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
            completed = 0
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                completed += 1
                
                try:
                    result = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                    if result is not None:
                        all_results.append(result)
                        success_count += 1
                        logger.info(f"âœ… è¿›åº¦ {completed}/{len(stocks)}: {symbol} è®¡ç®—å®Œæˆ ({len(result.columns)-1} ä¸ªæŒ‡æ ‡)")
                    else:
                        failed_stocks.append(symbol)
                        logger.warning(f"âš ï¸ è¿›åº¦ {completed}/{len(stocks)}: {symbol} è®¡ç®—ç»“æœä¸ºç©º")
                        
                except Exception as e:
                    failed_stocks.append(symbol)
                    logger.error(f"âŒ è¿›åº¦ {completed}/{len(stocks)}: {symbol} è®¡ç®—å¤±è´¥ - {e}")
        
        elapsed_time = time.time() - start_time
        
        if failed_stocks:
            logger.warning(f"è®¡ç®—å¤±è´¥çš„è‚¡ç¥¨ ({len(failed_stocks)}): {failed_stocks[:5]}{'...' if len(failed_stocks) > 5 else ''}")
        
        if all_results:
            try:
                # æ•°æ®åˆå¹¶å‰çš„é¢„å¤„ç†å’Œæ£€æŸ¥
                logger.info("å¼€å§‹åˆå¹¶å¤šåªè‚¡ç¥¨çš„è®¡ç®—ç»“æœ...")
                
                # å¼ºåŒ–çš„DataFrameæ¸…ç†å’Œæ ‡å‡†åŒ–
                logger.info("å¼€å§‹å¼ºåŒ–çš„DataFrameæ¸…ç†å’Œæ ‡å‡†åŒ–...")
                
                # ç¬¬ä¸€æ­¥ï¼šåˆæ­¥æ¸…ç†å’ŒéªŒè¯
                valid_dfs = []
                for i, df in enumerate(all_results):
                    if df is None or df.empty:
                        logger.warning(f"è·³è¿‡ç©ºçš„DataFrame (ç´¢å¼•: {i})")
                        continue
                    
                    # å®Œå…¨é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ä¸ºè¿ç»­æ•°å­—ç´¢å¼•
                    df_clean = df.copy()
                    df_clean = df_clean.reset_index(drop=True)
                    
                    # æ£€æŸ¥ç´¢å¼•å”¯ä¸€æ€§
                    if df_clean.index.has_duplicates:
                        logger.warning(f"DataFrame {i} å­˜åœ¨é‡å¤ç´¢å¼•ï¼Œè¿›è¡Œå»é‡")
                        df_clean = df_clean.loc[~df_clean.index.duplicated(keep='first')]
                        df_clean = df_clean.reset_index(drop=True)
                    
                    valid_dfs.append((i, df_clean))
                
                if not valid_dfs:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœå¯ä»¥å¤„ç†")
                    return pd.DataFrame()
                
                # ç¬¬äºŒæ­¥ï¼šç»Ÿä¸€åˆ—ç»“æ„
                logger.info(f"ç»Ÿä¸€ {len(valid_dfs)} ä¸ªDataFrameçš„åˆ—ç»“æ„...")
                
                # è·å–æ‰€æœ‰å”¯ä¸€çš„åˆ—å
                all_columns = set()
                for _, df in valid_dfs:
                    all_columns.update(df.columns)
                
                all_columns = sorted(list(all_columns))
                logger.info(f"å‘ç° {len(all_columns)} ä¸ªå”¯ä¸€åˆ—")
                
                # ä½¿ç”¨ç®€å•å®‰å…¨çš„åˆå¹¶æ–¹æ³•
                cleaned_results = []
                for i, df in valid_dfs:
                    try:
                        # ç®€å•å¤åˆ¶å’Œé‡ç½®ç´¢å¼•
                        clean_df = df.copy()
                        clean_df = clean_df.reset_index(drop=True)
                        
                        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹æˆ–å­—ç¬¦ä¸²ï¼Œä½†ä¿æŠ¤é‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—
                        # éœ€è¦ä¿æŠ¤çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—
                        protected_cols = ['Symbol', 'Ticker', 'Name', 'ENName', 'Code', 'Date']
                        
                        for col in clean_df.columns:
                            try:
                                # è·³è¿‡é‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—ï¼Œä¸è¿›è¡Œæ•°å€¼è½¬æ¢
                                if col in protected_cols:
                                    continue
                                    
                                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œå¦‚æœå¤±è´¥å°±ä¿æŒåŸæ ·
                                if clean_df[col].dtype == 'object':
                                    try:
                                        clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')
                                    except:
                                        pass
                            except:
                                pass
                        
                        cleaned_results.append(clean_df)
                        logger.debug(f"âœ… æˆåŠŸæ¸…ç†DataFrame {i}")
                        
                    except Exception as e:
                        logger.error(f"âŒ æ¸…ç†DataFrame {i} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
                
                if not cleaned_results:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœå¯ä»¥åˆå¹¶")
                    return pd.DataFrame()
                
                # å®‰å…¨åˆå¹¶DataFrame - ä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹æ³•
                logger.info(f"æ­£åœ¨åˆå¹¶ {len(cleaned_results)} ä¸ªæœ‰æ•ˆç»“æœ...")
                try:
                    # é€ä¸ªåˆå¹¶ï¼Œé¿å…åˆ—ä¸åŒ¹é…çš„é—®é¢˜
                    combined_df = None
                    for i, df in enumerate(cleaned_results):
                        if combined_df is None:
                            combined_df = df.copy()
                        else:
                            # ä½¿ç”¨outer joinç¡®ä¿æ‰€æœ‰åˆ—éƒ½è¢«ä¿ç•™
                            combined_df = pd.concat([combined_df, df], ignore_index=True, sort=False)
                        logger.debug(f"åˆå¹¶ç¬¬ {i+1}/{len(cleaned_results)} ä¸ªDataFrame")
                    
                    if combined_df is None or combined_df.empty:
                        logger.error("âŒ åˆå¹¶åçš„DataFrameä¸ºç©º")
                        return pd.DataFrame()
                    
                except Exception as merge_error:
                    logger.error(f"âŒ DataFrameåˆå¹¶å¤±è´¥: {merge_error}")
                    # é™çº§åˆ°æœ€ç®€å•çš„æ–¹æ³•
                    logger.info("å°è¯•ä½¿ç”¨æœ€ç®€å•çš„åˆå¹¶æ–¹æ³•...")
                    try:
                        # åªä¿ç•™åˆ—æ•°æœ€å°‘çš„DataFrameçš„åˆ—
                        min_cols = min(len(df.columns) for df in cleaned_results)
                        logger.info(f"ä½¿ç”¨æœ€å°åˆ—æ•°: {min_cols}")
                        
                        # æ‰¾åˆ°æœ‰æœ€å°åˆ—æ•°çš„ç¬¬ä¸€ä¸ªDataFrameä½œä¸ºæ¨¡æ¿
                        template_df = next(df for df in cleaned_results if len(df.columns) == min_cols)
                        template_cols = template_df.columns.tolist()
                        
                        # åªä¿ç•™å…¬å…±åˆ—è¿›è¡Œåˆå¹¶
                        aligned_dfs = []
                        for df in cleaned_results:
                            aligned_df = df[template_cols].copy()
                            aligned_dfs.append(aligned_df)
                        
                        combined_df = pd.concat(aligned_dfs, ignore_index=True)
                        logger.info(f"âœ… ç®€åŒ–åˆå¹¶æˆåŠŸï¼Œä¿ç•™ {len(template_cols)} åˆ—")
                        
                    except Exception as e2:
                        logger.error(f"âŒ ç®€åŒ–åˆå¹¶ä¹Ÿå¤±è´¥: {e2}")
                        return pd.DataFrame()
                
                # éªŒè¯åˆå¹¶ç»“æœ
                if combined_df.empty:
                    logger.error("âŒ åˆå¹¶åçš„DataFrameä¸ºç©º")
                    return pd.DataFrame()
                
                # æ£€æŸ¥é‡å¤è¡Œ
                initial_rows = len(combined_df)
                combined_df = combined_df.drop_duplicates()
                if len(combined_df) < initial_rows:
                    logger.info(f"ç§»é™¤äº† {initial_rows - len(combined_df)} è¡Œé‡å¤æ•°æ®")
                
                logger.info(f"âœ… å¹¶è¡Œè®¡ç®—å®Œæˆ: {success_count}/{len(stocks)} åªè‚¡ç¥¨æˆåŠŸ (è€—æ—¶: {elapsed_time:.2f}s)")
                # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                indicator_count = len(combined_df.columns) - (2 if 'Date' in combined_df.columns else 1)
                logger.info(f"ğŸ“Š æ€»æŒ‡æ ‡æ•°é‡: {indicator_count}")
                logger.info(f"ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {len(combined_df)}")
                logger.info(f"âš¡ å¹³å‡æ¯åªè‚¡ç¥¨è€—æ—¶: {elapsed_time/len(stocks):.2f}s")
                return combined_df
                
            except Exception as e:
                logger.error(f"âŒ åˆå¹¶è®¡ç®—ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                logger.error(f"å°è¯•é™çº§ä¸ºç®€å•åˆå¹¶...")
                
                # é™çº§æ–¹æ¡ˆï¼šæœ€å®‰å…¨çš„é€ä¸ªåˆå¹¶
                try:
                    logger.info("ä½¿ç”¨æœ€å®‰å…¨çš„é€ä¸ªåˆå¹¶æ–¹æ¡ˆ...")
                    
                    # åªä¿ç•™éç©ºçš„ç»“æœ
                    valid_results = [df for df in all_results if df is not None and not df.empty]
                    
                    if not valid_results:
                        logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœ")
                        return pd.DataFrame()
                    
                    logger.info(f"å¼€å§‹é€ä¸ªåˆå¹¶ {len(valid_results)} ä¸ªDataFrame...")
                    
                    # é€ä¸ªå®‰å…¨åˆå¹¶
                    combined_df = None
                    successful_merges = 0
                    
                    for i, df in enumerate(valid_results):
                        try:
                            # å½»åº•æ¸…ç†å•ä¸ªDataFrame
                            clean_df = df.copy()
                            clean_df = clean_df.reset_index(drop=True)
                            
                            # ç¡®ä¿åˆ—åä¸ºå­—ç¬¦ä¸²
                            clean_df.columns = [str(col) for col in clean_df.columns]
                            
                            # å»é™¤ä»»ä½•å¯èƒ½çš„é‡å¤ç´¢å¼•
                            if clean_df.index.has_duplicates:
                                clean_df = clean_df.loc[~clean_df.index.duplicated(keep='first')]
                                clean_df = clean_df.reset_index(drop=True)
                            
                            # è½¬æ¢ä¸ºå­—å…¸å†è½¬å›DataFrameï¼ˆæœ€å½»åº•çš„æ¸…ç†ï¼‰
                            # ä¿æŠ¤é‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—
                            protected_cols = ['Symbol', 'Ticker', 'Name', 'ENName', 'Code', 'Date']
                            data_dict = {}
                            for col in clean_df.columns:
                                try:
                                    if col in protected_cols:
                                        # å¯¹äºé‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—ï¼Œç›´æ¥ä¿ç•™åŸå€¼
                                        data_dict[col] = clean_df[col].tolist()
                                    else:
                                        data_dict[col] = clean_df[col].values.tolist()
                                except:
                                    if col in protected_cols:
                                        # å¯¹äºé‡è¦åˆ—ï¼Œå°è¯•ä¿ç•™åŸå€¼è€Œä¸æ˜¯è®¾ä¸ºNaN
                                        try:
                                            data_dict[col] = clean_df[col].astype(str).tolist()
                                        except:
                                            data_dict[col] = [''] * len(clean_df)
                                    else:
                                        data_dict[col] = [np.nan] * len(clean_df)
                            
                            clean_df = pd.DataFrame(data_dict)
                            
                            if combined_df is None:
                                combined_df = clean_df
                            else:
                                # é€ä¸ªæ·»åŠ è¡Œ
                                combined_df = pd.concat([combined_df, clean_df], ignore_index=True, sort=False)
                            
                            successful_merges += 1
                            logger.debug(f"æˆåŠŸåˆå¹¶ç¬¬ {i+1} ä¸ªDataFrame")
                            
                        except Exception as merge_error:
                            logger.warning(f"è·³è¿‡ç¬¬ {i+1} ä¸ªDataFrameï¼Œåˆå¹¶å¤±è´¥: {merge_error}")
                            continue
                    
                    if combined_df is not None and not combined_df.empty:
                        logger.info(f"âœ… é™çº§åˆå¹¶æˆåŠŸ: {successful_merges}/{len(valid_results)} ä¸ªç»“æœ")
                        # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                        indicator_count = len(combined_df.columns) - (2 if 'Date' in combined_df.columns else 1)
                        logger.info(f"ğŸ“Š æ€»æŒ‡æ ‡æ•°é‡: {indicator_count}")
                        logger.info(f"ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {len(combined_df)}")
                        return combined_df
                    else:
                        logger.error("âŒ é™çº§åˆå¹¶åç»“æœä¸ºç©º")
                        return pd.DataFrame()
                    
                except Exception as e2:
                    logger.error(f"âŒ é™çº§åˆå¹¶ä¹Ÿå¤±è´¥: {e2}")
                    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šä¿å­˜å•ä¸ªæœ€å¤§çš„ç»“æœ
                    try:
                        if all_results:
                            largest_df = max(all_results, key=lambda x: len(x) if x is not None else 0)
                            if largest_df is not None and not largest_df.empty:
                                result_df = largest_df.copy().reset_index(drop=True)
                                logger.warning(f"âš ï¸ ä½¿ç”¨æœ€å¤§çš„å•ä¸ªç»“æœ: {len(result_df)} è¡Œ")
                                return result_df
                    except:
                        pass
                    return pd.DataFrame()
        else:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸè®¡ç®—ä»»ä½•è‚¡ç¥¨çš„æŒ‡æ ‡")
            return pd.DataFrame()
    
    def _calculate_all_stocks_sequential(self, stocks: List[str]) -> pd.DataFrame:
        """é¡ºåºè®¡ç®—å¤šåªè‚¡ç¥¨çš„æŒ‡æ ‡"""
        logger.info(f"ä½¿ç”¨é¡ºåºæ¨¡å¼è®¡ç®— {len(stocks)} åªè‚¡ç¥¨")
        
        all_results = []
        success_count = 0
        start_time = time.time()
        
        for i, symbol in enumerate(stocks, 1):
            stock_start_time = time.time()
            logger.info(f"ğŸ“ˆ å¤„ç†ç¬¬ {i}/{len(stocks)} åªè‚¡ç¥¨: {symbol}")
            
            result = self.calculate_all_indicators_for_stock(symbol)
            if result is not None:
                all_results.append(result)
                success_count += 1
                stock_elapsed = time.time() - stock_start_time
                # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                indicator_count = len(result.columns) - (2 if 'Date' in result.columns else 1)
                logger.info(f"âœ… {symbol}: å®Œæˆ {indicator_count} ä¸ªæŒ‡æ ‡ (è€—æ—¶: {stock_elapsed:.2f}s)")
            else:
                logger.warning(f"âš ï¸ {symbol}: è®¡ç®—å¤±è´¥")
        
        elapsed_time = time.time() - start_time
        
        if all_results:
            try:
                # æ•°æ®åˆå¹¶å‰çš„é¢„å¤„ç†å’Œæ£€æŸ¥
                logger.info("å¼€å§‹åˆå¹¶å¤šåªè‚¡ç¥¨çš„è®¡ç®—ç»“æœ...")
                
                # é‡ç½®æ‰€æœ‰DataFrameçš„ç´¢å¼•ä»¥é¿å…å†²çª
                cleaned_results = []
                for i, df in enumerate(all_results):
                    if df is None or df.empty:
                        logger.warning(f"è·³è¿‡ç©ºçš„DataFrame (ç´¢å¼•: {i})")
                        continue
                    cleaned_results.append(df.reset_index(drop=True))
                
                if not cleaned_results:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœå¯ä»¥åˆå¹¶")
                    return pd.DataFrame()
                
                # å®‰å…¨åˆå¹¶DataFrame
                combined_df = pd.concat(cleaned_results, ignore_index=True, sort=False)
                
                # æ£€æŸ¥é‡å¤è¡Œ
                initial_rows = len(combined_df)
                combined_df = combined_df.drop_duplicates()
                if len(combined_df) < initial_rows:
                    logger.info(f"ç§»é™¤äº† {initial_rows - len(combined_df)} è¡Œé‡å¤æ•°æ®")
                
                logger.info(f"âœ… é¡ºåºè®¡ç®—å®Œæˆ: {success_count}/{len(stocks)} åªè‚¡ç¥¨æˆåŠŸ (æ€»è€—æ—¶: {elapsed_time:.2f}s)")
                # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                indicator_count = len(combined_df.columns) - (2 if 'Date' in combined_df.columns else 1)
                logger.info(f"ğŸ“Š æ€»æŒ‡æ ‡æ•°é‡: {indicator_count}")
                logger.info(f"ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {len(combined_df)}")
                logger.info(f"â±ï¸ å¹³å‡æ¯åªè‚¡ç¥¨è€—æ—¶: {elapsed_time/len(stocks):.2f}s")
                return combined_df
                
            except Exception as e:
                logger.error(f"âŒ é¡ºåºè®¡ç®—åˆå¹¶ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return pd.DataFrame()
        else:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸè®¡ç®—ä»»ä½•è‚¡ç¥¨çš„æŒ‡æ ‡")
            return pd.DataFrame()
    
    def save_results(self, df: pd.DataFrame, filename: str = "enhanced_quantitative_indicators.csv") -> str:
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶ï¼ŒåŒ…å«ä¸­æ–‡æ ‡ç­¾è¡Œï¼Œç©ºå€¼ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼ˆå…¼å®¹SASï¼‰"""
        if df.empty:
            logger.warning("DataFrameä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return ""
        
        try:
            output_path = self.output_dir / filename
            
            # è·å–æ ‡å‡†å­—æ®µé¡ºåºå¹¶é‡æ–°æ’åˆ—DataFrame
            standard_columns = self._get_standard_column_order()
            available_columns = [col for col in standard_columns if col in df.columns]
            
            # é‡æ–°æ’åˆ—DataFrameåˆ—é¡ºåº
            df_reordered = df[available_columns]
            
            # è·å–åˆ—åå’Œä¸­æ–‡æ ‡ç­¾
            columns = df_reordered.columns.tolist()
            chinese_labels = self.get_field_labels(columns)
            
            # å¤„ç†ç©ºå€¼ï¼šå°†NaNæ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä»¥å…¼å®¹SAS
            df_clean = df_reordered.copy()
            df_clean = df_clean.fillna('')  # å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            
            logger.info("ğŸ“ ç©ºå€¼å¤„ç†: å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ä»¥å…¼å®¹SAS")
            
            # ä½¿ç”¨æ‰‹åŠ¨æ–¹å¼å†™å…¥CSVæ–‡ä»¶ä»¥åŒ…å«ä¸­æ–‡æ ‡ç­¾è¡Œ
            with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
                import csv
                writer = csv.writer(f)
                
                # ç¬¬ä¸€è¡Œï¼šå­—æ®µåï¼ˆè‹±æ–‡åˆ—åï¼‰
                writer.writerow(columns)
                
                # ç¬¬äºŒè¡Œï¼šä¸­æ–‡æ ‡ç­¾
                writer.writerow(chinese_labels)
                
                # ç¬¬ä¸‰è¡Œå¼€å§‹ï¼šå…·ä½“æ•°æ®
                for _, row in df_clean.iterrows():
                    writer.writerow(row.values)
            
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            logger.info(f"æ•°æ®å½¢çŠ¶: {df_reordered.shape}")
            logger.info(f"åŒ…å«ä¸­æ–‡æ ‡ç­¾è¡Œçš„CSVæ ¼å¼:")
            logger.info(f"  ç¬¬ä¸€è¡Œ: å­—æ®µå ({len(columns)} ä¸ªå­—æ®µ)")
            logger.info(f"  ç¬¬äºŒè¡Œ: ä¸­æ–‡æ ‡ç­¾")
            logger.info(f"  ç¬¬ä¸‰è¡Œå¼€å§‹: å…·ä½“æ•°æ® ({len(df_reordered)} è¡Œæ•°æ®)")
            logger.info(f"  ç©ºå€¼å¤„ç†: NaN â†’ '' (ç©ºå­—ç¬¦ä¸²ï¼Œå…¼å®¹SAS)")
            
            # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
            indicator_count = len(df_reordered.columns) - (2 if 'Date' in df_reordered.columns else 1)
            logger.info(f"æŒ‡æ ‡æ•°é‡: {indicator_count}")
            
            return str(output_path)
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return ""
    
    def run(self, max_stocks: Optional[int] = None, output_filename: str = "enhanced_quantitative_indicators.csv"):
        """è¿è¡Œå®Œæ•´çš„æŒ‡æ ‡è®¡ç®—æµç¨‹"""
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå¢å¼ºç‰ˆQlibæŒ‡æ ‡è®¡ç®—å™¨")
        logger.info(f"âš™ï¸ å¤šçº¿ç¨‹æ¨¡å¼: {'å¯ç”¨' if self.enable_parallel else 'ç¦ç”¨'}")
        if self.enable_parallel:
            logger.info(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {self.max_workers}")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # è®¡ç®—æŒ‡æ ‡
        results_df = self.calculate_all_indicators(max_stocks=max_stocks)
        
        total_elapsed = time.time() - start_time
        
        if not results_df.empty:
            # ä¿å­˜ç»“æœ
            output_path = self.save_results(results_df, output_filename)
            
            logger.info("=" * 80)
            logger.info("âœ… æŒ‡æ ‡è®¡ç®—å®Œæˆï¼")
            # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
            indicator_count = len(results_df.columns) - (2 if 'Date' in results_df.columns else 1)
            logger.info(f"ğŸ“Š æ€»å…±è®¡ç®—äº† {indicator_count} ä¸ªæŒ‡æ ‡")
            logger.info(f"ğŸ“ˆ åŒ…å« {results_df['Symbol'].nunique()} åªè‚¡ç¥¨")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_elapsed:.2f} ç§’")
            logger.info(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_path}")
            logger.info("=" * 80)
            
            # æ˜¾ç¤ºæŒ‡æ ‡ç»Ÿè®¡
            self._show_indicators_summary(results_df)
        else:
            logger.error(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ (è€—æ—¶: {total_elapsed:.2f}s)")
    
    def _show_indicators_summary(self, df: pd.DataFrame):
        """æ˜¾ç¤ºæŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦"""
        logger.info("ğŸ“Š æŒ‡æ ‡åˆ†ç±»ç»Ÿè®¡:")
        logger.info("-" * 50)
        
        # ç»Ÿè®¡å„ç±»æŒ‡æ ‡æ•°é‡
        alpha158_count = len([col for col in df.columns if col.startswith('ALPHA158_')])
        alpha360_count = len([col for col in df.columns if col.startswith('ALPHA360_')])
        technical_count = len([col for col in df.columns if any(prefix in col for prefix in ['SMA', 'EMA', 'RSI', 'MACD', 'ADX', 'ATR', 'BB_', 'STOCH']) and not col.startswith(('ALPHA158_', 'ALPHA360_'))])
        candlestick_count = len([col for col in df.columns if col.startswith('CDL')])
        financial_count = len([col for col in df.columns if any(prefix in col for prefix in ['PriceToBook', 'MarketCap', 'PE', 'ROE', 'ROA', 'turnover', 'TobinsQ'])])
        volatility_count = len([col for col in df.columns if 'Volatility' in col or 'SemiDeviation' in col])
        
        logger.info(f"Alpha158æŒ‡æ ‡: {alpha158_count} ä¸ª")
        logger.info(f"Alpha360æŒ‡æ ‡: {alpha360_count} ä¸ª")
        logger.info(f"æŠ€æœ¯æŒ‡æ ‡: {technical_count} ä¸ª")
        logger.info(f"èœ¡çƒ›å›¾å½¢æ€: {candlestick_count} ä¸ª")
        logger.info(f"è´¢åŠ¡æŒ‡æ ‡: {financial_count} ä¸ª")
        logger.info(f"æ³¢åŠ¨ç‡æŒ‡æ ‡: {volatility_count} ä¸ª")
        logger.info(f"æ€»è®¡: {alpha158_count + alpha360_count + technical_count + candlestick_count + financial_count + volatility_count} ä¸ª")

    def calculate_all_indicators_streaming(self, output_file: str, max_stocks: Optional[int] = None, batch_size: int = 20):
        """
        æµå¼è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„æŒ‡æ ‡ï¼Œé€è¡Œå†™å…¥CSVï¼Œæå¤§èŠ‚çœå†…å­˜
        """
        stocks = self.get_available_stocks()
        if max_stocks:
            stocks = stocks[:max_stocks]
        logger = logging.getLogger(__name__)
        logger.info(f"[æµå¼æ¨¡å¼] å¼€å§‹æµå¼è®¡ç®— {len(stocks)} åªè‚¡ç¥¨çš„æŒ‡æ ‡ (æ‰¹æ¬¡å¤§å°: {batch_size})")
        
        # é¢„å®šä¹‰æ ‡å‡†å­—æ®µé¡ºåºï¼Œç¡®ä¿ä¸€è‡´æ€§
        standard_columns = self._get_standard_column_order()
        logger.info(f"[æµå¼æ¨¡å¼] ä½¿ç”¨é¢„å®šä¹‰å­—æ®µé¡ºåºï¼Œå…± {len(standard_columns)} ä¸ªå­—æ®µ")
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å®é™…å­˜åœ¨çš„åˆ—åï¼ˆç”¨äºéªŒè¯ï¼‰
        actual_columns = set()
        for i, symbol in enumerate(stocks):
            try:
                result = self.calculate_all_indicators_for_stock(symbol)
                if result is not None and not result.empty:
                    actual_columns.update(result.columns)
                if i % 10 == 0:
                    gc.collect()
            except Exception as e:
                logger.warning(f"æ”¶é›†åˆ—åæ—¶è·³è¿‡ {symbol}: {e}")
        
        # è¿‡æ»¤å‡ºå®é™…å­˜åœ¨çš„æ ‡å‡†åˆ—
        available_columns = [col for col in standard_columns if col in actual_columns]
        logger.info(f"[æµå¼æ¨¡å¼] å®é™…å¯ç”¨å­—æ®µ: {len(available_columns)} ä¸ª")
        
        # ç¬¬äºŒæ­¥ï¼šå†™å…¥CSVå¤´éƒ¨
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(available_columns)
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ‰¹å¤„ç†å¹¶é€è¡Œå†™å…¥
        current_batch = []
        batch_num = 0
        total_rows = 0
        for i, symbol in enumerate(stocks):
            try:
                result = self.calculate_all_indicators_for_stock(symbol)
                if result is not None and not result.empty:
                    current_batch.append(result)
                if len(current_batch) >= batch_size or i == len(stocks) - 1:
                    batch_num += 1
                    batch_rows = 0
                    with open(output_file, 'a', newline='', encoding='utf-8') as f:
                        writer = csv.writer(f)
                        for df in current_batch:
                            for _, row in df.iterrows():
                                row_data = [row[col] if col in row and not pd.isna(row[col]) else '' for col in available_columns]
                                writer.writerow(row_data)
                                batch_rows += 1
                    total_rows += batch_rows
                    logger.info(f"[æµå¼æ¨¡å¼] ç¬¬ {batch_num} æ‰¹å†™å…¥å®Œæˆ: {batch_rows} è¡Œ")
                    current_batch.clear()
                    gc.collect()
            except Exception as e:
                logger.warning(f"å†™å…¥æ•°æ®æ—¶è·³è¿‡ {symbol}: {e}")
        logger.info(f"[æµå¼æ¨¡å¼] æµå¼è®¡ç®—å®Œæˆï¼Œæ€»è¡Œæ•°: {total_rows}")

    def calculate_indicators_incremental(self, output_file: str, 
                                       max_stocks: Optional[int] = None,
                                       force_update: bool = False,
                                       batch_size: int = 20,
                                       backup_output: bool = True) -> bool:
        """
        å¢å¼ºç‰ˆå¢é‡è®¡ç®—æŒ‡æ ‡ï¼Œåªè®¡ç®—éœ€è¦æ›´æ–°çš„è‚¡ç¥¨
        åŸºäº"Stock X Date X Indicator"ç»´åº¦è¿›è¡Œå¢é‡åˆ¤æ–­
        
        Parameters:
        -----------
        output_file : str
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        max_stocks : Optional[int]
            æœ€å¤§è‚¡ç¥¨æ•°é‡é™åˆ¶
        force_update : bool
            æ˜¯å¦å¼ºåˆ¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨
        batch_size : int
            æ‰¹æ¬¡å¤§å°
        backup_output : bool
            æ˜¯å¦å¤‡ä»½è¾“å‡ºæ–‡ä»¶
            
        Returns:
        --------
        bool : æ˜¯å¦æˆåŠŸ
        """
        if not self.enable_incremental:
            logger.error("å¢é‡è®¡ç®—æ¨¡å¼æœªå¯ç”¨")
            return False
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stocks = self.get_available_stocks()
        if max_stocks:
            stocks = stocks[:max_stocks]
        
        # æ˜¾ç¤ºè®¡ç®—è¦†ç›–èŒƒå›´
        logger.info("=" * 80)
        logger.info("ğŸ“Š å¢å¼ºç‰ˆå¢é‡è®¡ç®—è¦†ç›–èŒƒå›´")
        logger.info("=" * 80)
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
        logger.info(f"ğŸ“ˆ æ€»è‚¡ç¥¨æ•°: {len(stocks)} åª")
        logger.info(f"ğŸ”§ å¼ºåˆ¶æ›´æ–°: {'æ˜¯' if force_update else 'å¦'}")
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"ğŸ’¾ è‡ªåŠ¨å¤‡ä»½: {'æ˜¯' if backup_output else 'å¦'}")
        if max_stocks:
            logger.info(f"ğŸ¯ è‚¡ç¥¨é™åˆ¶: æœ€å¤š {max_stocks} åª")
        
        # åˆ†ææ•°æ®æ—¶é—´èŒƒå›´
        data_start_date = None
        data_end_date = None
        if stocks:
            try:
                sample_stock = stocks[0]
                sample_data = self.read_qlib_binary_data(sample_stock)
                if sample_data is not None and not sample_data.empty:
                    data_start_date = sample_data.index.min()
                    data_end_date = sample_data.index.max()
                    logger.info(f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {data_start_date} è‡³ {data_end_date}")
                    logger.info(f"ğŸ“Š æ•°æ®å¤©æ•°: {(data_end_date - data_start_date).days + 1} å¤©")
            except Exception as e:
                logger.warning(f"æ— æ³•è·å–æ•°æ®æ—¶é—´èŒƒå›´: {e}")
        
        # æ£€æŸ¥ç°æœ‰è¾“å‡ºæ–‡ä»¶çš„æ—¥æœŸèŒƒå›´
        output_start_date = None
        output_end_date = None
        if os.path.exists(output_file):
            try:
                existing_data = pd.read_csv(output_file, low_memory=False)
                logger.info(f"ğŸ“‹ ç°æœ‰è¾“å‡ºæ–‡ä»¶: {len(existing_data)} è¡Œ")
                if 'Date' in existing_data.columns:
                    # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆå­—æ®µåï¼‰å’Œç¬¬äºŒè¡Œï¼ˆä¸­æ–‡æ ‡ç­¾ï¼‰
                    date_data = existing_data['Date'].iloc[2:] if len(existing_data) > 2 else existing_data['Date']
                    # å¤„ç†å¯èƒ½åŒ…å«æ—¶é—´çš„æ—¥æœŸæ ¼å¼
                    output_start_date = pd.to_datetime(date_data, format='mixed').min()
                    output_end_date = pd.to_datetime(date_data, format='mixed').max()
                    logger.info(f"ğŸ“… å·²è®¡ç®—æ—¶é—´èŒƒå›´: {output_start_date} è‡³ {output_end_date}")
                    if data_start_date and data_end_date:
                        # è®¡ç®—è¦†ç›–ç‡
                        total_days = (data_end_date - data_start_date).days + 1
                        calculated_days = (output_end_date - output_start_date).days + 1
                        coverage = (calculated_days / total_days) * 100
                        logger.info(f"ğŸ“Š æ•°æ®è¦†ç›–ç‡: {coverage:.1f}% ({calculated_days}/{total_days} å¤©)")
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶: {e}")
        else:
            logger.info("ğŸ“‹ ç°æœ‰è¾“å‡ºæ–‡ä»¶: ä¸å­˜åœ¨ï¼ˆå°†åˆ›å»ºæ–°æ–‡ä»¶ï¼‰")
        
        # åˆ†æéœ€è¦æ›´æ–°çš„è‚¡ç¥¨å’Œæ—¥æœŸèŒƒå›´
        needs_update = []
        skip_count = 0
        update_date_ranges = []
        
        for symbol in stocks:
            # è·å–è‚¡ç¥¨çš„æ—¥æœŸèŒƒå›´
            start_date, end_date = self._get_stock_date_range(symbol)
            if start_date is None or end_date is None:
                logger.warning(f"è·³è¿‡ {symbol}: æ— æ³•è·å–æ—¥æœŸèŒƒå›´")
                continue
            
            date_range = (start_date, end_date)
            should_update, reason = self._needs_update(symbol, force_update, date_range)
            
            if should_update:
                # è®¡ç®—å¢é‡å¼€å§‹æ—¥æœŸ
                incremental_start_date = None
                if not force_update and output_end_date:
                    # å¦‚æœç°æœ‰æ•°æ®åˆ°2025-05-30ï¼Œå¢é‡è®¡ç®—åº”è¯¥ä»2025-05-31å¼€å§‹
                    incremental_start_date = (output_end_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
                    logger.info(f"{symbol}: å¢é‡è®¡ç®—ä» {incremental_start_date} å¼€å§‹")
                
                needs_update.append((symbol, reason, date_range, incremental_start_date))
                update_date_ranges.append(date_range)
            else:
                skip_count += 1
                logger.debug(f"è·³è¿‡ {symbol}: {reason}")
        
        logger.info(f"éœ€è¦æ›´æ–°: {len(needs_update)} åªè‚¡ç¥¨")
        logger.info(f"è·³è¿‡æ›´æ–°: {skip_count} åªè‚¡ç¥¨")
        
        # æ˜¾ç¤ºæœ¬æ¬¡è®¡ç®—èŒƒå›´
        if needs_update and update_date_ranges:
            if force_update:
                logger.info(f"ğŸ¯ æœ¬æ¬¡è®¡ç®—èŒƒå›´: å…¨é‡è®¡ç®— ({data_start_date} è‡³ {data_end_date})")
            else:
                # è®¡ç®—éœ€è¦æ›´æ–°çš„æ—¥æœŸèŒƒå›´
                update_start = min([r[0] for r in update_date_ranges])
                update_end = max([r[1] for r in update_date_ranges])
                logger.info(f"ğŸ¯ æœ¬æ¬¡è®¡ç®—èŒƒå›´: å¢é‡è®¡ç®— ({len(needs_update)} åªè‚¡ç¥¨)")
                
                # å¦‚æœæœ‰ç°æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œæ˜¾ç¤ºçœŸæ­£çš„å¢é‡èŒƒå›´
                if output_start_date and output_end_date:
                    if pd.to_datetime(update_end) > output_end_date:
                        # çœŸæ­£çš„å¢é‡èŒƒå›´ï¼šä»ç°æœ‰æ•°æ®ç»“æŸæ—¥æœŸåä¸€å¤©å¼€å§‹
                        true_incremental_start = (output_end_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
                        logger.info(f"ğŸ“ˆ å¢é‡è®¡ç®—èŒƒå›´: {true_incremental_start} è‡³ {update_end}")
                        logger.info(f"ğŸ“Š é¢„è®¡æ–°å¢å¤©æ•°: {(pd.to_datetime(update_end) - output_end_date).days} å¤©")
                    elif pd.to_datetime(update_start) < output_start_date:
                        logger.info(f"ğŸ“ˆ è¡¥å……å†å²æ•°æ®: {update_start} è‡³ {output_start_date}")
                else:
                    logger.info(f"ğŸ“… å…¨é‡è®¡ç®—èŒƒå›´: {update_start} è‡³ {update_end}")
        
        logger.info("=" * 80)
        
        if not needs_update:
            logger.info("âœ… æ‰€æœ‰è‚¡ç¥¨éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°")
            return True
        
        # å¤‡ä»½ç°æœ‰è¾“å‡ºæ–‡ä»¶
        backup_path = ""
        if backup_output and os.path.exists(output_file):
            backup_path = self._backup_output_file(output_file)
            if backup_path:
                self.metadata['last_output_backup'] = backup_path
        
        # åˆ†æ‰¹å¤„ç†éœ€è¦æ›´æ–°çš„è‚¡ç¥¨
        success_count = 0
        failed_count = 0
        all_new_data = []
        
        for i in range(0, len(needs_update), batch_size):
            batch = needs_update[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(needs_update) + batch_size - 1) // batch_size
            
            logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch)} åªè‚¡ç¥¨)")
            
            batch_results = []
            
            for symbol, reason, date_range, incremental_start_date in batch:
                try:
                    if incremental_start_date:
                        logger.info(f"è®¡ç®— {symbol} ({reason}) - å¢é‡èŒƒå›´: {incremental_start_date} è‡³ {date_range[1]}")
                    else:
                        logger.info(f"è®¡ç®— {symbol} ({reason}) - æ—¥æœŸèŒƒå›´: {date_range}")
                    
                    result = self.calculate_all_indicators_for_stock(symbol, incremental_start_date)
                    
                    if result is not None and not result.empty:
                        batch_results.append(result)
                        self._update_stock_status(symbol, True, len(result), date_range)
                        success_count += 1
                        logger.info(f"âœ… {symbol}: å®Œæˆ ({len(result)} è¡Œ)")
                    else:
                        self._update_stock_status(symbol, False, 0, date_range)
                        failed_count += 1
                        logger.warning(f"âš ï¸ {symbol}: è®¡ç®—ç»“æœä¸ºç©º")
                        
                except Exception as e:
                    self._update_stock_status(symbol, False, 0, date_range)
                    failed_count += 1
                    logger.error(f"âŒ {symbol}: è®¡ç®—å¤±è´¥ - {e}")
            
            # åˆå¹¶æ‰¹æ¬¡ç»“æœ
            if batch_results:
                batch_data = pd.concat(batch_results, ignore_index=True, sort=False)
                all_new_data.append(batch_data)
                logger.info(f"æ‰¹æ¬¡ {batch_num} å®Œæˆ: {len(batch_data)} è¡Œ")
            
            # ä¿å­˜çŠ¶æ€
            self._save_stock_status()
            self._save_data_hashes()
        
        # åˆå¹¶æ‰€æœ‰æ–°æ•°æ®
        if all_new_data:
            new_data = pd.concat(all_new_data, ignore_index=True, sort=False)
            logger.info(f"æ–°æ•°æ®æ€»è®¡: {len(new_data)} è¡Œ")
            
            # ä¸ç°æœ‰è¾“å‡ºæ–‡ä»¶åˆå¹¶
            final_data = self._merge_with_existing_output(new_data, output_file)
            
            # æ£€æŸ¥åˆå¹¶ç»“æœ
            if final_data is not None and not final_data.empty:
                # ä¿å­˜æœ€ç»ˆç»“æœ
                self.save_results(final_data, output_file)
                logger.info(f"ä¿å­˜æœ€ç»ˆç»“æœ: {len(final_data)} è¡Œ -> {output_file}")
            else:
                logger.error("âŒ åˆå¹¶ç»“æœä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
                logger.info("ğŸ”„ ä¿ç•™ç°æœ‰æ–‡ä»¶")
                # ä¸è¿›è¡Œä»»ä½•æ“ä½œï¼Œä¿ç•™ç°æœ‰æ–‡ä»¶
        else:
            # æ²¡æœ‰æ–°æ•°æ®æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ä¿ç•™ç°æœ‰æ–‡ä»¶
            if os.path.exists(output_file):
                logger.info("ğŸ“‹ æ²¡æœ‰æ–°æ•°æ®ï¼Œä¿ç•™ç°æœ‰è¾“å‡ºæ–‡ä»¶")
                # è¯»å–ç°æœ‰æ–‡ä»¶çš„è¡Œæ•°ä½œä¸ºæœ€ç»ˆç»“æœ
                try:
                    existing_data = pd.read_csv(output_file, skiprows=2, low_memory=False)
                    logger.info(f"ğŸ“‹ ç°æœ‰æ–‡ä»¶ä¿ç•™: {len(existing_data)} è¡Œ")
                except Exception as e:
                    logger.warning(f"ğŸ“‹ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")
            else:
                logger.warning("ğŸ“‹ æ²¡æœ‰æ–°æ•°æ®ä¸”è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ›´æ–°å…ƒæ•°æ®
        self.metadata.update({
            "last_update": datetime.now().isoformat(),
            "total_stocks": len(stocks),
            "processed_stocks": success_count,
            "failed_stocks": failed_count,
            "skipped_stocks": skip_count,
            "output_file": output_file,
            "last_output_backup": backup_path
        })
        self._save_metadata()
        
        logger.info("=" * 80)
        logger.info("âœ… å¢å¼ºç‰ˆå¢é‡è®¡ç®—å®Œæˆï¼")
        logger.info(f"ğŸ“Š æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, è·³è¿‡: {skip_count}")
        logger.info(f"ğŸ“ˆ æ€»è¡Œæ•°: {len(all_new_data) > 0 and sum(len(df) for df in all_new_data) or 0}")
        logger.info(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")
        logger.info("=" * 80)
        
        return True
    
    def get_update_summary(self) -> Dict:
        """è·å–æ›´æ–°æ‘˜è¦"""
        if not self.enable_incremental:
            return {"error": "å¢é‡è®¡ç®—æ¨¡å¼æœªå¯ç”¨"}
        
        total_stocks = len(self.get_available_stocks())
        processed_stocks = len([s for s in self.stock_status.values() if s.get('success', False)])
        failed_stocks = len([s for s in self.stock_status.values() if not s.get('success', False)])
        
        return {
            "total_stocks": total_stocks,
            "processed_stocks": processed_stocks,
            "failed_stocks": failed_stocks,
            "last_update": self.metadata.get('last_update'),
            "output_file": self.metadata.get('output_file')
        }
    
    def analyze_data_coverage(self) -> Dict:
        """
        åˆ†ææ•°æ®è¦†ç›–ç‡
        
        Returns:
        --------
        Dict: è¦†ç›–ç‡åˆ†æç»“æœ
        """
        if not self.enable_incremental:
            return {"error": "å¢é‡è®¡ç®—æ¨¡å¼æœªå¯ç”¨"}
        
        stocks = self.get_available_stocks()
        if not stocks:
            return {"error": "æ²¡æœ‰å¯ç”¨çš„è‚¡ç¥¨æ•°æ®"}
        
        # åˆ†ææ•°æ®æ—¶é—´èŒƒå›´
        data_ranges = {}
        total_days = 0
        
        for symbol in stocks[:10]:  # é‡‡æ ·å‰10åªè‚¡ç¥¨
            start_date, end_date = self._get_stock_date_range(symbol)
            if start_date and end_date:
                start_dt = pd.to_datetime(start_date)
                end_dt = pd.to_datetime(end_date)
                days = (end_dt - start_dt).days + 1
                data_ranges[symbol] = {
                    'start_date': start_date,
                    'end_date': end_date,
                    'days': days
                }
                total_days += days
        
        # åˆ†æå·²å¤„ç†çš„æ•°æ®
        processed_info = {}
        for symbol, status in self.stock_status.items():
            if status.get('success', False):
                date_range = self.date_ranges.get(symbol, {})
                if date_range.get('start_date') and date_range.get('end_date'):
                    start_dt = pd.to_datetime(date_range['start_date'])
                    end_dt = pd.to_datetime(date_range['end_date'])
                    days = (end_dt - start_dt).days + 1
                    processed_info[symbol] = {
                        'start_date': date_range['start_date'],
                        'end_date': date_range['end_date'],
                        'days': days,
                        'rows': status.get('rows', 0)
                    }
        
        # è®¡ç®—è¦†ç›–ç‡
        total_processed_stocks = len(processed_info)
        total_processed_days = sum(info['days'] for info in processed_info.values())
        
        coverage = {
            'total_stocks': len(stocks),
            'processed_stocks': total_processed_stocks,
            'coverage_percentage': (total_processed_stocks / len(stocks)) * 100 if stocks else 0,
            'sample_data_ranges': data_ranges,
            'processed_data_ranges': processed_info,
            'total_processed_days': total_processed_days,
            'average_days_per_stock': total_processed_days / total_processed_stocks if total_processed_stocks > 0 else 0
        }
        
        return coverage
    
    def clean_cache(self, keep_backups: bool = False):
        """æ¸…ç†ç¼“å­˜"""
        if not self.enable_incremental:
            logger.error("å¢é‡è®¡ç®—æ¨¡å¼æœªå¯ç”¨")
            return
        
        try:
            # åˆ é™¤ç¼“å­˜æ–‡ä»¶
            for file_path in [self.metadata_file, self.stock_status_file, 
                            self.data_hashes_file, self.date_ranges_file]:
                if file_path.exists():
                    file_path.unlink()
                    logger.info(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶: {file_path}")
            
            # æ¸…ç†å¤‡ä»½æ–‡ä»¶
            if not keep_backups and self.output_backup_dir.exists():
                shutil.rmtree(self.output_backup_dir)
                logger.info(f"åˆ é™¤å¤‡ä»½ç›®å½•: {self.output_backup_dir}")
            
            # é‡æ–°åˆå§‹åŒ–
            self.metadata = self._load_metadata()
            self.stock_status = self._load_stock_status()
            self.data_hashes = self._load_data_hashes()
            self.date_ranges = self._load_date_ranges()
            
            logger.info("âœ… ç¼“å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def list_backups(self) -> List[str]:
        """åˆ—å‡ºå¤‡ä»½æ–‡ä»¶"""
        if not self.enable_incremental:
            return []
        
        backups = []
        if self.output_backup_dir.exists():
            for backup_file in self.output_backup_dir.glob("backup_*.csv"):
                backups.append(str(backup_file))
        
        return sorted(backups, reverse=True)
    
    def restore_backup(self, backup_file: str, output_file: str) -> bool:
        """æ¢å¤å¤‡ä»½æ–‡ä»¶"""
        if not self.enable_incremental:
            logger.error("å¢é‡è®¡ç®—æ¨¡å¼æœªå¯ç”¨")
            return False
        
        try:
            if not os.path.exists(backup_file):
                logger.error(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
                return False
            
            shutil.copy2(backup_file, output_file)
            logger.info(f"âœ… å¤‡ä»½æ¢å¤å®Œæˆ: {backup_file} -> {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¢å¤å¤‡ä»½å¤±è´¥: {e}")
            return False

    def _get_standard_column_order(self):
        """
        è·å–æ ‡å‡†å­—æ®µé¡ºåºï¼Œç¡®ä¿ä¸åŒå¸‚åœºç”Ÿæˆçš„CSVæ–‡ä»¶å­—æ®µé¡ºåºä¸€è‡´
        """
        # æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„ç‰¹å®šé¡ºåºå®šä¹‰å­—æ®µ
        custom_columns = [
            'Date',
            'Symbol',
            'Open',
            'High',
            'Low',
            'Close',
            'Volume',
            'RealizedVolatility_20',
            'NegativeSemiDeviation_20',
            'ContinuousVolatility_20',
            'PositiveSemiDeviation_20',
            'Volatility_10',
            'Volatility_30',
            'Volatility_60',
            'CDL2CROWS',
            'CDL3BLACKCROWS',
            'CDL3INSIDE',
            'CDL3LINESTRIKE',
            'CDL3OUTSIDE',
            'CDL3STARSINSOUTH',
            'CDL3WHITESOLDIERS',
            'CDLABANDONEDBABY',
            'CDLADVANCEBLOCK',
            'CDLBELTHOLD',
            'CDLBREAKAWAY',
            'CDLCLOSINGMARUBOZU',
            'CDLCONCEALBABYSWALL',
            'CDLCOUNTERATTACK',
            'CDLDARKCLOUDCOVER',
            'CDLDOJI',
            'CDLDOJISTAR',
            'CDLDRAGONFLYDOJI',
            'CDLENGULFING',
            'CDLEVENINGDOJISTAR',
            'CDLEVENINGSTAR',
            'CDLGAPSIDESIDEWHITE',
            'CDLGRAVESTONEDOJI',
            'CDLHAMMER',
            'CDLHANGINGMAN',
            'CDLHARAMI',
            'CDLHARAMICROSS',
            'CDLHIGHWAVE',
            'CDLHIKKAKE',
            'CDLHIKKAKEMOD',
            'CDLHOMINGPIGEON',
            'CDLIDENTICAL3CROWS',
            'CDLINNECK',
            'CDLINVERTEDHAMMER',
            'CDLKICKING',
            'CDLKICKINGBYLENGTH',
            'CDLLADDERBOTTOM',
            'CDLLONGLEGGEDDOJI',
            'CDLLONGLINE',
            'CDLMARUBOZU',
            'CDLMATCHINGLOW',
            'CDLMATHOLD',
            'CDLMORNINGDOJISTAR',
            'CDLMORNINGSTAR',
            'CDLONNECK',
            'CDLPIERCING',
            'CDLRICKSHAWMAN',
            'CDLRISEFALL3METHODS',
            'CDLSEPARATINGLINES',
            'CDLSHOOTINGSTAR',
            'CDLSHORTLINE',
            'CDLSPINNINGTOP',
            'CDLSTALLEDPATTERN',
            'CDLSTICKSANDWICH',
            'CDLTAKURI',
            'CDLTASUKIGAP',
            'CDLTHRUSTING',
            'CDLTRISTAR',
            'CDLUNIQUE3RIVER',
            'CDLUPSIDEGAP2CROWS',
            'CDLXSIDEGAP3METHODS',
            'PriceToBookRatio',
            'MarketCap',
            'PERatio',
            'PriceToSalesRatio',
            'ROE',
            'ROA',
            'ProfitMargins',
            'QuickRatio',
            'DebtToEquity',
            'TobinsQ',
            'DailyTurnover',
            'turnover_c1d',
            'turnover_c5d',
            'turnover_m5d',
            'turnover_c10d',
            'turnover_m10d',
            'turnover_c20d',
            'turnover_m20d',
            'turnover_c30d',
            'turnover_m30d',
            'CurrentRatio',
            'SMA_5',
            'SMA_10',
            'SMA_20',
            'SMA_50',
            'EMA_5',
            'EMA_10',
            'EMA_20',
            'EMA_50',
            'DEMA_20',
            'TEMA_20',
            'KAMA_30',
            'WMA_20',
            'MACD',
            'MACD_Signal',
            'MACD_Histogram',
            'MACDEXT',
            'MACDFIX',
            'RSI_14',
            'CCI_14',
            'CMO_14',
            'MFI_14',
            'WILLR_14',
            'ULTOSC',
            'ADX_14',
            'ADXR_14',
            'APO',
            'AROON_DOWN',
            'AROON_UP',
            'AROONOSC_14',
            'BOP',
            'DX_14',
            'MINUS_DI_14',
            'MINUS_DM_14',
            'PLUS_DI_14',
            'PLUS_DM_14',
            'PPO',
            'TRIX_30',
            'MOM_10',
            'ROC_10',
            'ROCP_10',
            'ROCR_10',
            'ROCR100_10',
            'BB_Upper',
            'BB_Middle',
            'BB_Lower',
            'STOCH_K',
            'STOCH_D',
            'STOCHF_K',
            'STOCHF_D',
            'STOCHRSI_K',
            'STOCHRSI_D',
            'ATR_14',
            'NATR_14',
            'TRANGE',
            'OBV',
            'AD',
            'ADOSC',
            'HT_DCPERIOD',
            'HT_DCPHASE',
            'HT_INPHASE',
            'HT_QUADRATURE',
            'HT_SINE',
            'HT_LEADSINE',
            'HT_TRENDMODE',
            'HT_TRENDLINE',
            'AVGPRICE',
            'MEDPRICE',
            'TYPPRICE',
            'WCLPRICE',
            'MIDPOINT',
            'MIDPRICE',
            'MAMA',
            'FAMA',
            'LINEARREG',
            'LINEARREG_ANGLE',
            'LINEARREG_INTERCEPT',
            'LINEARREG_SLOPE',
            'STDDEV',
            'TSF',
            'VAR',
            'MAXINDEX',
            'MININDEX'
        ]
        
        # Alpha360æŒ‡æ ‡å­—æ®µï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚çš„é¡ºåºï¼šCLOSEä»59åˆ°0ï¼Œç„¶åOPENä»59åˆ°0ï¼Œç„¶åHIGHä»59åˆ°0ï¼Œç„¶åLOWä»59åˆ°0ï¼Œç„¶åVWAPä»59åˆ°0ï¼Œæœ€åVOLUMEä»59åˆ°0ï¼‰
        alpha360_columns = []
        
        # CLOSEä»59åˆ°0
        for i in range(59, -1, -1):
            alpha360_columns.append(f'ALPHA360_CLOSE{i}')
        
        # OPENä»59åˆ°0
        for i in range(59, -1, -1):
            alpha360_columns.append(f'ALPHA360_OPEN{i}')
        
        # HIGHä»59åˆ°0
        for i in range(59, -1, -1):
            alpha360_columns.append(f'ALPHA360_HIGH{i}')
        
        # LOWä»59åˆ°0
        for i in range(59, -1, -1):
            alpha360_columns.append(f'ALPHA360_LOW{i}')
        
        # VWAPä»59åˆ°0
        for i in range(59, -1, -1):
            alpha360_columns.append(f'ALPHA360_VWAP{i}')
        
        # VOLUMEä»59åˆ°0
        for i in range(59, -1, -1):
            alpha360_columns.append(f'ALPHA360_VOLUME{i}')
        
        # Alpha158æŒ‡æ ‡å­—æ®µï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚çš„é¡ºåºï¼‰
        alpha158_columns = [
            'ALPHA158_KMID',
            'ALPHA158_KLEN',
            'ALPHA158_KMID2',
            'ALPHA158_KUP',
            'ALPHA158_KUP2',
            'ALPHA158_KLOW',
            'ALPHA158_KLOW2',
            'ALPHA158_KSFT',
            'ALPHA158_KSFT2',
            'ALPHA158_OPEN0',
            'ALPHA158_HIGH0',
            'ALPHA158_LOW0',
            'ALPHA158_VWAP0',
            'ALPHA158_VOLUME0',
            'ALPHA158_ROC5',
            'ALPHA158_ROC10',
            'ALPHA158_ROC20',
            'ALPHA158_ROC30',
            'ALPHA158_ROC60',
            'ALPHA158_MA5',
            'ALPHA158_MA10',
            'ALPHA158_MA20',
            'ALPHA158_MA30',
            'ALPHA158_MA60',
            'ALPHA158_STD5',
            'ALPHA158_STD10',
            'ALPHA158_STD20',
            'ALPHA158_STD30',
            'ALPHA158_STD60',
            'ALPHA158_BETA5',
            'ALPHA158_BETA10',
            'ALPHA158_BETA20',
            'ALPHA158_BETA30',
            'ALPHA158_BETA60',
            'ALPHA158_RSQR5',
            'ALPHA158_RSQR10',
            'ALPHA158_RSQR20',
            'ALPHA158_RSQR30',
            'ALPHA158_RSQR60',
            'ALPHA158_MAX5',
            'ALPHA158_MIN5',
            'ALPHA158_MAX10',
            'ALPHA158_MIN10',
            'ALPHA158_MAX20',
            'ALPHA158_MIN20',
            'ALPHA158_MAX30',
            'ALPHA158_MIN30',
            'ALPHA158_MAX60',
            'ALPHA158_MIN60',
            'ALPHA158_QTLU5',
            'ALPHA158_QTLD5',
            'ALPHA158_QTLU10',
            'ALPHA158_QTLD10',
            'ALPHA158_QTLU20',
            'ALPHA158_QTLD20',
            'ALPHA158_QTLU30',
            'ALPHA158_QTLD30',
            'ALPHA158_QTLU60',
            'ALPHA158_QTLD60',
            'ALPHA158_RANK5',
            'ALPHA158_RANK10',
            'ALPHA158_RANK20',
            'ALPHA158_RANK30',
            'ALPHA158_RANK60',
            'ALPHA158_RSV5',
            'ALPHA158_RSV10',
            'ALPHA158_RSV20',
            'ALPHA158_RSV30',
            'ALPHA158_RSV60',
            'ALPHA158_RESI5',
            'ALPHA158_RESI10',
            'ALPHA158_RESI20',
            'ALPHA158_RESI30',
            'ALPHA158_RESI60',
            'ALPHA158_IMAX5',
            'ALPHA158_IMAX10',
            'ALPHA158_IMAX20',
            'ALPHA158_IMAX30',
            'ALPHA158_IMAX60',
            'ALPHA158_IMIN5',
            'ALPHA158_IMIN10',
            'ALPHA158_IMIN20',
            'ALPHA158_IMIN30',
            'ALPHA158_IMIN60',
            'ALPHA158_IMXD5',
            'ALPHA158_IMXD10',
            'ALPHA158_IMXD20',
            'ALPHA158_IMXD30',
            'ALPHA158_IMXD60',
            'ALPHA158_CORR5',
            'ALPHA158_CORR10',
            'ALPHA158_CORR20',
            'ALPHA158_CORR30',
            'ALPHA158_CORR60',
            'ALPHA158_CORD5',
            'ALPHA158_CORD10',
            'ALPHA158_CORD20',
            'ALPHA158_CORD30',
            'ALPHA158_CORD60',
            'ALPHA158_CNTP5',
            'ALPHA158_CNTP10',
            'ALPHA158_CNTP20',
            'ALPHA158_CNTP30',
            'ALPHA158_CNTP60',
            'ALPHA158_CNTN5',
            'ALPHA158_CNTN10',
            'ALPHA158_CNTN20',
            'ALPHA158_CNTN30',
            'ALPHA158_CNTN60',
            'ALPHA158_CNTD5',
            'ALPHA158_CNTD10',
            'ALPHA158_CNTD20',
            'ALPHA158_CNTD30',
            'ALPHA158_CNTD60',
            'ALPHA158_SUMP5',
            'ALPHA158_SUMP10',
            'ALPHA158_SUMP20',
            'ALPHA158_SUMP30',
            'ALPHA158_SUMP60',
            'ALPHA158_SUMN5',
            'ALPHA158_SUMN10',
            'ALPHA158_SUMN20',
            'ALPHA158_SUMN30',
            'ALPHA158_SUMN60',
            'ALPHA158_SUMD5',
            'ALPHA158_SUMD10',
            'ALPHA158_SUMD20',
            'ALPHA158_SUMD30',
            'ALPHA158_SUMD60',
            'ALPHA158_VMA5',
            'ALPHA158_VMA10',
            'ALPHA158_VMA20',
            'ALPHA158_VMA30',
            'ALPHA158_VMA60',
            'ALPHA158_VSTD5',
            'ALPHA158_VSTD10',
            'ALPHA158_VSTD20',
            'ALPHA158_VSTD30',
            'ALPHA158_VSTD60',
            'ALPHA158_WVMA5',
            'ALPHA158_WVMA10',
            'ALPHA158_WVMA20',
            'ALPHA158_WVMA30',
            'ALPHA158_WVMA60',
            'ALPHA158_VSUMP5',
            'ALPHA158_VSUMP10',
            'ALPHA158_VSUMP20',
            'ALPHA158_VSUMP30',
            'ALPHA158_VSUMP60',
            'ALPHA158_VSUMN5',
            'ALPHA158_VSUMN10',
            'ALPHA158_VSUMN20',
            'ALPHA158_VSUMN30',
            'ALPHA158_VSUMN60',
            'ALPHA158_VSUMD5',
            'ALPHA158_VSUMD10',
            'ALPHA158_VSUMD20',
            'ALPHA158_VSUMD30',
            'ALPHA158_VSUMD60'
        ]
        
        # åˆå¹¶æ‰€æœ‰å­—æ®µï¼ŒæŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„é¡ºåº
        all_columns = custom_columns + alpha360_columns + alpha158_columns
        
        return all_columns


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¢å¼ºç‰ˆQlibæŒ‡æ ‡è®¡ç®—å™¨ - é›†æˆAlpha158ã€Alpha360æŒ‡æ ‡ä½“ç³»',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  # å…¨é‡è®¡ç®—æ¨¡å¼ (é»˜è®¤)
  python qlib_indicators.py
  
  # å¢å¼ºç‰ˆå¢é‡è®¡ç®—æ¨¡å¼
  python qlib_indicators.py --incremental --output indicators.csv
  
  # å¼ºåˆ¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨
  python qlib_indicators.py --incremental --force-update
  
  # åªè®¡ç®—å‰10åªè‚¡ç¥¨
  python qlib_indicators.py --max-stocks 10
  
  # æŒ‡å®šæ•°æ®ç›®å½•å’Œè´¢åŠ¡æ•°æ®ç›®å½•
  python qlib_indicators.py --data-dir ./data --financial-dir ./financial_data
  
  # è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶å
  python qlib_indicators.py --output indicators_2025.csv
  
  # ç¦ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—
  python qlib_indicators.py --disable-parallel
  
  # è‡ªå®šä¹‰çº¿ç¨‹æ•°é‡
  python qlib_indicators.py --max-workers 16
  
  # æµå¼å†™å…¥æ¨¡å¼ (èŠ‚çœå†…å­˜)
  python qlib_indicators.py --streaming --batch-size 50
  
  # è°ƒè¯•æ¨¡å¼
  python qlib_indicators.py --log-level DEBUG --max-stocks 5

æ—¶é—´çª—å£è®¾ç½®:
  # è®¡ç®—æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®
  python qlib_indicators.py --start-date 2023-01-01 --end-date 2023-12-31

  # åªè®¡ç®—2024å¹´çš„æ•°æ®
  python qlib_indicators.py --start-date 2024-01-01 --end-date 2024-12-31

  # è®¡ç®—æœ€è¿‘30å¤©çš„æ•°æ®
  python qlib_indicators.py --recent-days 30

  # è®¡ç®—æœ€è¿‘ä¸€å¹´çš„æ•°æ®
  python qlib_indicators.py --recent-days 365

  # ç»“åˆå¢é‡è®¡ç®—å’Œæ—¶é—´çª—å£
  python qlib_indicators.py --incremental --recent-days 90

å¢å¼ºç‰ˆå¢é‡è®¡ç®—ç®¡ç†:
  # æŸ¥çœ‹å¢é‡è®¡ç®—æ‘˜è¦
  python qlib_indicators.py --incremental --summary
  
  # åˆ†ææ•°æ®è¦†ç›–ç‡
  python qlib_indicators.py --incremental --analyze-coverage
  
  # æ¸…ç†ç¼“å­˜
  python qlib_indicators.py --incremental --clean-cache
  
  # åˆ—å‡ºå¤‡ä»½æ–‡ä»¶
  python qlib_indicators.py --incremental --list-backups
  
  # æ¢å¤å¤‡ä»½
  python qlib_indicators.py --incremental --restore-backup backup_file.csv

å¤šçº¿ç¨‹æ€§èƒ½ä¼˜åŒ–:
  - ğŸš€ å¤šåªè‚¡ç¥¨å¹¶è¡Œè®¡ç®—: æ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦
  - âš¡ å•åªè‚¡ç¥¨æŒ‡æ ‡ç±»å‹å¹¶è¡Œ: Alpha158ã€Alpha360ã€æŠ€æœ¯æŒ‡æ ‡ç­‰åŒæ—¶è®¡ç®—
  - ğŸ§µ æ™ºèƒ½çº¿ç¨‹ç®¡ç†: è‡ªåŠ¨ä¼˜åŒ–çº¿ç¨‹æ•°é‡ï¼Œé¿å…èµ„æºç«äº‰
  - ğŸ“Š å®æ—¶è¿›åº¦æ˜¾ç¤º: è¯¦ç»†çš„è®¡ç®—è¿›åº¦å’Œæ€§èƒ½ç»Ÿè®¡
  - ğŸ”’ çº¿ç¨‹å®‰å…¨: ç¡®ä¿æŒ‡æ ‡å»é‡å’Œæ•°æ®ä¸€è‡´æ€§

å¢å¼ºç‰ˆå¢é‡è®¡ç®—ç‰¹æ€§:
  - ğŸ”„ æ™ºèƒ½å¢é‡æ›´æ–°: åŸºäº"Stock X Date X Indicator"ç»´åº¦åˆ¤æ–­
  - ğŸ“Š æ•°æ®å“ˆå¸Œæ£€æµ‹: åŸºäºMD5å“ˆå¸Œå€¼æ£€æµ‹æ•°æ®å˜åŒ–
  - ğŸ“… æ—¥æœŸèŒƒå›´åˆ†æ: æ™ºèƒ½æ£€æµ‹æ•°æ®æ—¶é—´èŒƒå›´å˜åŒ–
  - ğŸ’¾ è‡ªåŠ¨å¤‡ä»½: è®¡ç®—å‰è‡ªåŠ¨å¤‡ä»½ç°æœ‰ç»“æœ
  - ğŸ” çŠ¶æ€è·Ÿè¸ª: è¯¦ç»†è®°å½•æ¯åªè‚¡ç¥¨çš„å¤„ç†çŠ¶æ€
  - ğŸ›¡ï¸ å®¹é”™æ¢å¤: æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œé”™è¯¯æ¢å¤
  - ğŸ“ˆ è¦†ç›–ç‡åˆ†æ: å®æ—¶æ˜¾ç¤ºæ•°æ®è¦†ç›–ç‡ç»Ÿè®¡

æ”¯æŒçš„æŒ‡æ ‡ç±»å‹:
  - Alpha158æŒ‡æ ‡ä½“ç³»: ~158ä¸ª (KBARã€ä»·æ ¼ã€æˆäº¤é‡ã€æ»šåŠ¨æŠ€æœ¯æŒ‡æ ‡)
  - Alpha360æŒ‡æ ‡ä½“ç³»: ~360ä¸ª (è¿‡å»60å¤©æ ‡å‡†åŒ–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®)
  - æŠ€æœ¯æŒ‡æ ‡: ~60ä¸ª (ç§»åŠ¨å¹³å‡ã€MACDã€RSIã€å¸ƒæ—å¸¦ç­‰)
  - èœ¡çƒ›å›¾å½¢æ€: 61ä¸ª (é”¤å­çº¿ã€åå­—æ˜Ÿã€åæ²¡å½¢æ€ç­‰)
  - è´¢åŠ¡æŒ‡æ ‡: ~15ä¸ª (å¸‚å‡€ç‡ã€æ¢æ‰‹ç‡ã€æ‰˜å®¾Qå€¼ç­‰)
  - æ³¢åŠ¨ç‡æŒ‡æ ‡: ~8ä¸ª (å·²å®ç°æ³¢åŠ¨ç‡ã€åŠå˜å·®ç­‰)
  
æ€»è®¡çº¦695ä¸ªæŒ‡æ ‡ï¼Œå…·å¤‡å»é‡åŠŸèƒ½å’Œå¤šçº¿ç¨‹åŠ é€Ÿ
        '''
    )
    
    parser.add_argument(
        '--data-dir',
        default=r"D:\stk_data\trd\us_data",
        help='Qlibæ•°æ®ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--financial-dir',
        help='è´¢åŠ¡æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ~/.qlib/financial_data)'
    )
    
    parser.add_argument(
        '--max-stocks',
        type=int,
        help='æœ€å¤§è‚¡ç¥¨æ•°é‡é™åˆ¶'
    )
    
    parser.add_argument(
        '--output',
        default='enhanced_quantitative_indicators.csv',
        help='è¾“å‡ºæ–‡ä»¶å'
    )
    
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ«'
    )
    
    parser.add_argument(
        '--disable-parallel',
        action='store_true',
        help='ç¦ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—'
    )
    
    parser.add_argument(
        '--max-workers',
        type=int,
        help='æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°+4ï¼Œæœ€å¤§32)'
    )
    
    parser.add_argument('--streaming', action='store_true', help='æ˜¯å¦å¯ç”¨æµå¼å†™å…¥æ¨¡å¼')
    parser.add_argument('--batch-size', type=int, default=20, help='æµå¼å†™å…¥æ‰¹æ¬¡å¤§å°')
    
    # æ—¶é—´çª—å£å‚æ•°
    parser.add_argument('--start-date', type=str, help='è®¡ç®—å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚: 2023-01-01)')
    parser.add_argument('--end-date', type=str, help='è®¡ç®—ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚: 2023-12-31)')
    parser.add_argument('--recent-days', type=int, help='è®¡ç®—æœ€è¿‘Nå¤©çš„æ•°æ® (ä¸start-date/end-dateäº’æ–¥ï¼Œå¦‚: --recent-days 30)')
    
    # å¢é‡è®¡ç®—ç›¸å…³å‚æ•°
    parser.add_argument('--incremental', action='store_true', help='å¯ç”¨å¢å¼ºç‰ˆå¢é‡è®¡ç®—æ¨¡å¼')
    parser.add_argument('--cache-dir', default='indicator_cache', help='å¢é‡è®¡ç®—ç¼“å­˜ç›®å½•')
    parser.add_argument('--force-update', action='store_true', help='å¼ºåˆ¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨')
    parser.add_argument('--backup-output', action='store_true', default=True, help='æ˜¯å¦å¤‡ä»½è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--enable-parallel', action='store_true', default=True, help='å¯ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—')
    
    # å¢é‡è®¡ç®—ç®¡ç†å‘½ä»¤
    parser.add_argument('--summary', action='store_true', help='æ˜¾ç¤ºå¢é‡è®¡ç®—æ‘˜è¦')
    parser.add_argument('--clean-cache', action='store_true', help='æ¸…ç†å¢é‡è®¡ç®—ç¼“å­˜')
    parser.add_argument('--list-backups', action='store_true', help='åˆ—å‡ºå¤‡ä»½æ–‡ä»¶')
    parser.add_argument('--restore-backup', help='æ¢å¤æŒ‡å®šçš„å¤‡ä»½æ–‡ä»¶')
    parser.add_argument('--analyze-coverage', action='store_true', help='åˆ†ææ•°æ®è¦†ç›–ç‡')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.remove()
    logger.add(
        lambda msg: print(msg, end=""),
        level=args.log_level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    
    try:
        # éªŒè¯æ—¶é—´çª—å£å‚æ•°
        if args.recent_days and (args.start_date or args.end_date):
            logger.warning("åŒæ—¶æŒ‡å®šäº†--recent-dayså’Œ--start-date/--end-dateå‚æ•°ï¼Œå°†ä¼˜å…ˆä½¿ç”¨--recent-days")
            
        # åˆ›å»ºè®¡ç®—å™¨
        calculator = QlibIndicatorsEnhancedCalculator(
            data_dir=args.data_dir,
            financial_data_dir=args.financial_dir,
            enable_parallel=not args.disable_parallel,
            start_date=args.start_date,
            end_date=args.end_date,
            recent_days=args.recent_days,
            max_workers=args.max_workers,
            cache_dir=args.cache_dir,
            enable_incremental=args.incremental
        )
        
        # å¤„ç†å¢é‡è®¡ç®—ç®¡ç†å‘½ä»¤
        if args.summary:
            summary = calculator.get_update_summary()
            logger.info("ğŸ“Š å¢å¼ºç‰ˆå¢é‡è®¡ç®—æ‘˜è¦:")
            for key, value in summary.items():
                logger.info(f"  {key}: {value}")
            return
        
        if args.analyze_coverage:
            coverage = calculator.analyze_data_coverage()
            if "error" in coverage:
                logger.error(f"âŒ è¦†ç›–ç‡åˆ†æå¤±è´¥: {coverage['error']}")
                return
            
            logger.info("ğŸ“Š æ•°æ®è¦†ç›–ç‡åˆ†æ:")
            logger.info(f"  æ€»è‚¡ç¥¨æ•°: {coverage['total_stocks']}")
            logger.info(f"  å·²å¤„ç†è‚¡ç¥¨æ•°: {coverage['processed_stocks']}")
            logger.info(f"  è¦†ç›–ç‡: {coverage['coverage_percentage']:.1f}%")
            logger.info(f"  æ€»å¤„ç†å¤©æ•°: {coverage['total_processed_days']}")
            logger.info(f"  å¹³å‡æ¯åªè‚¡ç¥¨å¤©æ•°: {coverage['average_days_per_stock']:.1f}")
            
            if coverage['sample_data_ranges']:
                logger.info("ğŸ“… æ ·æœ¬æ•°æ®æ—¶é—´èŒƒå›´:")
                for symbol, info in list(coverage['sample_data_ranges'].items())[:5]:
                    logger.info(f"  {symbol}: {info['start_date']} è‡³ {info['end_date']} ({info['days']} å¤©)")
            return
        
        if args.clean_cache:
            calculator.clean_cache(keep_backups=True)
            return
        
        if args.list_backups:
            backups = calculator.list_backups()
            if backups:
                logger.info("ğŸ“ å¤‡ä»½æ–‡ä»¶åˆ—è¡¨:")
                for backup in backups:
                    logger.info(f"  {backup}")
            else:
                logger.info("ğŸ“ æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")
            return
        
        if args.restore_backup:
            if calculator.restore_backup(args.restore_backup, args.output):
                logger.info(f"âœ… å¤‡ä»½æ¢å¤å®Œæˆ: {args.output}")
            return
        
        # è¿è¡Œè®¡ç®—
        if args.incremental:
            # å¢å¼ºç‰ˆå¢é‡è®¡ç®—æ¨¡å¼
            logger.info("ğŸ”„ å¯åŠ¨å¢å¼ºç‰ˆå¢é‡è®¡ç®—æ¨¡å¼...")
            logger.info("ğŸ“Š åŸºäº'Stock X Date X Indicator'ç»´åº¦è¿›è¡Œå¢é‡åˆ¤æ–­")
            success = calculator.calculate_indicators_incremental(
                output_file=args.output,
                max_stocks=args.max_stocks,
                force_update=args.force_update,
                batch_size=args.batch_size,
                backup_output=args.backup_output
            )
            if not success:
                logger.error("âŒ å¢å¼ºç‰ˆå¢é‡è®¡ç®—å¤±è´¥")
        elif args.streaming:
            # æµå¼æ¨¡å¼
            calculator.calculate_all_indicators_streaming(
                output_file=args.output,
                max_stocks=args.max_stocks,
                batch_size=args.batch_size
            )
        else:
            # æ ‡å‡†å…¨é‡è®¡ç®—æ¨¡å¼
            calculator.run(
                max_stocks=args.max_stocks,
                output_filename=args.output
            )
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­è®¡ç®—")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
